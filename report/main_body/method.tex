\chapter{Μέθοδος}
\label{chap:method}

Όπως αναφέραμε στο προηγούμενο κεφάλαιο, οι θεμελιακές υλοποιήσεις των νευρωνικών δικτύων από κάψουλες (\cite{hinton2011transforming, sabour2017dynamic, hinton2018matrix}) βασίζονται σε υποθέσεις των οποίων η εγκυρότητα δεν έχει δοκιμαστεί εκτενώς. Για τον λόγο αυτό (αλλά και για λόγους σύγκρισης με άλλες μεθόδους), δύο από τις τέσσερεις μεθόδους που χρησιμοποιούμε στο πειραματικό μέρος της παρούσας διπλωματικής αναπτύχθηκαν (ή τροποποιήθηκαν) σε πηγαίο κώδικα σύμφωνα με την αρχιτεκτονική και τον αλγόριθμο δρομολόγησης που παρουσιάζονται στα έργα \cite{sabour2017dynamic} και \cite{hinton2018matrix} αντίστοιχα. \par

Οι δύο τελευταίες μέθοδοί του παρόντος κεφαλαίου, πατώντας στις βασικές δομικές αρχές των νευρωνικών δικτύων με κάψουλες, επιδιώκουν να βελτιώσουν ορισμένες από τις ανεπάρκειες της τεχνολογίας. Έτσι, προτείνονται καινοτόμοι αλγόριθμοι και αρχιτεκτονικές νευρωνικών δικτύων που εστιάζουν είτε σε προβλήματα ταχύτητας και κλιμακωσιμότητας (τρίτη μέθοδος) είτε στα προβλήματα εγκυρότητας των αρχικών υποθέσεων (τέταρτη μέθοδος).


\section{\en{Dynamic Routing Between Capsules}}
\label{sec:method_1}

Η μέθοδος της \textquote{Δυναμικής Δρομολόγησης με Κάψουλες} υλοποιήθηκε σε κώδικα ακολουθώντας πιστά την ομώνυμη δημοσίευση των \en{Sabour S. et al.} \cite{sabour2017dynamic}. Αν και υπήρχαν έτοιμες υλοποιήσεις του έργου στο διαδίκτυο, δυστυχώς ο κώδικάς τους ήταν αναχρονισμένος και δε λειτουργούσε σε σύγχρονα συστήματα με τις νέες εκδόσεις των πακέτων λογισμικού. Για αυτό και υλοποιήθηκε εκ νέου στη γλώσσα \en{python 3} χρησιμοποιώντας τη βιβλιοθήκη \en{tensorflow 2}. \par

Σε αυτήν την ενότητα θα ξεκινήσουμε παρουσιάζοντας τη γενική αρχιτεκτονική του νευρωνικού δικτύου με κάψουλες που προτείνει το εν λόγω έργο. Στη συνέχεια και σε ξεχωριστή υπο\textendash ενότητα θα παρουσιάσουμε τον αλγόριθμο δρομολόγησης που λαμβάνει χώρα μεταξύ των δυο διαδοχικών επιπέδων από κάψουλες ενώ παράλληλα θα κάνουμε ορισμένες σχετικές σημειώσεις. Έπειτα, θα αναφερθούμε στη συνάρτηση σφάλματος που χρησιμοποιείται για την εκπαίδευση του νευρωνικού δικτύου κάτω από τις διάφορες συνθήκες παραμετροποίησης. Τέλος,για λοιπές λεπτομέρειες υλοποίησης όπως τιμές αρχικοποίησης, τύπος βελτιστοποιητή (\en{optimizer}) κτλ. παραπέμπουμε τον αναγνώστη στην ιστοσελίδα όπου είναι αναρτημένος ο κώδικάς μας.

\subsection{Αρχιτεκτονική Νευρωνικού Δικτύου}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/chapter method/first_method_architecture_encoder.pdf}
    \caption{Η αρχιτεκτονική του νευρωνικού δικτύου με κάψουλες της πρώτης μεθόδου. \textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
    \label{fig:method_1_architecture}
  \end{figure}

Το βασικό μοντέλο που χρησιμοποιείται στην πλειοψηφία των πειραμάτων μας παρουσιάζεται στο σχήμα \ref{fig:method_1_architecture}. Αποτελείται από 3 επίπεδα εκ των οποίων τα πρώτα δύο είναι συνελικτικά (ονόματι \en{Conv1} και \en{PrimaryCaps}) και το τρίτο πλήρως διασυνδεδεμένο (επίπεδο \en{DigitCaps}). Πιο αναλυτικά, το επίπεδο \en{Conv1} έχει 256 πυρήνες συνέλιξης μεγέθους $9 \times 9$, βήμα (\en{stride}) ίσο με 1 και συνάρτηση ενεργοποίησης \en{ReLU}.\par

Αναφορικά με το επίπεδο \en{PrimaryCaps}, ο ρόλος του είναι αυτός των ανάστροφων γραφικών, όπως εξηγήσαμε στην ενότητα \ref{sec:capsule_theory}. Πρακτικά, αποτελεί ένα συνελικτικό επίπεδο με 32 κανάλια από 8\en{D} κάψουλες\textendash διανύσματα. Δηλαδή, κάθε κάψουλα περιέχει 8 μονάδες συνέλιξης με πυρήνες μεγέθους $9 \times 9$ και βήμα 2. Κατά αυτόν τον τρόπο, κάθε κάψουλα \textquote{βλέπει} $256 \times 9 \times 9$ στοιχεία (μονάδες) από τους χάρτες χαρακτηριστικών του προηγούμενου επιπέδου.\par

Όπως φαίνεται και από το σχετικό σχήμα, με αυτό το επίπεδο σχηματίζεται ένα πλέγμα από $32 \times 6 \times 6$ κάψουλες. Να σημειώσουμε ότι επειδή το επίπεδο \en{PrimaryCaps} αποτελεί ένα συνελικτικό επίπεδο από κάψουλες, τα βάρη των πυρήνων (κυλιόμενων παραθύρων) σε κάθε πλαίσιο στους άξονες $x-y$ $6 \times 6$ διαμοιράζονται. Η ειδοποιός διαφορά με ένα συνελικτικό επίπεδο χωρίς κάψουλες είναι στην συνάρτηση ενεργοποίησης. Αυτή αφενός είναι μια συνάρτηση σύνθλιψης (\en{squashing function}) που θα ορίσουμε στη συνέχεια και αφετέρου εφαρμόζεται σε ομάδες 8 στοιχείων τη φορά (δηλαδή σε κάθε 8\en{D} κάψουλα). Συνεπώς, το επίπεδο \en{PrimaryCaps} μπορεί να θεωρηθεί ως απλό συνελικτικό επίπεδο με $32 \times 8$ κανάλια στο οποίο, ανά ομάδες των 8 στοιχείων στη διάσταση βάθους, εφαρμόζεται μη γραμμικότητα τύπου \textquote{μπλοκ} (\en{block non\textendash linearity}).\par

Το τελευταίο επίπεδο είναι το \en{DigitCaps} το οποίο έχει σαν έξοδο (συνήθως 10 από) 16\en{D} κάψουλες\textendash διανύσματα των οποίων το μήκος ($L_2$ νόρμα), όπως έχουμε προαναφέρει, χρησιμοποιείται για τον εντοπισμό της κλάσης πρόβλεψης. Ανάμεσα στα πλήρως διασυνδεδεμένα επίπεδα \en{PrimaryCaps} και \en{DigitCaps} λαμβάνει χώρα ο αλγόριθμος δρομολόγησης μέσω συμφωνίας που αναλύουμε παρακάτω. Όπως φαίνεται και από το σχήμα, μεταξύ των δύο επιπέδων παρεμβάλλονται και μια σειρά από πίνακες βαρών ($6\times 6 \times 32 \times 10$ τέτοιοι πίνακες διάστασης $8 \times 16$) που μετασχηματίζουν τις τιμές της κάθε 8\en{D} κάψουλας του επιπέδου \en{PrimaryCaps} σε 16\en{D} ψήφους, μια για κάθε κάψουλα γονέα (άρα 10 ψήφοι αντιστοιχούν σε κάθε κάψουλα επιπέδου \en{PrimaryCaps}). Επισημαίνουμε ότι οι πίνακες βαρών τροποποιούνται κατά την εκπαίδευση και δεν εξαρτώνται από το εκάστοτε μεμονωμένο παράδειγμα (αποθηκεύουν πληροφορία μεταξύ μερών \textendash όλου που είναι ανεξάρτητη από την οπτική γωνία).\par

\subsubsection{Ανακατασκευή}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/chapter method/first_method_decoder.pdf}
    \caption{Η αρχιτεκτονική του αποκωδικοποιητή που διευκολύνει την εκπαίδευση του νευρωνικού δικτύου με κάψουλες. \textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
    \label{fig:method_1_decoder_architecture}
  \end{figure}

Προκειμένου να ενθαρρυνθεί η σύλληψη των παραμέτρων στιγμιότυπου του κάθε ψηφίου από το αντίστοιχο διάνυσμα \en{DigitCaps} χρησιμοποιείται ένας αποκωδικοποιητής του σχήματος \ref{fig:method_1_decoder_architecture}. Αυτός δομείται από 3 πλήρως διασυνδεδεμένα επίπεδα με αριθμό νευρώνων $160 \times 512, 512\times 1024$ και $ 1024 \times 28 \times 28 $ αντίστοιχα. Να σημειώσουμε ότι μεταξύ του επιπέδου \en{DigitCaps} και του πρώτου πλήρως διασυνδεδεμένου επιπέδου του αποκωδικοποιητή παρεμβάλλεται μια μάσκα που σκοπό έχει να μηδενίσει όλες τις κάψουλες εξόδου παρά αυτήν που σχετίζεται με το ψηφίο\textendash στόχο (\en{target}) κατά τη διάρκεια εκπαίδευσης ή αυτή που έχει το μεγαλύτερο μήκος κατά τη διάρκεια ελέγχου. \footnote{Όπως έχει γίνει σαφές, ο αύξοντας αριθμός της κάψουλας με τη μεγαλύτερη $\mathcal{L}_2$ νόρμα (μήκος) είναι και η κλάση πρόβλεψης $\hat{y}$.} Έτσι, ο αποκωδικοποιητής δέχεται σαν είσοδο μόνο ένα διάνυσμα (\en{DigitCap}) και έχει ως έξοδο μια εικόνα $28 \times 28$ που μέσω εκπαίδευσης επιδιώκεται να είναι όσο πιο πιστό αντίγραφο γίνεται της εικόνας εισόδου.\par

\subsection{Συνάρτηση Σύνθλιψης}

Όπως έχουμε αναφέρει, τόσο για τον υπολογισμό των \en{PrimaryCaps} όσο και για τον υπολογισμό των \en{DigitCaps} μέσω του αλγορίθμου δρομολόγησης, χρησιμοποιείται η συνάρτηση σύνθλιψης (\en{squashing function}). Πρόκειται για μια μη γραμμική συνάρτηση που δέχεται ένα διάνυσμα τυχαίου μήκους και εγγυάται ότι στην έξοδό της, το μήκος του διανύσματος θα ανήκει στο διάστημα $(0,1)$. Με αυτόν τον τρόπο, το μήκος του διανύσματος μοντελοποιεί την πιθανότητα ενεργοποίησης της εκάστοτε κάψουλας.\par

Με μαθηματικούς όρους, έχουμε:
\begin{equation}
    squash(x) = \frac{\left\lVert x\right\rVert^2}{1+\left\lVert x\right\rVert^2} \frac{x}{\left\lVert x\right\rVert}
\end{equation}

\subsection{Δυναμικός Αλγόριθμος Δρομολόγησης μέσω Συμφωνίας}

Ο αλγόριθμος μέσω συμφωνίας λαμβάνει χώρα μεταξύ δύο επιπέδων από κάψουλες και όπως έχουμε εξηγήσει αναλαμβάνει τον ρόλο της ανάθεσης μερών των αντικειμένων σε ένα από τα αντικείμενα στόχους. Πιο αναλυτικά, η διαδικασία που λαμβάνει χώρα για τον σχηματισμό των καψουλών γονέων από τις κάψουλες παιδιά είναι η εξής:
\begin{enumerate}
    \item Από την κάθε μια κάψουλα παιδί (έστω $u_i$) παράγονται τόσοι ψήφοι ($\hat{u}_{j|i}$) όσες και οι κάψουλες γονείς. Οι ψήφοι αποτελούν προβλέψεις της πόζας της εκάστοτε κάψουλας γονέα και υπολογίζονται πολλαπλασιάζοντας την κάψουλα παιδί με τον πίνακα βαρών $W_{ij}$ που συνδέει το τμήμα του αντικειμένου $i$ με την οντότητα που αναπαριστά η κάψουλα $j$. Έτσι έχουμε: 
    \begin{equation}
    \hat{u}_{j|i} = W_{ij} u_i
    \end{equation}
    \item Μέσω του αλγορίθμου δρομολόγησης που θα εξετάσουμε στη συνέχεια, υπολογίζονται τα διανύσματα των καψουλών γονέων ($s_j$) ως σταθμισμένα αθροίσματα των ψήφων παιδιών, δηλαδή:
    \begin{equation}
    s_j = \sum_i c_{ij} \hat{u}_{j|i}
    \end{equation}
    όπου τα $c_{ij}$ είναι οι παράμετροι σύζευξης (\en{coupling coefficients}). Αυτές υπολογίζονται σύμφωνα με τη συνάρτηση \en{softmax}, δηλαδή:
    \begin{equation}
    c_{ij} = \frac{exp(b_{ij})}{\sum_k exp(b_{ik})}
    \end{equation}
    ανάλογα με τα $b_{ij}$ (\en{log priors}) που τροποποιούνται σε κάθε επανάληψη του αλγορίθμου.
\end{enumerate}

Πλέον, είμαστε σε θέση να παρουσιάσουμε τον δυναμικό αλγόριθμο δρομολόγησης μέσω συμφωνίας αναλυτικά. Αυτός, όπως έχει γίνει σαφές, λαμβάνει μέρος αμέσως μετά τον υπολογισμό των ψήφων.

\algnewcommand{\Initialize_gr}[1]{%
  \State \textbf{\gr{Αρχικοποίηση:}}
  \State \hspace{\algorithmicindent }\parbox[t]{.8\linewidth}{\raggedright #1}
}

\algnewcommand{\Initializee}[1]{%
  \State \textbf{Initialize:}
  \State \hspace{\algorithmicindent }\parbox[t]{.8\linewidth}{\raggedright #1}
}

\en{
\begin{algorithm}[H]
    \caption{\gr{Δυναμικός Αλγόριθμος Δρομολόγησης μέσω Συμφωνίας}}\label{alg:dynam_routing}
    \begin{algorithmic}[1]
        \Function{Routing}{$\hat{u}_{j|i}, r, l$}
        \Initialize_gr{$\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και κάψουλα} $j \in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets 0$}
            \For{$r$ \gr{επαναλήψεις}}
            \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$: $c_i \gets softmax(b_i)$
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $s_j \gets \sum_i c_{ij}\hat{u}_{j|i}$
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $v_j \gets squash(s_j)$
            \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και} \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets b_{ij} + \hat{u}_{j|i} \times v_j$
            \EndFor
            \State \Return $v_j$
        \EndFunction
    \end{algorithmic}
    \end{algorithm}
}

\subsubsection{Μερικές Σημειώσεις Σχετικά με τον Αλγόριθμο Δρομολόγησης}

Προκειμένου να διευκολύνουμε τον αναγνώστη στη διαισθητική κατανόηση αλλά και πρακτική υλοποίηση του αλγορίθμου \ref{alg:dynam_routing} κάνουμε τις εξής παρατηρήσεις:
\begin{itemize}
    \item Σε κάθε κάψουλα γονέα (\en{DigitCaps}) αντιστοιχούν τόσες ψήφοι όσες είναι και οι κάψουλες παιδιά (\en{PrimaryCaps}). Επειδή κάθε ψήφος παράγεται από έναν μοναδικό πίνακα βαρών $W_{ij}$, γίνεται αντιληπτό ότι κάθε γονέας \textquote{βλέπει} διαφορετικές ψήφους.
    \item Αρχικοποιώντας τα $b_{ij}$ με την τιμή $0$, στην πρώτη επανάληψη του αλγορίθμου δρομολόγησης διαμορφώνονται τα διανύσματα των καψουλών γονέων ($s_j$) ως οι μέσοι όροι των ψήφων που τους αντιστοιχούν.
    \item Σε κάθε επανάληψη, κάθε κάψουλα παιδί $i$ τροποποιεί τα βάρη δρομολόγησής της (\en{coupling coefficients}) $c_{ij}$ ανάλογα με το πόσο συμφωνεί η πρόβλεψή του ($\hat{u}_{j|i}$) με το διάνυσμα $s_j$ της κάθε κάψουλας γονέα. Η συμφωνία αυτή εκτιμάται με τη χρήση του εσωτερικού γινομένου $\hat{u}_{j|i} \times v_j$.
    \item Όταν το διάνυσμα πρόβλεψης $\hat{u}_{j|i}$ παρουσιάζει μεγάλη συμφωνία με το διάνυσμα $s_j$, η ποσότητα $c_{ij}$ θα αυξηθεί, προκαλώντας (πιθανότατα) ακόμα μεγαλύτερη συμφωνία στην επόμενη επανάληψη. Αναπόφευκτα, λόγω του περιορισμού $\sum_j c_{ij} = 1$, αν μια κάψουλα γονέας (έστω $k$) παρουσιάζει μικρή συμφωνία με την αντίσοιχη πρόβλεψη $\hat{u}_{k|i}$ της κάψουλας $i$, η ποσότητα $c_{ik}$ θα μειωθεί. Συνεπώς, η συγκεκριμένη κάψουλα παιδί $i$ θα έχει μικρό ποσοστό στο μερίδιο διαμόρφωσης του διανύσματος $s_k$.
    \item Κάψουλες για τις οποίες πολλές ψήφοι συμφωνούν καταλήγουν μετά από λίγες επαναλήψεις του αλγορίθμου δρομολόγησης (συνήθως 3) να έχουν διανύσματα με μεγάλο μήκος (και άρα μεγάλη πιθανότητα). Αντίθετα, κάψουλες γονείς που έχουν μικρή συμφωνία με τις προβλέψεις των παιδιών, λαμβάνουν μικρά σε μήκος διανύσματα (λόγω των μικρών $c_{ij}$) που αντικρούουν το ένα το άλλο (\en{cancel each other out}) κατά την πράξη του σταθμισμένου αθροίσματος.
    \item Πρακτικά, επειδή η κάθε κάψουλα γονέα $j$ διαμορφώνεται από τις αντίστοιχες ψήφους των καψουλών παιδιών, μεγάλη συμφωνία μεταξύ αυτών προκύπτει από μεγάλη συμφωνία των παιδιών στον χώρο των ψήφων (για τον γονέα $j$). Με άλλα λόγια, η ύπαρξη μιας πυκνής συστάδας (\en{cluster}) από ψήφους για την πόζα ενός γονέα $j$ συνεπάγεται ότι το διάνυσμά του γονέα $s_j$ θα έχει μεγάλο μέτρο\footnote{Με εξαίρεση αν, λόγω ανταγωνισμού, υπάρχουν πολλές, πυκνότερες συστάδες που αντιστοιχούν σε άλλες κάψουλες γονείς.}. Με αυτήν την παρατήρηση, ο όρος \textquote{φιλτραρίσματος μέσω της πολυδιάστατης σύμπτωσης} (\en{high dimensional coincidence filtering}) γίνεται πλήρως κατανοητός.
\end{itemize}


\subsection{Συνάρτηση Σφάλματος}
\label{sec:method1_loss_fn}
Για την εκπαίδευση του νευρωνικού δικτύου με κάψουλες εισάγεται η συνάρτηση \textquote{Απώλειας Περιθωρίου} (\en{Margin Loss}). Μέσω αυτής της συνάρτησης, κατά την εκπαίδευση ενθαρρύνεται η προσαρμογή των βαρών του δικτύου ώστε το διάνυσμα \en{DigitCap} που αναπαριστά την εκάστοτε κλάση πρόβλεψης $\hat{y}$ να έχει μεγάλο μήκος ενώ τα άλλα διανύσματα που αντιστοιχούν στις υπόλοιπες κλάσεις που δεν εντοπίζονται στην εικόνα εισόδου να έχουν (σχεδόν) μηδενικό μήκος. Επιπλέον, επιτρέπει την εκπαίδευση σε περιπτώσεις όπου στην εικόνα εισόδου απαντώνται περισσότερα του ενός αντικείμενα που ανήκουν σε διαφορετικές κλάσεις. Αναλυτικότερα, η συνάρτηση σφάλματος ορίζεται ξεχωριστά για κάθε κάψουλα \en{DigitCap} $k$ ως εξής:
\begin{equation}
    L_k = T_k max(0, m^+ - \left\lVert v_k\right\rVert)^2 + \lambda (1-T_k) max(0, \left\lVert v_k\right\rVert - m^-)^2
\end{equation}
όπου $T_k = 1$ αν και μόνο αν το αντικείμενο κλάσης $k$ βρίσκεται στην εικόνα εισόδου. Επίσης, συνήθως είναι $m^+ = 0.9$ και $m^- = 0.1$ ενώ η τιμή της παραμέτρου λ είναι $\lambda = 0.5$. Η συνάρτηση σφάλματος προκύπτει ως: \begin{equation}
  \mathcal{L} = \sum_k L_k.
\end{equation}

Σε περίπτωση που χρησιμοποιείται και το πλήρως διασυνδεδεμένο δίκτυο αποκωδικοποιητή που έχει ως έξοδο προβλέψεις (σε μορφή διανύσματος) $\hat{x}$ τότε έχουμε: 
\begin{equation}
\mathcal{L} = \sum_k L_k + \lambda_{rec} \left\lVert x - \hat{x}\right\rVert^2 
\end{equation}
όπου $\lambda_{rec} = 0.0005$ ο όρος που μειώνει τη βαρύτητα του σφάλματος ανακατασκευής.

\subsection{Παραλλαγές Δυναμικού Αλγορίθμου Δρομολόγησης}

Στο πειραματικό μέρος που ακολουθεί το παρόν κεφάλαιο, πειραματιστήκαμε με ορισμένες παραλλαγές του αλγορίθμου αυτού προκειμένου να εξετάσουμε τη σημασία του αλγορίθμου δρομολόγησης. Πιο αναλυτικά, κατασκευάσαμε σε αδρές γραμμές δύο νέους αλγορίθμους που δε χρησιμοποιούν τον επαναληπτικό αλγόριθμο δρομολόγησης αλλά χρησιμοποιούν έναν παραπλήσιο, ταχύ αλγόριθμο δρομολόγησης. Τους αλγορίθμους αυτούς τους ονομάζουμε \en{Argmax Rooting} και \en{Max Rooting}. Τους παρουσιάζουμε παρακάτω με την σειρά που τους αναφέραμε.

\subsubsection{Αλγόριθμος \en{Argmax Scaled Rooting} και \en{Argmax Rooting}}

Ο παρόν αλγόριθμος σκοπό έχει να εξετάσει αν υψηλές επιδόσεις επιτυγχάνονται με την επιλογή μόνο μιας κάψουλας παιδί (κάψουλας επιπέδου $l$) για δρομολόγηση στην εκάστοτε κάψουλα του επόμενου επιπέδου. Όπως θα φανεί από τα πειραματικά αποτελέσματα, δεν απαιτείται ο γραμμικός συνδυασμός των ψήφων για τη συγκρότηση των διανυσμάτων του επόμενου επιπέδου. Η επιλογή ενός \textquote{εκπρόσωπου} για κάθε κάψουλα επιπέδου $(l+1)$ με κριτήριο το ποια ψήφος για την κάψουλα έχει το μεγαλύτερο βάρος δρομολόγησης θα δούμε στη συνέχεια πως είναι μια αποδοτική μέθοδος δρομολόγησης.\par

Οι δύο αλγόριθμοι που παρουσιάζονται παρακάτω εμφανίζουν, με μια πρώτη ματιά, ελάχιστες διαφορές. Η διαφορά τους έγκειται στο ότι ενώ ο πρώτος αλγόριθμος, κλιμακώνει τις επιλεχθέντες ψήφους (ψήφους \textquote{εκπρόσωποι} της εκάστοτε κάψουλας επιπέδου $l+1$) με τα αντίστοιχα βάρη δρομολόγησης και ύστερα εφαρμόζει τη συνάρτηση \en{squash}, ο δεύτερος απλά έχει σαν έξοδο τις μη\textendash κλιμακωμένες, επιλεχθέντες ψήφους. \par

Αναφορικά με τον αλγόριθμο \ref{alg:dynam_argmax_scaled_routing}, είναι σημαντικό να τονίσουμε ότι δεν αναιρεί τις βασικές υποθέσεις των νευρωνικών δικτύων από κάψουλες. Και πάλι χρησιμοποιούνται ορισμένες επαναλήψεις του δυναμικού αλγορίθμου προκειμένου να πραγματοποιηθεί η πολυδιάστατη συμφωνία (\en{high\textendash dimensional coincidence filtering}). Αυτή τη φορά όμως, ο βαθμός συμφωνίας ενσωματώνεται στα βάρη δρομολόγησης (τα οποία και τελικά κλιμακώνουν τις κάψουλες επιπέδου $(l+1)$). Σε τελική ανάλυση, οι ψήφοι που συμφωνούν για την πόζα μιας κάψουλας επόμενου επιπέδου θα έχουν (εν γένει) μεγαλύτερα βάρη δρομολόγησης λόγω του ανταγονισμού που υπάρχει μεταξύ των καψουλών γονέων. Έτσι, κλιμακώνοντας τις τελικές κάψουλες με τα βάρη δρομολόγησης, αυξάνουμε το μήκος των διανυσμάτων που εκπροσωπούν κάψουλες επιπέδου $(l+1)$ στις οποίες υπήρξε μεγάλη συμφωνία ψήφων ενώ προκαλούμε το αντίθετο αποτέλεσμα σε κάψουλες με χαμηλή συμφωνία.\par

\en{
\begin{algorithm}[H]
    \caption{\gr{Αλγόριθμος} \en{Argmax Scaled Rooting}}\label{alg:dynam_argmax_scaled_routing}
    \begin{algorithmic}[1]
        \Function{Argmax Scaled Routing}{$\hat{u}_{j|i}, r, l$}
        \Initialize_gr{$\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και κάψουλα} $j \in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets 0$}
            \For{$r$ \gr{επαναλήψεις}}
              \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$: $c_i \gets softmax(b_i)$
              \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $s_j \gets \sum_i c_{ij}\hat{u}_{j|i}$
              \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $v_j \gets squash(s_j)$
              \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και} \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets b_{ij} + \hat{u}_{j|i} \times v_j$
            \EndFor
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $i_{indices} \gets \underset{i}{\mathrm{argmax}}(c_{ij})$
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $s_j \gets c_{i=i_{indices}j} \hat{u}_{j|i=i_{indices}}$
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $v_j \gets squash(s_j)$
            \State \Return $v_j$
        \EndFunction
    \end{algorithmic}
    \end{algorithm}
}

Αναφορικά με τον αλγόριθμο \ref{alg:dynam_argmax_routing} πρόκειται για την απλούστερη μορφή του αλγορίθμου \en{argmax} που δεν περιλαμβάνει την κλιμάκωση των καψουλών γονέων. Αυτό έχει σαν αποτέλεσμα, το μήκος των καψουλών επιπέδου $(l+1)$ να διαμορφώνεται αποκλειστικά από τα εκπαιδευόμενα βάρη του δικτύου. Κατά αυτόν τον τρόπο και σε αντίθεση με τον προηγούμενο αλγόριθμο, το φιλτράρισμα μέσω υψηλής διαστατικότητας συμπτώσεις (\en{high\textendash dimensional coincidence filtering}) δε διαδραματίζει κάποιο ρόλο στη διαμόρφωση του μήκους των καψουλών εξόδου. Με άλλα λόγια, αν και ο αλγόριθμος αυτός περιλαμβάνει σημαντικό μέρος του δυναμικού αλγορίθμου, τη διαμόρφωση των καψουλών εξόδου την αναλαμβάνουν ως επι το πλείστον οι πίνακες μετασχηματισμού $W$. Έτσι, γίνεται πιο εμφανής ο βαθμός συμβολής του αρχικού αλγορίθμου δρομολόγησης στις επιδόσεις του δικτύου.

\en{
\begin{algorithm}[H]
    \caption{\gr{Αλγόριθμος} \en{Argmax Rooting}}\label{alg:dynam_argmax_routing}
    \begin{algorithmic}[1]
        \Function{Argmax Routing}{$\hat{u}_{j|i}, r, l$}
        \Initialize_gr{$\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και κάψουλα} $j \in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets 0$}
            \For{$r$ \gr{επαναλήψεις}}
              \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$: $c_i \gets softmax(b_i)$
              \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $s_j \gets \sum_i c_{ij}\hat{u}_{j|i}$
              \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $v_j \gets squash(s_j)$
              \State $\forall$ \gr{κάψουλα} $i$ $\in$ \gr{επίπεδο} $l$ \gr{και} \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $b_{ij} \gets b_{ij} + \hat{u}_{j|i} \times v_j$
            \EndFor
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $i_{indices} \gets \underset{i}{\mathrm{argmax}}(c_{ij})$
            \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $s_j \gets \hat{u}_{j|i=i_{indices}}$
            \State \Return $v_j$
        \EndFunction
    \end{algorithmic}
    \end{algorithm}
}

\subsubsection{Αλγόριθμος \en{Max Rooting}}
Ο αλγόριθμος αυτός αποτελεί μια ακραία εκδοχή του αλγορίθμου δρομολόγησης με μηδενικό αριθμό επαναλήψεων. Μέσω αυτού, εξετάζουμε την περίπτωση όπου ο δυναμικός, επαναληπτικός αλγόριθμος δρομολόγησης (Αλγόριθμος \ref{alg:dynam_routing}) δεν έχει κανένα ρόλο στη διαμόρφωση των καψουλών επιπέδου $(l+1)$. Συγκρίνοντας τις επιδόσεις, μπορούμε να κρίνουμε την απόδοση του αργού, δυναμικού αλγορίθμου \ref{alg:dynam_routing} σε εργασίες ταξινόμησης. Ο αλγόριθμος που εξετάζουμε, απλώς επιλέγει από τις ψήφους της κάθε κάψουλας επιπέδου $(l+1)$ την κάψουλα με το μεγαλύτερο μήκος και δρομολογεί αυτήν στο επόμενο επίπεδο.

\en{
\begin{algorithm}[H]
    \caption{\gr{Αλγόριθμος} \en{Max Rooting}}\label{alg:dynam_max_routing}
    \begin{algorithmic}[1]
        \Function{Max Routing}{$\hat{u}_{j|i}, l$}
          \State $\forall$ \gr{κάψουλα} $j$ $\in$ \gr{επίπεδο} $(l+1)$: $v_j \gets max_i(\hat{u}_{j|i})$ \label{op:special_max_method1}
          \State \Return $v_j$
        \EndFunction
    \end{algorithmic}
    \end{algorithm}
}
Σημειώνουμε ότι στην γραμμή \ref{op:special_max_method1} του αλγορίθμου \ref{alg:dynam_argmax_routing} το κριτήριο για την επιλογή του μεγίστου (ένα μέγιστο για κάθε $j$) είναι η $L_2$ νόρμα των διανυσμάτων $\hat{u}_{j|i}$.

\section{\en{Matrix Capsules with EM Routing}}

Η μέθοδος αυτή χρησιμοποιείται σε ορισμένα πειράματα της διπλωματικής προκειμένου να δοκιμαστεί αν ορισμένες βασικές υποθέσεις των νευρωνικών δικτύων με κάψουλες διατηρούνται στη νεότερη υλοποίηση της τεχνολογίας από την ομάδα του \en{G. Hinton}. Αν και οι δύο αρχικές μέθοδοι που περιγράφουμε δε διαφέρουν στη θεωρία που έχουμε περιγράψει, η πιο καινούρια υλοποίηση παρουσιάζει τεχνικές βελτιώσεις με τις οποίες επιτυγχάνεται αυξημένη ακρίβεια στα πειραματικά αποτελέσματα. Οι πιο βασικές από αυτές τις βελτιώσεις είναι οι εξής:
\begin{itemize}
  \item Η αντικατάσταση του δυναμικού αλγορίθμου δρομολόγησης με τον αλγόριθμο δρομολόγησης βασισμένο στη μεγιστοποίηση προσδοκιών (\en{EM}). Κατά τη δρομολόγηση των ψήφων του ενός επιπέδου ($l$) στις κάψουλες του επόμενου ($l=1$), κάθε κάψουλα γονέας μοντελοποιείται σαν μια Γκαουσιανή κατανομή (με $\mu \in \Re^{16} και \sigma \in \Re^{16}$) που προσαρμόζεται επαναλληπτικά για να \textquote{εξηγήσει} τις ψήφους\textendash σημεία (\en{datapoints}).
  \item Η αλλαγή του τρόπου αναπαράστασης της κάψουλας η οποία τώρα επιτελείται χρησιμοποιώντας έναν πίνακα $4\times 4$ για την πόζα $M_i$ και μια ξεχωριστή λογιστική μονάδα (\en{logistic unit}) $\alpha_i$ για την πιθανότητα ύπαρξης της, συνυφασμένης με την κάψουλα, οντότητας. Όπως είναι λογικό, οι ψήφοι $V_{ij}$ και πάλι προκύπτουν ως εξής: $V_{ij} = M_iW_{ij}, W_{ij} \in \Re^{4\times 4}$.
\end{itemize}

Δυστυχώς, η δημοσίευση του έργου \cite{hinton2018matrix} δε συνοδεύτηκε από κώδικα. Πολλοί ερευνητές επιδίωξαν να δημοσιεύσουν τη δική τους υλοποίηση αλλά τα πειραματικά αποτελέσματα ήταν πολύ πιο χαμηλά από αυτά που αναφέρονται στο σχετικό έργο. Η καλύτερη, ανοιχτού κώδικα υλοποίηση που εντοπίσαμε ακούει στο όνομα \en{Avoiding Implementation Pitfalls of "Matrix Capsules with EM Routing by Hinton et al."} \cite{gritzman2019avoiding}. Στην παρούσα διπλωματική εργασία, ακολουθούμε αυτήν την υλοποίηση (με μικρές αλλαγές) καθώς φαίνεται να είναι η πιο πιστή και επιτυχημένη υλοποίηση του δικτύου που περιγράφεται στο βασικό έργο. Μάλιστα, επιτυγχάνει ακρίβεια αποτελεσμάτων που είναι πολύ κοντά σε αυτήν που αναγράφεται στο \cite{hinton2018matrix}.\par

Σε αυτή την ενότητα, πρώτα θα παρουσιάσουμε την αρχιτεκτονική του νευρωνικού δικτύου με κάψουλες. Έπειτα θα παραθέσουμε τον νέο αλγόριθμο δρομολόγησης με συμφωνία μαζί με ορισμένες παρατηρήσεις. Τέλος, θα αναφερθούμε σε μερικές λεπτομέρειες υλοποίησης που αφορούν κυρίως τη διαδικασία δρομολόγησης μεταξύ διαδοχικών συνελικτικών επιπέδων από κάψουλες. Για περισσότερες λεπτομέρειες υλοποίησης, παραπέμπουμε τον αναγνώστη για άλλη μια φορά στην ιστοσελίδα όπου είναι αναρτημένος ο κώδικάς μας.

\subsection{Αρχιτεκτονική Νευρωνικού Δικτύου}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{images/chapter method/second_method_architecxture.pdf}
  \caption{Η αρχιτεκτονική του νευρωνικού δικτύου με κάψουλες της δεύτερης μεθόδου. \textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
  \label{fig:method_2_architecture}
\end{figure}

Στο σχήμα \ref{fig:method_2_architecture} παρατηρούμε τη γενική αρχιτεκτονική που χρησιμοποιείται σε αυτή τη μέθοδο. Κατά την περιγραφή της χρησιμοποιούμε τις μεταβλητές \en{A, B, C, D} για να αναφερθούμε σε παραμετροποιήσιμα χαρακτηριστικά του δικτύου. Όπως παρατηρούμε, όλα τα επίπεδα είναι συνελικτικά με εξαίρεση το τελευταίο, πλήρως διασυνδεδεμένο επίπεδο ενώ επίσης όλα αποτελούν επίπεδα από κάψουλες με εξαίρεση το πρώτο.\par

Ξεκινώντας από το πρώτο, πρόκειται για ένα απλό συνελικτικό επίπεδο με κυλιόμενο παράθυρο $5 \times 5$, βήμα 2 και μη γραμμική συνάρτηση ενεργοποίησης \en{ReLU}. Το επίπεδο αυτό απαρτίζεται από \en{A} τέτοια κυλιόμενα παράθυρα με αποτέλεσμα να παράγονται \en{A} χάρτες χαρακτηριστικών. Η διάσταση των χαρτών χαρακτηριστικών εξαρτάται από το μέγεθος της εικόνας εισόδου και μπορεί να υπολογιστεί από τις σχέσεις \ref{eq:width_conv} και  \ref{eq:height_conv}.\par

Συνεχίζοντας την ανάλυση του σχήματος από αριστερά προς τα δεξιά έχουμε το πρώτο επίπεδο από κάψουλες (\en{PrimaryCaps}). Το βάθος του επιπέδου αυτού είναι \en{B}. Το χαρακτηριστικό αυτό το ονομάζουμε και αριθμό των τύπων καψουλών (\en{number of capsule types}) και κάθε τύπος κάψουλας αναπαριστά (άρρητα) μια συγκεκριμένη οντότητα\footnote{Κάψουλες ίδιου τύπου μαθαίνουν να αναγνωρίζουν την ίδια οντότητα αλλά σε διαφορετικές περιοχές της εικόνας.}. Το επίπεδο \en{PrimaryCaps} προκύπτει από τους προαναφερθέντες χάρτες χαρακτηριστικών έπειτα από συνέλιξη με πυρήνα $1 \times 1$ και βήμα 1. Πιο συγκεκριμένα, ο πίνακας πόζας μιας κάψουλας (\en{PrimaryCap}) προκύπτει από ένα σύνολο $ A \times 4 \times 4$ εκπαιδευόμενων (\en{learned}) βαρών που επενεργούν πάνω στο σημείο του επιπέδου \en{Conv1} που αντιστοιχεί στη συγκεκριμένη κάψουλα. Ουσιαστικά, κάθε πεδίο του $4 \times 4$ πίνακα προκύπτει ως γραμμικός συνδυασμός των \en{A} στοιχείων του επιπέδου \en{Conv1} (ένα από κάθε κανάλι) βεβαρημένων από \en{A} εκπαιδευόμενες παραμέτρους. Η τιμή ενεργοποίησης της κάψουλας προκύπτει με παρόμοιο τρόπο (δηλαδή αποτελεί το γραμμικό συνδυασμό των ίδιων \en{A} στοιχείων του \en{Conv1} με τα \en{A} βάρη) αλλά αυτή τη φορά, στο αποτέλεσμα εφαρμόζεται η σιγμοειδής συνάρτηση (ώστε το αποτέλεσμα να ανήκει στο διάστημα [0,1]). Σημειώνουμε ότι για κάθε τύπο κάψουλας (δηλαδή για καθένα από τα \en{B} κανάλια από κάψουλες) τα βάρη $ A \times (4 \times 4 + 1) $ διαμοιράζονται.\par

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{images/chapter method/second_method_EM.pdf}
  \caption{Στο σχήμα παρατηρούμε τη διασύνδεση μεταξύ των καψουλών δύο διαδοχικών επιπέδων κατά τις φάσεις \en{M} και \en{E} του αλγορίθμου δρομολόγησης. Για λόγους απλότητας, απεικονίζεται η περίπτωση όπου έχουμε μονοδιάστατη συνέλιξη (\en{1D convolution}). \textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
  \label{fig:method_2_EM}
\end{figure}

Στη συνέχεια ακολουθούν τα επίπεδα \en{ConvCaps1} και \en{ConvCaps2}. Πρόκειται για δύο συνελικτικά επίπεδα από κάψουλες τα οποία έχουν \en{C} και \en{D} τύπους από κάψουλες, πυρήνες $3 \times 3$ και βήμα 2 και 1 αντίστοιχα. Ο σχηματισμός όμως των \en{ConvCaps1} και \en{ConvCaps2} δεν είναι προφανής. Πιο αναλυτικά, για τον σχηματισμό του καθενός τέτοιου επιπέδου λαμβάνει χώρα ο αλγόριθμος δρομολόγησης με συμφωνία βασιζόμενος στον \en{EM} που θα αναλύσουμε παρακάτω. Στο σημείο αυτό αξίζει να σημειωθεί ότι αντίθεση με δύο πλήρως διασυνδεδεμένα επίπεδα από κάψουλες, σε δύο συνελικτικά επίπεδα από κάψουλες οι γονείς (κάψουλες του επιπέδου $L + 1$) ανταγωνίζονται να διεκδικήσουν τις ψήφους των παιδιών (κάψουλες επιπέδου $L$) που ανήκουν σε μια γειτονιά ($K\times K = 3 \times 3$) του επιπέδου $L$ (κεντραρισμένη στην κάψουλα γονέα). Κατά αυτόν τον τρόπο, η εκάστοτε κάψουλα γονέας του επιπέδου $L + 1$ στέλνει ανατροφοδότηση (σύμφωνα με τον αλγόριθμο δρομολόγησης) μόνο στα παιδιά που ανήκουν στο οπτικό του πεδίο (βλ. σχήμα \ref{fig:method_2_EM}). Αντίστοιχα, τα παιδιά δεν παράγουν ψήφους για όλες τις κάψουλες του επόμενου επιπέδου αλλά μόνο για αυτές στων οποίων το οπτικό πεδίο ανήκουν. Επειδή μάλιστα οι πίνακες μετασχηματισμού $W_ij$ διαμοιράζονται στις χωρικές διαστάσεις $x-y$, για τον σχηματισμό του επιπέδου \en{ConvCaps1} απαιτούνται $K \times K \times B \times C \times 4 \times 4$ εκπαιδευόμενα (\en{learned}) βάρη (ανάλογα για το \en{ConvCaps2} απαιτούνται $K \times K \times C \times D \times 4 \times 4$). Αυτό το νούμερο προκύπτει λογικά αν αναλογιστεί κανείς ότι:
\begin{itemize}
  \item Ο κάθε πίνακας μετασχηματισμού έχει διάσταση $4 \times 4$. 
  \item Σε κάθε σημείο $i$ του πυρήνα (\en{kernel}) αντιστοιχεί ξεχωριστός πίνακας βαρών $Wij$, άρα έχουμε $K \times K$ τέτοιους πίνακες μεγέθους $4\times 4$.
  \item Το κυλιόμενο παράθυρο $K \times K$ έχει βάθος \en{B}, όσος και ο αριθμός των τύπων καψουλών εισόδου. Άρα είναι $B \times K \times K \times 4 \times 4$ βάρη.
  \item Θέλουμε \en{C} κανάλια εξόδου (ή αλλιώς τύπους από κάψουλες). Συνεπώς, έχουμε \en{C} κυλιόμενα παράθυρα.
\end{itemize}  

Το τελευταίο επίπεδο είναι το \en{Class Capsules} όπου κάθε κάψουλά του αντιστοιχεί σε μια κλάση. Η κάψουλα με τη μεγαλύτερη τιμή ενεργοποίησης (\en{logistic unit}) σηματοδοτεί την κλάση πρόβλεψης του νευρωνικού δικτύου όταν αυτό τροφοδοτείται με μια εικόνα ενός αντικειμένου. Τα περιεχόμενα των καψουλών στο τελευταίο επίπεδο σχηματίζονται με τον αλγόριθμο δρομολόγησης που παρουσιάζουμε παρακάτω. Επειδή τα δύο τελευταία επίπεδα είναι πλήρως διασυνδεδεμένα μεταξύ τους, κάθε κάψουλα του επιπέδου \en{ConvCaps2} παράγει μια ψήφο για κάθε κάψουλα του επιπέδου \en{Class Capsules}. Έτσι απαιτούνται $D \times E$ πίνακες μετασχηματισμού μεγέθους $4 \times 4$. Για να μη χαθεί η πληροφορία της θέσης (στον $x - y$ άξονα) στην οποία εντοπίζεται το κάθε αντικείμενο\textendash μέρος, μετά τον πολλαπλασιασμό της πόζας της κάθε κάψουλας με το κατάλληλο πίνακα μετασχηματισμού, στα πρώτα δύο κελιά της δεξιότερης στήλης του πίνακα ψήφου υπερτίθενται η γραμμή και η στήλη της κάψουλας από την οποία προήλθε (η ψήφος).\par

Όπως εξηγήσαμε, ακολουθούμε μια ελαφρός τροποποιημένη έκδοση της αρχιτεκτονικής που παρουσιάζεται στο \cite{hinton2018matrix} (βλ. πίνακα \ref{tab:method2_params_ABCD}). Η διαφορά έγκειται ότι η υλοποίηση που χρησιμοποιούμε (ακολουθώντας τις παρατηρήσεις και τον κώδικα του \cite{gritzman2019avoiding}) έχει συνήθως μικρότερο βάθος επιπέδων, χωρίς αυτό να είναι ο καθοριστικός παράγοντας της ελάχιστα χαμηλότερης απόδοσης που παρατηρείται όταν αντιπαραβάλλεται με τα αποτελέσματα της αυθεντικής υλοποίησης.
\begin{table}[h]
\begin{center}
  \begin{tabular}{| c | c c |} 
   \hline
   Παράμετρος & Αυθεντική Υλοποίηση & Απλοποιημένη \\ [0.5ex] 
   \hline\hline
   \en{A} & 32 & 64 \\ 
   \hline
   \en{B} & 32 & 8 \\
   \hline
   \en{C} & 32 & 16 \\
   \hline
   \en{D} & 32 & 16 \\ [1ex] 
   \hline
  \end{tabular}
  \caption{\label{tab:method2_params_ABCD}Πίνακας στον οποίο παρουσιάζεται η παραμετροποίηση της αρχιτεκτονικής όπως παρουσιάζεται στο έργο \cite{hinton2018matrix} (αυθεντική υλοποίηση) και όπως παρουσιάζεται στο \cite{gritzman2019avoiding} (απλοποιημένη). Η παράμετρος \en{E} είναι πάντα ίση με τον αριθμό των κλάσεων εξόδου.}
  \end{center}
\end{table}

Κλείνοντας την παράγραφο αυτή, να επισημάνουμε ότι για λόγους σύγκρισης με το έργο \cite{sabour2017dynamic} υπάρχει η επιλογή να χρησιμοποιηθεί η αρχιτεκτονική που παρουσιάσαμε στην εικόνα \ref{fig:method_1_architecture} μέχρι και το επίπεο των \en{Primary Capsules}.

\subsection{Υπολογισμός Τιμής Ενεργοποίησης}

Κατά αντιστοιχία με την προηγούμενη μέθοδο, οι κάψουλες παιδιά ψηφίζουν για το ποια εκτιμούν ότι είναι η πόζα του κάθε γονέα που \textquote{βλέπουν}. Σε αυτούς τους γονείς δρομολογούν τις ψήφους με βαρύτητα που υποδεικνύεται από τις πιθανότητες ανάθεσης (\en{assignment probabilities}) - συμβολίζονται με $R_{ij}$ και διαμορφώνονται δυναμικά από τον νέο αλγόριθμο δρομολόγησης. Προτού περιγράψουμε το νέο αλγόριθμο δρομολόγησης, κρίνεται σκόπιμο να εξηγήσουμε τον μηχανισμό με τον οποίο εκτιμάται αν θα ενεργοποιηθεί μια κάψουλα γονέας ή όχι (τιμή $\alpha_j$). Η λογική που ακολουθείται για τον σκοπό αυτό είναι εμπνευσμένη από την αρχή του ελαχίστου μήκους περιγραφής (\en{minimum description length principle}).\par

Πιο αναλυτικά, μέσα από τον αλγόριθμο δρομολόγησης που θα παρουσιάσουμε παρακάτω επιλέγεται κατά πόσον θα ενεργοποιηθεί ή όχι μια κάψουλα γονέας ανάλογα με το ποια κατάσταση ελαχιστοποιεί το κόστος περιγραφής\footnote{Φυσικά, η τιμή ενεργοποίησης $a_j$ λαμβάνει τιμές στο διάστημα [0,1] οπότε τυπικά υπάρχουν άπειρες ενδιάμεσες καταστάσεις.}. Για τον σχηματισμό του, λαμβάνονται υπόψη τα εξής:
\begin{itemize}
  \item Στην περίπτωση που δεν ενεργοποιηθεί μια κάψουλα γονέας, το κόστος προκύπτει από την τιμή $-\beta_u$ που πρέπει να \textquote{πληρώσουμε} για κάθε κάψουλα\textendash παιδί που αναθέτεται στον συγκεκριμένο γονέα (έστω $j$). Στη συνήθη περίπτωση που υπάρχει μερική ανάθεση μεταξύ παιδιού\textendash πατέρα ($R_{ij} \neq 1$), στο κόστος προστίθεται η ποσότητα $-\beta_u$ βεβαρημένη με το κλάσμα ανάθεσης $R_{ij}$. Συνεπώς, το κόστος για μια πλήρως απενεργοποιημένη κάψουλα $j$ θα είναι ίσο με:
  \begin{equation}
    -\beta_u \sum_i R_{ij}, \text{ όπου } i \in FoV(j) 
  \end{equation}
  \item Στην περίπτωση ενεργοποίησης της κάψουλας γονέα $j$ τότε \textquote{πληρώνουμε} πρώτα ένα κόστος $-\beta_{\alpha}$ που οφείλεται στην κωδικοποίηση των παραμέτρων που σχετίζονται με την κάψουλα γονέα. Επίσης, στο κόστος αυτό προστίθενται τα κόστη περιγραφής των διαφορών μεταξύ των ψήφων και της πόζας της κάψουλας $j$ στην οποία αναθέτονται (ζυγισμένα υπό τις πιθανότητες ανάθεσης). Η τελευταία ποσότητα μπορεί να προσεγγιστεί ως εξής:
  \begin{equation}
    cost_j = \sum_h cost_j^h = \sum_h \sum_i -R_{ij}ln(P^h_{i|j}),
  \end{equation} όπου ο δείκτης $h$ αναφέρεται στην $h$-οστή διάσταση του διανύσματος βάσης.
  Προς επεξήγηση της ανωτέρω σχέσης, να αναφέρουμε ότι ουσιαστικά, η ποσότητα $-R_{ij}ln(P^h_{i|j})$ περιγράφει το κόστος της διαφοράς της ψήφου $i$ και της πόζας της κάψουλας $j$ για τη διάσταση $h \in [1,4\times 4]$. Υπενθυμίζουμε, στο σημείο αυτό ότι οι ψήφοι $V_{ij} \in \Re^{4 \times 4}$ μοντελοποιούνται σαν σημεία στον χώρο $\Re^16$ και οι κάψουλες $M_j$ σαν Γκαουσιανές κατανομές με μέση τιμή $\mu_j \in \Re^{16}$ και διαγώνιο πίνακα συνδιακύμανσης $\varSigma$ με στοιχεία διαγωνίου που συγκροτούν το διάνυσμα $\sigma \in \Re^{16}$. Έτσι, μπορούμε να προσεγγίσουμε το κόστος για την περιγραφή ενός σημείου (ψήφου) $V_{ij}$ από την κατανομή (κάψουλα γονέας) $j$ ως την αρνητική, λογαριθμική, πολυμεταβλητή, γκαουσιανή πυκνότητα πιθανότητας (\en{negative log multivariate gaussian probability density}) παραμετροποιημένη από τα $\mu_j$ και $\sigma_j$ της κάψουλας $j$ υπολογισμένη στο σημείο της ψήφου $V_{ij}$ από την κάψουλα $i$ (φυσικά το κόστος θα είναι ζυγισμένο από την πιθανότητα ανάθεσης $R_{ij}$). Έτσι με μαθηματικούς όρους, η γκαουσιανή Σ.Π.Π. (\en{PDF}) υπολογισμένη στο σημείο $V_{ij}$ για τη διάσταση $h$ είναι:
  \begin{equation}
    P^h_{i|j} = \frac{1}{\sqrt{2\pi(\sigma_j^h)^2}}exp(-\frac{(V_{ij}^h-\mu_j^h)^2}{2(\sigma_j^h)^2}).
  \end{equation}
  Έτσι, ο αρνητικός λογάριθμος της ανωτέρω ποσότητας είναι:
  \begin{equation}
    -ln(P^h_{i|j}) = \frac{(V_{ij}^h-\mu_j^h)^2}{2(\sigma_j^h)^2} + ln(\sigma_j^h) + \frac{ln(2\pi)}{2}.
  \end{equation}
  Επανερχόμενοι στο συνολικό κόστος για τη διάσταση $h$, έχουμε:

  \begin{align}
    cost_j^h &= \sum_i - R_{ij} ln(P^h_{i|j}) \\ &= \frac{\sum_i R_{ij}(V_{ij}^h - \mu_j^h)^2}{2(\sigma_j^h)^2} + (ln(\sigma_j^h) + \frac{ln(2\pi)}{2})\sum_i R_{ij} \\ &= \frac{\sum_i R_{ij}(\sigma_j^h)^2}{2(\sigma_j^h)^2} + (ln(\sigma_j^h) + \frac{ln(2\pi)}{2})\sum_i R_{ij}  \\ &= (ln(\sigma_j^h) + \frac{1}{2} + \frac{ln(2\pi)}{2})\sum_i R_{ij} 
  \end{align}
\end{itemize}

Συνολικά λοιπόν, η συνάρτηση ενεργοποίησης της κάψουλας $j$ προκύπτει από την αντιπαραβολή του κόστους ενεργοποίησης και του κόστους απενεργοποίησής της. Τον ρόλο αυτό τον αναλαμβάνει η σιγμοειδής συνάρτηση (\en{sigmoid function}). Συνεπώς, έχουμε:
\begin{equation}
  \alpha_j = sigmoid(\lambda(\beta_{\alpha} - \beta_u \sum_i R_{ij} - \sum_h cost_j^h)).
\end{equation}
Το $\lambda$ είναι μια παράμετρος ανάστροφης θερμοκρασίας. Στην αρχή η παράμετρος είναι ίση με τη μονάδα και αυξάνεται σε κάθε επανάληψη του αλγορίθμου δρομολόγησης κάνοντας την κλίση της λογιστικής συνάρτησης μεγαλύτερη και ωθώντας τις τιμές ενεργοποίησης να λάβουν πιο ακραίες τιμές (0 ή 1). Αν ενσωματώσουμε στο $cost_j^h$ τον όρο $\beta_u \sum_i R_{ij}$ τότε έχουμε:
\begin{equation}
  \alpha_j = sigmoid(\lambda(\beta_{\alpha} - \sum_h \acute{cost}_j^h))
\end{equation}
όπου τώρα το κόστος είναι:
\begin{equation}
  \label{eq:cost_acute}
  \acute{cost} _j^h = (\beta_u + ln(\sigma_j^h) + const)\sum_i R_{ij}.
\end{equation}
Κλείνοντας την υποενότητα αυτή, να σημειώσουμε ότι οι ποσότητες $\beta_u$ και $\beta_{\alpha}$ μαθαίνονται κατά τη διάρκεια εκπαίδευσης του αλγορίθμου. Αυτός είναι και ένας λόγος για τον οποίο ο σταθερός όρος στη σχέση \ref{eq:cost_acute} μπορεί να παραληφθεί.

\subsection{Αλγόριθμος Δρομολόγησης \en{EM}}

Όπως έχουμε προαναφέρει, μεταξύ δύο διαδοχικών επιπέδων από κάψουλες λαμβάνει χώρα ο αλγόριθμος δρομολόγησης βασισμένο στη Μεγιστοποίηση Προσδοκειών (\en{Expectation Maximization}) μέσω του οποίου οι τιμές ενεργοποίησης και οι πόζες των καψουλών γονέων υπολογίζονται επαναληπτικά. Κατά αντιστοιχία με τον αυθεντικό αλγόριθμο \en{EM}, η επαναληπτική διαδικασία αποτελείται από δύο βήματα τα οποία εναλλάσσονται διαδεχόμενα το ένα το άλλο. Στο βήμα \en{E} υπολογίζονται οι πιθανότητες ανάθεσης $R_{ij}$ για κάθε ζευγάρι από κάψουλες μεταξύ των δύο επιπέδων. Ο υπολογισμός γίνεται χρησιμοποιώντας τις μέσες τιμές, τις διακυμάνσεις και τις τιμές ενεργοποίησης των καψουλών γονέων. Στο βήμα \en{M} υπολογίζονται οι παράμετροι των Γκαουσιανών κατανομών (που μοντελοποιούν τις κάψουλες γονείς) με βάση τα ανανεωμένα $R_{ij}$. Έτσι, μετά από ορισμένο αριθμό επαναλήψεων, ο αλγόριθμος συγκλίνει με αποτέλεσμα οι ενεργές κάψουλες γονείς να λαμβάνουν και να περιγράφουν συστάδες από όμοιες ψήφους παιδιών. \par

Μια περισσότερο τυπική περιγραφή του αλγορίθμου δρομολόγησης παρατίθεται παρακάτω. Σημειώνουμε ότι με $\Omega_L$ συμβολίζουμε το σύνολο όλων των καψουλών επιπέδου $L$. Επίσης, για τη διάσταση $H$ του πίνακα πόζας όταν αναδιατάσσεται σε διάνυσμα ισχύει ότι $Η = 16$.

\en{
\begin{algorithm}[H]
  \caption{\gr{Αλγόριθμος Δρομολόγησης Βασισμένος στον} \en{EM}}\label{alg:em_routing}
  \begin{algorithmic}[1]
      \Procedure{EM Routing}{$\alpha, V$}
      \Initialize_gr{$\forall$ $i$ $\in$ $\Omega_L$, $j \in \Omega_{L+1}: R_{ij} \gets 1/\left\lvert \Omega_{L+1}\right\rvert $
      }
      \For{$t$ \gr{επαναλήψεις}}
      \State $\forall j \in \Omega_{L+1}: M-Step(\alpha, R, V, j)$
      \State $\forall i \in \Omega_{L}: E-Step(\mu, \sigma, \alpha, V, i)$
      \EndFor
      \EndProcedure
      \Procedure{M-Step}{$\alpha, R, V, j$} \Comment{\gr{Για μια κάψουλα υψηλότερου επιπέδου, $j$}}
        \State $\forall i \in \Omega_L: R_{ij} \gets R_{ij} \ast \alpha_i$
        \State $\forall h: \mu_j^h \gets \frac{\sum_i R_{ij}V_{ij}^h}{\sum_i R_{ij}}$
        \State $\forall h: (\sigma_j^h)^2 \gets \frac{\sum_i R_{ij}(V_{ij}^h - \mu_j^h)^2}{\sum_i R_{ij}}$
        \State $\forall h: \acute{cost}^h \gets (\beta_u + log(\sigma_j^h)) \sum_i R_{ij}$ \label{op:sum_rij}
        \State $\alpha_j \gets logistic(\lambda(\beta_{\alpha} - \sum_h \acute{cost}^h))$ 
      \EndProcedure
      \Procedure{E-Step}{$\mu, \sigma, \alpha, V, i$} \Comment{\gr{Για μια κάψουλα χαμηλότερου επιπέδου, $i$}}
      \State $\forall j \in \Omega_{L+1}: p_j \gets \frac{1}{\sqrt{\prod _h^H 2\pi(\sigma_j^h)^2}} \exp(- \sum_h^H \frac{(V_{ij}^h - \mu_j^h)^2}{2(\sigma_j^h)^2})$
      \State $\forall j \in \Omega_{L+1}: R_{ij} \gets \frac{\alpha_j p_j}{\sum_{k \in \Omega_{L+1}} \alpha_k p_k}$
      \EndProcedure
  \end{algorithmic}
  \end{algorithm}
}

Για τον αλγόριθμο δρομολόγησης βασισμένο στον \en{EM} μπορούμε να κάνουμε τις εξής σημειώσεις:
\begin{itemize}
  \item Αρχικοποιούμε με ομοιόμορφο τρόπο τις πιθανότητες ανάθεσης $R_{ij}$ ώστε κάθε παιδί να συνδέεται ισοδύναμα με τους γονείς που ψηφίζει. Έπειτα, καλούμε (επαναληπτικά) το βήμα \en{M} και το βήμα \en{E}.
  \item Στο βήμα \en{M} υπολογίζουμε τα $\mu_j$ τα $\sigma_j$ και τα $\alpha_j$ βασιζόμενοι στα $R_{ij}, V_{ij}$ και εμμέσως, στις πιθανότητες ενεργοποίησης των παιδιών $\alpha_i$. Ουσιαστικά, στο βήμα αυτό υπολογίζεται ένα βελτιωμένο γκαουσιανό μοντέλο για την κάθε κάψουλα, μαζί με την πιθανότητα ανάθεσής της. 
  \item Στο βήμα \en{E} υπολογίζονται οι πιθανότητες μέλους (\en{membership probabilities}) $p_j$ που δείχνουν την πιθανότητα το \textquote{δείγμα} $i$ να ανήκει στην γκαουσιανή $j$. Επίσης, επανεκτημούνται οι πιθανότητες ανάθεσης $R_{ij}$. οι εν λόγω υπολογισμοί γίνονται χρησιμοποιώντας τις νέες κατανομές προέκυψαν από το βήμα \en{M}.
  \item Στο βήμα \ref{op:sum_rij} του αλγορίθμου (σχέση \ref{eq:cost_acute}) πρακτικά κλιμακώνουμε το κόστος ανάλογα με τη μέση ποσότητα δεδομένων που δέχονται οι κάψουλες γονείς στο εκάστοτε επίπεδο. Η ποσότητα αυτή υπολογίζεται από τον τύπο:
  \begin{equation}
    mean\_data = \frac{child_W \times child_H \times child_{CH}}{parent_W \times parent_H \times parent_{CH}},
  \end{equation}
  όπου $W, H$ και $CH$ δηλώνουν το πλάτος, το ύψος και το βάθος (αριθμός από τύπους) του επιπέδου από κάψουλες. \footnote{Η κλιμάκωση μπορεί να γίνει με διαίρεση του κόστους με τη μέση ποσότητα δεδομένων. Για περισσότερες πληροφορίες, παραπέμπουμε τον αναγνώστη στο \cite{gritzman2019avoiding}.}
  \item Μετά από δύο ή τρεις επαναλήψεις συνήθως, ο αλγόριθμος τερματίζει όπου και αναδιατάσσουμε τα διανύσματα $\mu_j$ σε μορφή πίνακα $4 \times 4$ ώστε να λάβουμε τις πόζες $M_j$ των καψουλών γονέων.
\end{itemize}

\subsection{Συνάρτηση Σφάλματος και Λοιπά Στοιχεία Υλοποίησης}

Για τους σκοπούς της εκπαίδευσης του νευρωνικού δικτύου με κάψουλες χρησιμοποιείται η απώλεια διασποράς (\en{spread loss}). Η συνάρτηση αυτή δίνεται από τη σχέση:
\begin{equation}
  L = \sum_{i\neq t} L_i, \text{όπου} L_i = (max(0, m - (\alpha_t - \alpha_i)))^2.
\end{equation}
Στην ανωτέρω σχέση το $\alpha_i$ είναι η πιθανότητα ενεργοποίησης της κάψουλας του τελευταίου επιπέδου με αύξοντα αριθμό $i$. $\alpha_t$ είναι η τιμή της πιθανότητας ενεργοποίησης της κάψουλας που έχει αύξοντα αριθμό ίδιο με αυτόν της κλάσης στόχου. Στα αρχικά παραδείγματα της εκπαίδευσης, το \textquote{περιθώριο} $m$ \footnote{Το περιθώριο συμβολίζει τη μέγιστη διαφορά που μπορούν να έχουν η πιθανότητα ενεργοποίησης της κάψουλας που αναφέρεται στη σωστή κλάση και της αντίστοιχης τιμής της κάψουλας $i \neq t$ και να μην προσμετρηθεί στο σφάλμα.} είναι μικρό ($0.2$) έτσι ώστε να αποφεύγεται ο σχηματισμός μόνιμα ανενεργών καψουλών. Καθώς η εκπαίδευση συνεχίζεται, το περιθώριο αυξάνεται σταδιακά στην τιμή $0.9$.\par

Τέλος, σημειώνουμε ότι ακολουθώντας την περιγραφή της υλοποίησης στο \cite{gritzman2019avoiding}, επιβάλουμε ένα κάτω φράγμα στη διασπορά $(\sigma_j^h)^2$ προσθέτοντας την τιμή $\epsilon = 10^(-4)$ έτσι ώστε να μην παρατηρείται το πρόβλημα της \textquote{σύνθλιψης της διακύμανσης} (\en{variance collapse}).


\section{\en{Multi-Head Self-Attention Capsules}}
\label{sec:method_3}

Σε αυτήν την ενότητα, θα αναλύσουμε μια ομάδα μεθόδων που χρησιμοποιούν μηχανισμό αυτο\textendash προσοχής για τη δρομολόγηση των καψουλών μεταξύ δυο διαδοχικών επιπέδων, οδηγώντας σε μια γρήγορη και κλιμακώσιμη υλοποίηση των νευρωνικών δικτύων με κάψουλες. Οι μέθοδοι αυτές, εμπνευσμένες από το έργο \cite{mazzia2021efficient} και χρησιμοποιώντας ελάχιστες παραμέτρους σε σχέση με τη βασική υλοποίηση \cite{sabour2017dynamic}, επιτυγχάνουν υψηλά ποσοστά ακρίβειας. \par

Πιο αναλυτικά, αρχικά θα αναφερθούμε στη βασική αρχιτεκτονική του νευρωνικού δικτύου που αναπτύξαμε, περιγράφοντάς τη με μεταβλητές που αφορούν τα παραμετροποιήσημα μεγέθη. Σε αυτό το πλαίσιο, θα αναφερθούμε και σε μια εναλλακτική αρχιτεκτονική που αντικαθιστά τα πρώτα συνελικιτκά επίπεδα του νευρωνικού δικτύου με αυτό που χρησιμοποιείται στο έργο \cite{sabour2017dynamic}. Έπειτα, θα παρουσιάσουμε τον αλγόριθμο που χρησιμοποιείται στο έργο \cite{mazzia2021efficient} αλλά και όλες τις παραλλαγές του που αναπτύξαμε εμείς όπως αυτή της πολυκέφαλης προσοχής. Στη συνέχεια, θα γίνει λόγος για το τμήμα του αποκωδικοποιητή (ή τμήμα ανακατασκευής) και δη τις δύο εναλλακτικές αρχιτεκτονικές που χρησιμοποιήσαμε στα πειράματα. Τέλος, θα διατυπώσουμε ορισμένες λεπτομέρειες υλοποίησης (συνάρτηση σύνθλιψης, συνάρτηση σφάλματος κτλ.) που θα διευκολύνουν τον αναγνώστη που επιθυμεί να μελετήσει τον σχετικό μας κώδικα.\par

Οι προσφορές της παρούσας μεθόδου μπορούν να συνοψιστούν ως εξής:
\begin{itemize}
  \item Ο πειραματισμός με μια απλή αρχιτεκτονική που μειώνει σημαντικά τον αριθμό των παραμέτρων χωρίς να θυσιάζεται η επίδοση του δικτύου.
  \item Ο πειραματισμός με πιο σύνθετους αποκωδικοποιητές χρησιμοποιώντας επίπεδα αποσυνέλιξης (\en{deconvolution}) που βελτιώνουν τη δυνατότητα γενίκευσης του δικτύου με κάψουλες.
  \item Η παρουσίαση ενός πρωτοπόρου, μη\textendash επαναληπτικού μηχανισμού δρομολόγησης καψουλών με μηχανισμό αυτο\textendash προσοχής πολλών κεφαλών που βελτιώνει την απόδοση.
  \item Η περιγραφή ενός αλγορίθμου που μπορεί να χρησιμοποιηθεί συμπληρωματικά με κάθε αλγόριθμο δρομολόγησης για την αποδοτική αρχικοποίηση των καψουλών γονέων (ελαττώνοντας έτσι τον αριθμό των αργών επαναλήψεων ενός δυναμικού, επαναληπτικού αλγορίθμου).
  \item Ο έλεγχος της ποιότητας γενίκευσης νευρωνικών δικτύων με κάψουλες με δοκιμές σε σύνολα δεδομένων όπως το \en{MultiMNIST} και το \en{SmallNorb} αλλά και με ελέγχους διαταραχής (\en{perturbation testing}).
\end{itemize}

\subsection{Αρχιτεκτονική Νευρωνικού Δικτύου}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.97\textwidth]{images/chapter method/therd_method_architecture_encoder.pdf}
  \caption{Η αρχιτεκτονική του νευρωνικού δικτύου με κάψουλες της τρίτης μεθόδου (αρχιτεκτονική \en{DepthConv}). Στο σχήμα περιγράφονται σχηματικά οι δύο εναλλακτικοί αλγόριθμοι δρομολόγησης που αναπτύξαμε. Για λόγους απλότητας, δεν απεικονίζουμε την περίπτωση αυτο\textendash προσοχής πολλών κεφαλών. Επίσης, για λόγους κατανόησης, δεν αναπαριστώνται ορισμένες αλγοριθμικές λεπτομέρειες όπως αυτή της κανονικοποίησης. Σημειώνεται ότι τα \en{V, Q, K} είναι ταυτόσημα με τη μήτρα \en{V}. \textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
  \label{fig:method_3_architecture}
\end{figure}

Η βασική αρχιτεκτονική του δικτύου φαίνεται στο σχήμα \ref{fig:method_3_architecture} (την ονομάζουμε αρχιτεκτονική \en{DepthConv}). Για τη μεταφορά από τον χώρο των εικονοστοιχείων στον χώρο των καψουλών χρησιμοποιούνται τέσσερα συνελικτικά επίπεδα και ένα επίπεδο συνέλιξης κατά βάθος (\en{Depthwise 2D convolution}). Οι συνήθεις παράμετροι των πρώτων συνελικτικών επιπέδων παρουσιάζονται στον πίνακα \ref{tab:method3_params}. Επισημαίνεται ότι μετά από κάθε συνελικτικό επίπεδο έχουμε ένα επίπεδο κανονικοποίησης κατά δέσμες (\en{Batch Normalization})\footnote{Εκτός από την αρχιτεκτονική για το σύνολο δεδομένων \en{SmallNORB} όπου τα πρώτα δύο είναι επίπεδα κανονικοποίησης κατά κανάλι παραδείγματος (\en{Instance Normalization})}. \par

\begin{table}[h]
  \begin{center}
    \begin{tabular}{| c | c c |} 
     \hline
     Επίπεδο & Πυρήνας & Αριθμός Φίλτρων \\ [0.5ex] 
     \hline\hline
     \en{A} & $5 \times 5$ & 32 \\ 
     \hline
     \en{B} & $3 \times 3$ & 64 \\
     \hline
     \en{C} & $3 \times 3$ & 64 \\
     \hline
     \en{D} & $3 \times 3$ & 128 \\ [1ex] 
     \hline
    \end{tabular}
    \caption{\label{tab:method3_params}Πίνακας στον οποίο παρουσιάζεται η παραμετροποίηση της αρχιτεκτονικής των πρώτων επιπέδων.}
    \end{center}
  \end{table}


Όπως προαναφέραμε, τα πρώτα τέσσερα επίπεδα διαδέχεται ένα επίπεδο συνέλιξης κατά βάθος. Ουσιαστικά, πρόκειται για ένα επίπεδο συνέλιξης με τόσα κυλιόμενα παράθυρα (δισδιάστατα φίλτρα) όση είναι και η διάσταση βάθους του προηγούμενου επιπέδου. Το καθένα από αυτά τα φίλτρα παράγει ένα χάρτη χαρακτηριστικών. Αξίζει να σημειωθεί ότι στην περίπτωσή μας, ακολουθώντας την υλοποίηση \cite{mazzia2021efficient}, το μέγεθος του κάθε δισδιάστατου πυρήνα το θέτουμε να είναι ίσο με τις διαστάσεις πλάτους και ύψους των χαρτών χαρακτηριστικών του προηγούμενου επιπέδου. Συνεπώς, από κάθε φίλτρο προκύπτει μια μονάδα (\en{scalar}). Συνολικά, δηλαδή, έχουμε σαν έξοδο έναν όγκο από αποκρίσεις (\en{activations}) μεγέθους $[1 \times 1 \times D_F]$, οπου $D_F$ ο αριθμός των φίλτρων του επιπέδου $D$. Το διάνυσμα αυτό το διαχωρίζουμε σε υποδιανύσματα με οκτώ στοιχεία το καθένα τα οποία και τροφοδοτούμε στη συνάρτηση σύνθλιψης (\en{squash}) σχηματίζοντας έτσι το πρώτο επίπεδο από κάψουλες (\en{PrimaryCapsules}).\par

Για παράδειγμα, στις περισσότερες παραμετροποιήσεις των πειραμάτων το συνελικτικό επίπεδο $D$ έχει σαν αποτέλεσμα τη δημιουργία 128 χαρτών χαρακτηριστικών μεγέθους $9 \times 9$. Τροφοδοτώντας αυτές τις αποκρίσεις σε ένα συνελικτικό επίπεδο κατά βάθος με 128 φίλτρα μεγέθους $9 \times 9$ λαμβάνουμε ένα διάνυσμα $128$ στοιχείων. Αυτό το διάνυσμα το χωρίζουμε σε κάψουλες διάστασης 8 με αποτέλεσμα να λάβουμε 16 \en{PrimaryCaps} (αφού πρώτα εφαρμόσουμε σε αυτές την τροποποιημένη συνάρτηση σύνθλιψης που εξηγούμε παρακάτω).\par

Η εν λόγω αρχιτεκτονική ενσωματώνει πολύ περισσότερα συνελικτικά επίπεδα από την αρχιτεκτονική που προτείνεται στο έργο \cite{sabour2017dynamic}. Παρόλα αυτά, μειώνοντας τον αριθμό των καψουλών που βρίσκονται στο πρώτο επίπεδο από κάψουλες, επιτυγχάνει να ελαττώσει σημαντικά τον αριθμό των εκπαιδευόμενων παραμέτρων. Για λόγους πειραματισμού, δοκιμάσαμε να μιμηθούμε τον τρόπο δημιουργίας του επιπέδου \en{PrimaryCaps} που χρησιμοποιείται στο έργο \cite{sabour2017dynamic}, όπως τον περιγράψαμε στην πρώτη μέθοδο του παρόντος κεφαλαίου. Έτσι, στη θέση των τεσσάρων συνελικτικών επιπέδων έχουμε ένα επίπεδο από 256 φίλτρα με πυρήνες $9 \times 9$ (βλέπε σχήμα \ref{fig:method_1_architecture}). Ειδικά για το σύνολο δεδομένων \en{SmallNORB}, η εναλλακτική αρχιτεκτονική για την δημιουργία του επιπέδου \en{PrimaryCaps} είναι ίδια με αυτή του έργου \cite{hinton2018matrix} (βλέπε σχήμα \ref{fig:method_2_architecture}). Θα αναφερόμαστε σε αυτήν την εναλλακτική αρχιτεκτονική ως \textquote{αρχιτεκτονική \en{DynamLike}} σε αντιπαραβολή με τη \textquote{αρχιτεκτονική \en{DepthConv}} που απεικονίζεται στο σχήμα \ref{fig:method_3_architecture}.\par

Συνεχίζοντας την περιγραφή της αρχιτεκτονικής από τα αριστερά προς τα δεξιά, μέσω του αλγορίθμου δρομολόγησης με μηχανισμό αυτο\textendash προσοχής διαμορφώνονται οι κάψουλες του τελευταίου επιπέδου (ονομάζονται και \en{DigitCaps}). Αναπόσπαστο στοιχείο της διαδικασίας σχηματισμού των καψουλών του ανώτερου επιπέδου είναι ο υπολογισμός των ψήφων για την πόζα των \en{DigitCaps}. Κατά αναλογία με την υλοποίηση \cite{sabour2017dynamic}, η κάθε κάψουλα επιπέδου \en{PrimaryCaps} χρησιμοποιώντας πίνακες μετασχηματισμού με εκπαιδευόμενα βάρη παράγει μια πρόβλεψη για κάθε κάψουλα επιπέδου \en{DigitCaps}. Αυτές οι ψήφοι στη συνέχεια φιλτράρονται μέσω ενός γρήγορου αλγορίθμου δρομολόγησης με αυτο\textendash προσοχή (αναλύεται παρακάτω) και σχηματίζονται οι κάψουλες του τελικού επιπέδου.

\subsection{Αλγόριθμοι Δρομολόγησης}

Μια σημαντική αδυναμία των νευρωνικών δικτύων με κάψουλες που διαπιστώθηκε κατά τη μελέτη σχετικής βιβλιογραφίας είναι αυτή της κλιμάκωσης σε μεγαλύτερες διατάξεις. Τροχοπέδη για την κλιμάκωση αποτελεί το αυξημένο υπολογιστικό κόστος και η αστάθεια των επαναληπτικών αλγορίθμων δρομολόγησης. Επιδιώκοντας να δοθεί λύση στο πρόβλημα, παρατηρήσαμε την πρόσφατη άνθιση των μοντέλων νευρωνικών δικτύων που χρησιμοποιούν μηχανισμό αυτο\textendash προσοχής (υπό τη μορφή μετασχηματιστών) τόσο για εφαρμογές επεξεργασίας φυσικής γλώσσας (\en{natural language processing}) όσο και για εφαρμογές όρασης υπολογιστών (\en{computer vision}). Συγκεκριμένα,  σημαντική πηγή έμπνευσης αποτέλεσαν τα έργα \cite{dosovitskiy2020image_is_worth_16, carion2020end}. Δανειζόμενοι στοιχεία από τη χρήση μηχανισμού αυτο\textendash προσοχής σε εικόνες και διατηρώντας παράλληλα τις βασικές υποθέσεις των νευρωνικών δικτύων με κάψουλες επινοήσαμε μια οικογένεια από γρήγορους, κλιμακώσιμους (\en{scalable}) και μη\textendash επαναληπτικούς αλγορίθμους δρομολόγησης.\par

Όπως γίνεται αντιληπτό από το κεφάλαιο \ref{chap:related_work}, δεν είναι η πρώτη φορά που χρησιμοποιείται μηχανισμός αυτο\textendash προσοχής στο πλαίσιο των νευρωνικών δικτύων με κάψουλες. Στα έργα \cite{hoogi2019self, huang2020capsnet} χρησιμοποιείται ο εν λόγω μηχανισμός ως επίπεδο που προηγείται του σχηματισμού των \en{PrimaryCaps}, το οποίο διαφέρει σημαντικά από την υλοποίησή μας που ενσωματώνει τον μηχανισμό αυτο\textendash προσοχής στον αλγόριθμο δρομολόγησης. Επίσης, αν και η υλοποίησή μας άντλησε στοιχεία από το έργο \cite{mazzia2021efficient} (το μόνο που εφαρμόζει τον μηχανισμό στον αλγόριθμο δρομολόγησης) είναι πολύ πιο πιστή στις βασικές υποθέσεις των δικτύων με κάψουλες. Εν αντιθέσει, για τον αλγόριθμο δρομολόγησης των \en{Mazzia et al.} (τον ονομάζουμε \textquote{απλοϊκό αλγόριθμο δρομολόγησης με αυτο\textendash προσοχή}) δεν καταφέραμε να δώσουμε μια λογική εξήγηση ου να συμφωνεί με τις αρχές των νευρωνικών δικτύων με κάψουλες. Επίσης, όπως θα φανεί στο επόμενο κεφάλαιο, η υλοποίησή μας με παρόμοιο αριθμό παραμέτρων επιτυγχάνει καλύτερες επιδόσεις (χωρίς ιδιαίτερο πειραματισμό των παραμέτρων).\par

Στην παρούσα υποενότητα θα παρουσιάσουμε τους δύο εναλλακτικούς αλγορίθμους που αναπτύξαμε καθώς και τη λογική πίσω από τον καθένα. Επίσης, για λόγους πληρότητας, θα παρουσιάσουμε τον \textquote{απλοϊκό αλγόριθμο δρομολόγησης με αυτο\textendash προσοχή} που χρησιμοποιείται στην υλοποίηση \cite{mazzia2021efficient}. Προτού όμως ξεκινήσουμε την παρουσίαση, κρίνεται σκόπιμο να ορίσουμε τον συμβολισμό που θα χρησιμοποιηθεί σε όλη την έκταση των αλγορίθμων μας. Χρησιμοποιούμε τον δικό μας συμβολισμό ο οποίος διαφέρει από αυτών των προηγούμενων αλγορίθμων καθώς δεν ήταν επαρκής ώστε να περιγράψει τις νέες έννοιες που εισάγουμε. Έτσι έχουμε:
\begin{itemize}
  \item Σαν σύμβαση, χρησιμοποιούμε τον δείκτη $i$ για να αναφερθούμε σε κάψουλες επιπέδου $L$ και τον δείκτη $j$ για τις κάψουλες επιπέδου $L+1$.
  \item Το σύνολο των καψουλών ενός επιπέδου $L$ το συμβολίζουμε ως $\Omega_L$ και τον πληθάριθμο του συνόλου (\en{cardinality}) ως $n^L = \left\lvert \Omega_L \right\rvert$.
  \item Τις επιμέρους κάψουλες ενός επιπέδου $L$ τις συμβολίζουμε με $C^L_i$ ενώ με $C^L$ συμβολίζουμε τη μήτρα στην οποία οργανώνονται όλες οι κάψουλες επιπέδου $L$. Το μέγεθος της κάθε κάψουλας επιπέδου $L$ (μήκος του διανύσματος με το οποίο αναπαρίσταται η κάψουλα) συμβολίζεται με $d^L$.
  \item Μεταξύ δύο επιπέδων από κάψουλες $L$ και $L+1$, κάθε κάψουλα $C^L_i \in \Omega_L$ προβλέπει ποια θα είναι η πόζα της κάθε κάψουλας $C^{L+1}_j \in \Omega_{L+1}$. Έτσι προκύπτουν $n^L \times n^{L+1}$ προβλέψεις (ή ψήφοι) όπου την κάθε μια τη συμβολίζουμε με $V_{ji}^L$. Αυτές οι ψήφοι οργανώνονται στη μήτρα $V^L$ η οποία όπως είναι λογικό, περιέχει $n^{L+1} \times n^L$ κάψουλες - ψήφους: $n^L$ ψήφους για κάθε κάψουλα $C^{L+1}_i$.
  \item Σε περίπτωση που θα θέλαμε να αναφερθούμε σε όλες τις ψήφους που αφορούν μια κάψουλα επιπέδου $L+1$ αρκεί να \textquote{δεσμεύσουμε} την ελεύθερη μεταβλητή $j$ και να γράψουμε $V^L_j$ (ή, καλύτερα, $V^L_{j:}$ ώστε να είναι εμφανές ότι η μεταβλητή $i$ που αφορά τις κάψουλες επιπέδου $L$ παραμένει ελεύθερη). Κατά αντιστοιχία, αν θα θέλαμε να αναφερθούμε στις ψήφους που προκύπτουν από την κάψουλα $C^L_i$ θα γράφαμε $V^L_i$ (ή, σαφέστερα, $V^L_{:i}$). Κατά αυτόν τον τρόπο, αν δεσμεύσουμε όλες τις διαστάσεις μιας μήτρας, τότε έχουμε ένα βαθμωτό μέγεθος. Για παράδειγμα, έστω κάψουλες $C^L \in \Re^{n^L \times d^L}$. Τότε αν δεσμεύσουμε την ελεύθερη μεταβλητή $i \in [1, n^L]$ τότε ισχύει $C_i^L \in \Re^{d^L}$. Αν δεσμεύσουμε και το επιμέρους τμήμα του διανύσματος της κάψουλας λαμβάνουμε έναν πραγματικό αριθμό, δηλαδή: $C_{ik}^L \in \Re$ \footnote{Μάλιστα είναι $C_{ik}^L \in [0,1]$ αφού η συνάρτηση \en{squash} δεν επιτρέπει τα διανύσματα των καψουλών να έχουν μήκος μεγαλύτερο της μονάδας.}.
  \item Όπως γίνεται κατανοητό από το ανωτέρω παράδειγμα, ο δείκτης $k_L$ θα χρησιμοποιείται για να αναφερόμαστε στα επιμέρους στοιχεία ενός διανύσματος μεγέθους $d^L$.
  \item Οι πίνακες από βάρη ενός επιπέδου $L$ που αποθηκεύουν τις σχέσεις μέρους - όλου οργανώνονται στη μήτρα $W^L$. Η μήτρα περιέχει όλα τα επιμέρους βάρη $W_{ji}^L$ που χρησιμοποιούν οι κάψουλες $C^L$ για να παράξουν τις προβλέψεις $V^L$. Για μια κάψουλα $C_i^L \in \Omega_L$, η ψήφος της υπολογίζεται με τη φόρμουλα: $V_{ji}^L = C_i^L \times W_{ji}^L$.
  \item Συνήθως, μπορεί να αναφερόμαστε στις κάψουλες επιπέδου $L$ ως κάψουλες παιδιά και στις αντίστοιχες του επιπέδου $L+1$ ως κάψουλες γονείς.
  \item Ο επιμέρους πίνακας που προκύπτει από τον μηχανισμό αυτο\textendash προσοχής των ψήφων $V^L_j$ για μια κάψουλα $C_j^{L+1}$ συμβολίζεται ως $Α^L_j$. Η μήτρα που περιέχει όλους τους πίνακες προσοχής ($\forall C_j^{L+1} \in \Omega_{L+1}$) συμβολίζεται με $A^L$.
  \item Στην περίπτωση που χρησιμοποιείται αυτο\textendash προσοχή πολλών κεφαλών (\en{multi\textendash head self\textendash attention}) η αναφορά σε επιμέρους κεφαλές γίνεται με τον δείκτη $h$ και ο συνολικός αριθμός των κεφαλών συμβολίζεται ως $nh^L$. Για παράδειγμα, η πρώτη κεφαλή (\en{head}) ενός πίνακα προσοχής για μια κάψουλα $C^{L+1}_j$ συμβολίζεται ως $A_{jh=1}^L$. Η αναφορά στο σύνολο των κεφαλών προσοχής ενός επιπέδου $L$ για μια κάψουλα $C_j^{L+1}$ γίνεται με τον συμβολισμό $H_j^L$. Δηλαδή, ισχύει ότι $A_{jh}^L \in H_j^L$.
  \item Η μήτρα βαρών δρομολόγησης μεταξύ επιπέδων $L$ και $L + 1$ συμβολίζεται με $\mathbf{R}^L$. Αυτή, μεταξύ δύο πλήρως διασυνδεδεμένων επιπέδων από κάψουλες, περιέχει όλα τα βάρη ακμών τα οποία συμβολίζονται με $\mathbf{R}_{ij}^L$.
\end{itemize}


\subsubsection{Απλοϊκός αλγόριθμος δρομολόγησης με αυτο\textendash προσοχή}

Στην παράγραφο αυτή γίνεται αναφορά στον αλγόριθμο που χρησιμοποιεί το έργο \cite{mazzia2021efficient}. Σε γενικές γραμμές, πρόκειται για τον μη\textendash επαναληπτικό αλγόριθμο ο οποίος χρησιμοποιεί τον μηχανισμό αυτο\textendash προσοχής προκειμένου να υπολογίσει τα βάρη δρομολόγησης (\en{coupling coefficients}). Αν και το έργο δεν προσφέρει μια πειστική, λογική εξήγηση για όλα τα βήματα του αλγορίθμου, οι υψηλές επιδόσεις του παρακίνησαν την κατασκευή των εξελιγμένων αλγορίθμων που παρουσιάζουμε σε επόμενες παραγράφους.\par

\en{
\begin{algorithm}[h]
  \caption{\gr{Απλοϊκός Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή}}\label{alg:method3_stupid_routing}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}, b^L \in \Re^{n^{L+1} \times n^L}$

  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line
    \Procedure{Main}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $A^L \gets$ \Call{Self-Attention}{$V^L$}
      \State $\textbf{R}^L \gets$ \Call{ComputeRootingCoefficients}{$A^L$}
      \State $\forall j \in \Omega_{L+1}: S_j^{L+1} \gets (\textbf{R}_j^L + b_j^L) \times V_j^L$ \Comment{Equiv.: $S_{jk}^{L+1} \gets \sum_i^{n^L} (\textbf{R}_{ji}^L + b_{ji}^L) \ast V_{jik}^L$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure
    % \item[] % Empty, unnumbered line
    \Procedure{ComputeVotes}{$C^L$}  \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: V_{ji:}^L \gets C_{i:}^L \times W_{ji::}^L$ \Comment{Equiv.: $V^L_{jik_{L+1}} \gets \sum_{k_{L}}^{d^L} C_{ik_L}^L \ast W_{jik_Lk_{L+1}}$}
      \State \Return $V^{L}$ \Comment{Output: $V^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure
    
    \Procedure{Self-Attention}{$V^L$}  \Comment{Input: $V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
      \State $\forall j \in \Omega_{L+1}: A_j^L \gets \frac{V^L_j \times {V_j^{L}}^T}{\sqrt{d^{L+1}}}$ \Comment{Equiv.: $A^L_{ji_1i_2} \gets \sum_k^{d^{L+1}} V^L_{ji_1k} \ast V^L_{ji_2k}$}
      \State \Return $A^{L}$ \Comment{Output: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

    \Procedure{ComputeRootingCoefficients}{$A^L$} \Comment{Input: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
      \State $\forall j \in \Omega_{L+1}: R_j^L \gets \sum_{i_1}^{n^L} A^L_{ji_1:}$ \Comment{Equiv.: $R_{ji_2}^L \gets \sum_{i_1}^{n^L} A^L_{ji_1i_2}$} \label{op:stupid_algo_is_sum_of_rows}
      \State $\forall i \in \Omega_{L}: \mathbf{R}_{:i}^L \gets \mathit{softmax}(R_{:i}^L)$ \Comment{Equiv.: $\mathbf{R}_{ji}^L \gets \frac{\exp(R_{ji}^L)}{\sum_j^{n^{L+1}} \exp(R_{ji}^L)}$} \label{op:stupid_algo_softmax_definition}% How to define softmax: $\mathit{softmax}(X \in \Re^{1\times n}) = $
      \State \Return $\mathbf{R}^L$ \Comment{Output: $\mathbf{R}^{L} \in \Re^{n^{L+1} \times n^{L}}$}
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}


Αν και τα ανωτέρω βήματα έχουν παρουσιαστεί πολύ αναλυτικά με ισοδύναμες (\en{"Equivalent"}), σημειακές εκφράσεις, μπορούμε να κάνουμε τα παρακάτω σχόλια:
\begin{itemize}
  \item Το σύμβολο $\ast$ συμβολίζει τον πολλαπλασιασμό μεταξύ δύο βαθμωτών μεγεθών (\en{scalars}).
  \item Η συνάρτηση $softmax()$ στο βήμα \ref{op:stupid_algo_softmax_definition} σκοπό έχει να επιβάλει τον περιορισμό $\sum_j^{n^{L+1}} R_{ji} = 1$ και να ενισχύσει το βάρος δρομολόγησης με το μεγαλύτερο μέτρο έτσι ώστε, τελικά, μια κάψουλα παιδί να μην ανήκει ολοκληρωτικά σε πολλούς γονείς (\en{single parent assumption}). Σε τεχνικό επίπεδο, η συνάρτηση δέχεται σαν είσοδο ένα διάνυσμα στήλη και μπορεί να οριστεί ως εξής:
  \begin{equation}
    \label{eq:softmax_algorithm}
    \mathit{softmax}(X \in \Re^{n \times 1}) = \frac{\exp(X_k)}{\sum_k^n \exp(X_k)}.
  \end{equation}
  \item Στη γραμμή \ref{op:stupid_algo_is_sum_of_rows} του αλγορίθμου, το αριστερό άθροισμα διενεργείται σημειακά σε διανύσματα γραμμές. Ουσιαστικά, για μια κάψουλα γονέα $j$, οι ομοιότητες της εκάστοτε ψήφου από την κάψουλα $C_i^L$ με τις άλλες ψήφους από τις κάψουλες $C_{\acute{i}}^L \in \Omega_L \setminus C_i^L$ αθροίζονται στην τιμή $R_{ji}^L$. Η ποιοτική ερμηνεία αυτής της πράξης δεν αναλύεται στο έργο \cite{mazzia2021efficient}. Σε μια προσπάθεια ερμηνείας, θα μπορούσαμε να αναφέρουμε ότι κάθε θέση $i$ του διανύσματος γραμμής $R_j^L$ που προκύπτει, φανερώνει ποιες ψήφοι καψουλών $C_i^L$ εμφανίζουν μεγάλη ομοιότητα με τις υπόλοιπες ψήφους για το συγκεκριμένο το $j$. Έτσι, στη συνέχεια, η κάθε κάψουλα $C_j^{L+1}$ θα συντίθεται μόνο από τις ψήφους των $C_{i}^L$ που εμφάνιζαν μεγάλη ομοιότητα με τις υπόλοιπες ψήφους από τις κάψουλες $C_{\acute{i}}^L \in \Omega_L \setminus C_i^L$ (πάντα για συγκεκριμένο $j$).
\end{itemize}

Πλέον, είμαστε σε θέση να παρουσιάσουμε τους δικούς μας αλγορίθμους οι οποίοι έχουν περισσότερο προφανή ποιοτική ερμηνεία και μάλιστα, επιτυγχάνουν καλύτερα πειραματικά αποτελέσματα.

\subsubsection{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 1 (Αλγόριθμος \en{RooMAV})}

Ο πρώτος αλγόριθμός μας που εξετάζουμε στην παρούσα ενότητα είναι αυτός της δρομολόγησης με αυτο\textendash προσοχή όπου προσθέτουμε τις ψήφους που εμφανίζουν υψηλή συμφωνία (\en{Rooting by Merged Agreeing Votes - RooMAV}). Ο αλγόριθμος αυτός, μοιάζει πολύ με τον απλοϊκό αλγόριθμο \ref{alg:method3_stupid_routing} αλλά εδώ, αντί να αθροίζουμε τις γραμμές του πίνακα προσοχής, αθροίζουμε τις προκύπτουσες αναπαραστάσεις (\en{embeddings}). \par

Η γενική ιδέα πίσω από τον αλγόριθμο \ref{alg:method3_sum_routing} φαίνεται στο σχήμα \ref{fig:method_3_architecture}. Αν και έχει δοθεί μεγάλη προσοχή στο να παρουσιαστεί ο αλγόριθμος με τον πιο σαφή τρόπο, πολλές φορές η κατανόηση του αλγορίθμου δε συνεπάγεται την αντίληψη της ποιοτικής του ερμηνείας. Για τον λόγο αυτό, κρίνεται σκόπιμη η διαισθητική παρουσίαση τόσο του αλγορίθμου 1 (\en{RooMAV}) όσο και του αλγορίθμου 2 που τον διαδέχεται. \par

Ο αλγόριθμος \ref{alg:method3_sum_routing} αποσκοπεί στο να φιλτράρει τις ψήφους\textendash διανύσματα $V^L$ με κριτήριο το εσωτερικό τους γινόμενο (μετρική συμφωνίας) και να κατασκευάσει νέες αναπαραστάσεις (\en{embeddings}) χρησιμοποιώντας τα διανύσματα που συμφωνούν μεταξύ τους για την πόζα της εκάστοτε κάψουλας $C_j^{L+1}$. Για την αποδοτική υλοποίηση του φιλτραρίσματος δανιζόμαστε στοιχεία από τον μηχανισμό προσοχής, όπως τον παρουσιάσαμε στην ενότητα \ref{sec:transformers}. Πιο αναλυτικά, κατασκευάζουμε ένα χάρτη προσοχής (\en{attention map}) $A_j^L$ για κάθε κάψουλα $C_j^L$. Για όλους μαζί τους χάρτες αυτούς, ισχύει ότι $A^L \in [-1,1]^{n^{L+1} \times n^L \times n^L}$. Με άλλα λόγια, σε κάθε θέση $[i_1,i_2]$ ενός εξ'αυτών (για μια κάψουλα $C_j^L$) αποθηκεύεται η ομοιότητα που έχει η ψήφος $V^L_{ji_1}$ με την ψήφο $V_{ji_2}^L$.\par

Πλέον, σε αυτήν τη φάση του αλγορίθμου έχουμε στη διάθεσή μας για κάθε κάψουλα $C_j^{L+1}$ τον βαθμό συμφωνίας μεταξύ όλων των ψήφων για την πόζα της. Από εκεί και πέρα, επιθυμούμε να φιλτράρουμε τις ψήφους με σκοπό να κρατήσουμε μόνον αυτές που εμφανίζουν μεγάλη ομοιότητα μεταξύ τους. Άλλωστε, όπως εξηγήσαμε στην ενότητα \ref{sec:capsule_theory}, όταν πολλές ψήφοι $V_{j:}^L$ συμφωνούν για το ποια είναι η πόζα της κάψουλας $C_j^{L+1}$ τότε υπάρχει μεγάλη πιθανότητα, το αντικείμενο το οποίο εκπροσωπεί η κάψουλα $C_j^{L+1}$ να είναι παρόν στην εικόνα. Αντίθετα, θα υπάρχουν πάντα ψήφοι που προέρχονται από κάψουλες παιδιά $C_i^L$ που δε θα συμφωνούν μεταξύ τους (διότι μπορεί να αντιστοιχούν σε τμήματα αντικειμένων που δεν είναι παρόντα στην εικόνα εισόδου). Αυτές τις ψήφους επιθυμούμε να τις εκμηδενίσουμε καθότι, διαφορετικά, θα εισάγουν \textquote{θόρυβο} στις προβλέψεις μας. Για τον λόγο αυτό, εφαρμόζουμε μια μη γραμμική συνάρτηση όπως η \en{ReLU} σημειακά στις υπολογισμένες ομοιότητες. \par

Ύστερα από τον υπολογισμό αυτό, έχουμε στη διάθεσή μας μια μήτρα με τις ίδιες διαστάσεις με τον $A^L$ αλλά με μη\textendash αρνητικά στοιχεία. Τον νέο τρισδιάστατο πίνακα αυτόν τον συμβολίζουμε ως $\prescript{+}{}{A^{L}}$ και περιέχει τους χάρτες προσοχής $\prescript{+}{}{A_j^{L}}$ με στοιχεία μη μηδενικά μόνο στα σημεία που αντιστοιχούν σε δύο κάψουλες που συμφωνούν μεταξύ τους. Με άλλα λόγια, μπορεί κανείς να φανταστεί το $\prescript{+}{}{A_j^{L}}$ σαν έναν δισδιάστατο πίνακα $[n^L \times n^L]$ (βλέπε σχήμα \ref{fig:method_3_architecture}). Ποιοτικά, κάθε γραμμή $i$ του πίνακα\footnote{Το ίδιο ισχύει και για τις στήλες αφού ο πίνακας είναι συμμετρικός.} $A_j^{L}$ περιέχει τις ομοιότητες που εμφανίζει η ψήφος $V_{ji}^L$ με όλες τις ψήφους ($V_{j:}^L$) για την κάψουλα $C_j^{L+1}$. Φιλτράροντας με τη μη\textendash γραμμική συνάρτηση, κρατάμε μόνο τις θετικές ομοιότητες. Σε τελική ανάλυση, η θέση $[1,1]$ του πίνακα $A_j^{L}$ είναι το εσωτερικό γινόμενο της ψήφου $V_{ji=1}^L$ με τον εαυτό της, η θέση $[1,2]$ και $[2,1]$ είναι το εσωτερικό γινόμενο του διανύσματος $V_{ji=1}^L$ με το $V_{ji=2}^L$ κ.ο.κ.\par

Το τελευταίο κοινό βήμα των αλγορίθμων \en{RooMAV} και \en{RoWSS} (παρουσιάζεται αργότερα) είναι αυτό του υπολογισμού των νέων αναπαραστάσεων, όπως προκύπτουν από τη συνένωση των ψήφων που συμφωνούν μεταξύ τους. Ακολουθώντας και πάλι τη θεωρεία των μετασχηματισμών, οι νέες αναπαραστάσεις (συμβολίζονται με $E^L \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$) υπολογίζονται από το βεβαρημένο ανάλογα με την ομοιότητα άθροισμα των ψήφων. Ας πάρουμε για παράδειγμα τον υπολογισμό του $E^L_{ji}$. Πρόκειται για τη νέα αναπαράσταση της ψήφου $V^L_{ji}$ η οποία λαμβάνει υπόψη τα \textquote{συμφραζόμενα} (\en{context}), δηλαδή, τις άλλες ψήφους καψουλών που αναπαριστούν διαφορετικά μέρη του αντικειμένου\textendash όλου στο οποίο συμφωνούν. Ο υπολογισμός λοιπόν πραγματοποιείται προσθέτοντάς τα διανύσματα ψήφων $V^L_{j:}$ με βάρη από το διάνυσμα γραμμή $\prescript{+}{}{A_{ji:}^{L}}$ σύμφωνα με την πράξη $E^L_{ji} \gets \prescript{+}{}{A_{ji:}^{L}} \times V^L_{j:}$. Όπως είναι εμφανές, εάν η ψήφος $V^L_{ji}$ δε συμφωνεί με μια ψήφο $V^L_{ji_2}, i_2 \neq i$ για το προβλεφθέν αντικείμενο, τότε η τελευταία, δε θα ληφθεί υπόψη για τον υπολογισμό της νέας αναπαράστασης από τα συμφραζόμενα (η αντίστοιχη θέση του διανύσματος γραμμής θα είναι μηδενική).\par

Το τελευταίο σημείο (και αυτό που διαφοροποιεί τους αλγορίθμους \en{RooMAV} και \en{RoWSS}) είναι αυτό του υπολογισμού των $C_j^{L+1}$. Στον αλγόριθμο \en{RooMAV}, απλά έχουμε ότι $C_j^{L+1} \gets \sum_i^{n^L} E^L_{ji}$. Δηλαδή, η κάψουλα $C_j^{L+1}$ προκύπτει από το άθροισμα όλων των νέων αναπαραστάσεων των ψήφων που την αφορούν. Αυτό μπορεί να φανεί σαν ένα ακόμα βήμα \textquote{φιλτραρίσματος μέσω της πολυδιάστατης σύμπτωσης} (\en{high dimensional coincidence filtering}) αφού οι ψήφοι που εμφάνιζαν μεγάλη συμφωνία μεταξύ τους αφενός έχουν νέες αναπαραστάσεις με μεγαλύτερο μήκος και αφετέρου, κατά την πρόσθεση του τελευταίου βήματος θα ενισχυθούν μεταξύ τους και θα αποσιωπήσουν τις αναπαραστάσεις με τις οποίες δε συμφωνούν. Φυσικά, κοιτώντας την ευρύτερη εικόνα, αν καμία αναπαράσταση $E^L_{ji}$ δε συμφωνεί σημαντικά με τις υπόλοιπες για μια κάψουλα $C_j^{L+1}$ τότε η κάψουλα αυτή θα έχει διάνυσμα με μικρό μέτρο.\par

Σαν τελικό σχόλιο να αναφέρουμε ότι ο αλγόριθμος αυτός, όντας μη\textendash επαναληπτικός, δεν περιλαμβάνει ρητά την ανατροφοδότηση από πάνω προς τα κάτω (\en{top down feedback}). Επεξηγηματικά, να αναφέρουμε ότι η συμφωνία που μπορεί να υπάρξει στις ψήφους για μια κάψουλα $C_j^{L+1}$ δε θα επηρεάσει τη διαμόρφωση των άλλων καψουλών $C_{\grave{j}}^{L+1}$. Σημαντικός λόγος για αυτό το χαρακτηριστικό είναι ότι δεν επιβάλουμε κάποιον περιορισμό (τύπου \textquote{υπόθεσης μοναδικού πατέρα} - \en{single parent assumption}) ώστε να προκαλέσουμε ανταγωνισμό μεταξύ των καψουλών γονέων για το ποιες ψήφους θα \textquote{εξηγήσουν}. Σύμφωνα με την παρούσα υλοποίηση, το κάθε τμήμα αντικειμένου μπορεί να ανήκει σε περισότερα του ενός αντικείμενα και να συμμετέχει στη διαμόρφωση της πόζας τους.\par

Σε αυτό το πλαίσιο, αναπτύξαμε τον αλγόριθμο \en{RoWSS} ο οποίος διαφέρει στο τελευταίο βήμα και διατηρεί όλες τις βασικές υποθέσεις των νευρωνικών δικτύων με κάψουλες ενώ παράλληλα, είναι γρήγορος και μη\textendash επαναληπτικός. περισσότερα για αυτόν στην επόμενη υπο\textendash ενότητα.


\en{
\begin{algorithm}[H]
  \caption{\gr{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 1 (Αλγόριθμος \en{RooMAV}) }}\label{alg:method3_sum_routing}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line
    \Procedure{Main-RooMAV}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $A^L \gets$ \Call{Self-Attention}{$V^L$}
      \State $\prescript{+}{}{A^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap}{$A^L$}
      \State $E^L \gets$ \Call{NewEmb}{$\prescript{+}{}{A^{L}}, V^L$} \Comment{Computes new, context-aware, votes.} 
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets \sum_i^{n^L} E^L_{ji}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets \sum_i^{n^L} E^L_{jik}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure
    % \item[] % Empty, unnumbered line
    \Procedure{ComputeVotes}{$C^L$}  \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: V_{ji:}^L \gets C_{i:}^L \times W_{ji::}^L$ \Comment{Equiv.: $V^L_{jik_{L+1}} \gets \sum_{k_{L}}^{d^L} C_{ik_L}^L \ast W_{jik_Lk_{L+1}}$}
      \State \Return $V^{L}$ \Comment{Output: $V^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure
    
    \Procedure{Self-Attention}{$V^L$}  \Comment{Input: $V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
      \State $\forall j \in \Omega_{L+1}: A_j^L \gets \frac{V^L_j \times {V_j^L}^T}{\sqrt{d^{L+1}}}$ \Comment{Equiv.: $A^L_{ji_1i_2} \gets \sum_k^{d^{L+1}} V^L_{ji_1k} \ast V^L_{ji_2k}$}
      \State \Return $A^{L}$ \Comment{Output: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

    \Procedure{ComputeNonNegativeAttentionMap}{$A^L$}\Comment{Input: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
      \State $\forall j \in \Omega_{L+1}:  \prescript{+}{}{A^{L}_j} \gets \mathbf{ReLU}(A^L_j)$ \Comment{Equiv.: $\prescript{+}{}{A^{L}_{ji\tilde{i}}} \gets ReLU(A^L_{ji\tilde{i}})$} \label{op:method3_sum_pointwise_relu_this_op_may_be_different}
      \State \Return $\prescript{+}{}{A^{L}}$ \Comment{Output: $\prescript{+}{}{A^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

    \Procedure{NewEmb}{$\prescript{+}{}{A^{L}}, V^L$} \Comment{Input: $\prescript{+}{}{A^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}, V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
    \State $\forall j \in \Omega_{L+1}: E^L_j \gets \prescript{+}{}{A_j^L} \times V_j^L$ \Comment{Equiv.: $E^L_{jik} \gets \sum_{\tilde{i}}^{n^L} \prescript{+}{}{A_{ji\tilde{i}}^L} \ast V^L_{j\tilde{i}k}$} \label{op:method3_sum_weighted_sum} % Πες για την ερμηνεία, πρόκειται για σταθμισμένο άθροισμα των votes. Αλλά εσύ το κάνες για κάθε ένα row. Υπολογίζεις μια νέα αναπαράσταση (embeding) που είναι context-aware, δηλαδή σχηματίζεται από όλες τις ψήφους (για την κάψουλα j) που συμφωνούν με την εκάστοτε κάψουλα i (εκπρόσωπος γραμμής). Από εδώ είναι εύκολο να πας λογικά στον επόμενο αλγόριθμο που βρίσκει το max row.
    \State \Return $E^L$ \Comment{Output: $E^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}

\subsubsection{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 2 (Αλγόριθμος \en{RoWSS})}
Ο αλγόριθμος \en{Rooting by Winner of Similarity Score - RoWSS} (αλγόριθμος \ref{alg:method3_max_rooting}) μοιράζεται πολλά στοιχεία με τον αλγόριθμο \en{RooMAV} για αυτό και δε θα επαναλάβουμε την επεξήγηση των πρώτων βημάτων, παρά μόνο θα συνεχίσουμε από το σημείο στο οποίο οι δύο αλγόριθμοι διαφέρουν. Ειδικότερα, αν και και οι δύο αλγόριθμοι υπολογίζουν τις νέες αναπαραστάσεις (\en{embeddings}) $E^L$, ο αλγόριθμος \en{RoWSS} χρησιμοποιεί έναν πιο σύνθετο μηχανισμό για τη συγκρότηση των $C^{L+1}$.\par

Ο πιο σύνθετος μηχανισμός περιλαμβάνει τη διεργασία της \textquote{εύρεσης των δεικτών νικητών} (\en{find winner indices}). Πρόκειται ουσιαστικά για έναν μηχανισμό που στην πρώτη φάση του υπολογίζει τα σκορ ομοιότητας (\en{similarity scores}) της κάθε γραμμής, για κάθε δισδιάστατο πίνακα $A_j^L$. Το σκορ αυτό προκύπτει από την πράξη $SC_j^L \gets (\sum_{i_2}^{n^L} A^L_{j:i_2})^T$ η οποία πραγματοποιείται στη γραμμή \ref{op:method3_max_sum_of_vector_columns} του αλγορίθμου. Κατά αυτόν τον τρόπο, συνολικά, για κάθε $j$ δημιουργείται ο πίνακας $SC^L \in [-1, 1]^{n^{L+1}\times n^L}$.\par

Ποιοτικά, ο πίνακας $SC^L_j$ (για μια τυχαία κάψουλα γονέα $C_j^{L+1}$) δείχνει τη συνολική ομοιότητα που εμφανίζει κάθε ψήφος $V_{ji}^L$ με όλες τις ψήφους $V_{j:}^L$ (συμπεριλαμβανομένου και του εαυτού της). Συνεπώς, έχοντας μια τέτοια μετρική είναι εύκολο έπειτα να επιλεγεί μια ψήφος\textendash εκπρόσωπος ως αυτή που εμφανίζει τη μεγαλύτερη ομοιότητα με τις υπόλοιπες (γραμμή \ref{op:method3_max_argmax_say_that_input_is_a_column} όπου η πράξη $argmax$ δέχεται διανύσματα στήλες). Προτού γίνει αυτό όμως, στη γραμμή \ref{op:method3_max_softmax_like_previous_softmax} του αλγορίθμου λαμβάνει χώρα η πράξη ομαλής μεγιστοποίησης (\en{softmax}) των σκορ ανά $j$ και λαμβάνεται ο πίνακας $SoftSC^L$ (ο ορισμός της πράξης ομαλούς μεγιστοποίησης είναι ίδιος με αυτόν της σχέσης \ref{eq:softmax_algorithm}).\par

Η εφαρμογή της συνάρτησης ομαλής μεγιστοποίησης (\en{softmax}) έχει σαν στόχο να επιβάλει τον περιορισμό $\sum_i^{n^L} SC_{ji}^L = 1$. Εκφρασμένο διαφορετικά, έχει σκοπό να επιβάλει την υπόθεση μοναδικού πατέρα (\en{single parent assumption})\footnote{Φυσικά, όπως και σε όλους τους αλγορίθμους δρομολόγησης, μπορεί μια κάψουλα $C_i^L$ να μοιράσει την ψήφο της σε δύο κάψουλες γονείς αλλά ποτέ να δώσει ολοκληρωτικά την ψήφο της (συντελεστής δρομολόγησης μονάδα) και στις δύο.}. Με τη (προαιρετική) πράξη αυτή, προκαλούμε ανταγωνισμό μεταξύ των καψουλών του επόμενου επιπέδου ($C^{L+1}$) στο να προσελκύσουν όσο το δυνατόν περισσότερες ψήφους. Επίσης, μοναδικό χαρακτηριστικό του αλγορίθμου είναι ότι με μη\textendash επαναληπτικό τρόπο επιτυγχάνεται αυτό που έχουμε ονομάσει στα προηγούμενα κεφάλαιο ως ανατροφοδότηση από πάνω προς τα κάτω (\en{top down feedback}). Για παράδειγμα, έστω ότι μια κάψουλα $C_i^L$ έχει ψήφους $V_{j_1i}^L$ και $V_{j_2i}^L$ που εμφανίζουν μεγάλη ομοιότητα με τις υπόλοιπες ψήφους για τα $j_1$ και $j_2$ αντίστοιχα. Αντί να διαδραματίσει σημαντικό ρόλο στη διαμόρφωση και των δύο διανυσμάτων $C_{j_1}^{L+1}$ και $C_{j_2}^{L+1}$, λόγο της υπόθεσης μοναδικού πατέρα, θα λάβει ανατροφοδότηση από τους βαθμούς συμφωνίας με τις υπόλοιπες ψήφους και τελικά θα συμβάλλει σημαντικά στη διαμόρφωση μόνο της μιας εκ των καψουλών $C_{j_1}^{L+1}$ και $C_{j_2}^{L+1}$ (αυτή όπου η ψήφος της $V_{j_1i}^L$ είτε $V_{j_2i}^L$ συμφωνούσε λίγο περισσότερο με τις υπόλοιπες ψήφους $V_{j_1:}^L$ και $V_{j_2:}^L$ αντίστοιχα).\par

Συνεχίζοντας την περιγραφή του αλγορίθμου, χρησιμοποιώντας τον νέο πίνακα $ SoftSC^L $ βρίσκονται οι δείκτες των γραμμών με το μεγαλύτερο σκορ (δείκτες των νικητών, των νέων αναπαραστάσεων δηλαδή που θα εκπροσωπήσουν την κλάση). Οι δείκτες αυτοί οργανώνονται σε έναν πίνακα $ Winners^L \in \mathbb{Z}^{n^{L+1}\times 1} $, ένα δείκτη δηλαδή για κάθε κάψουλα $C_j^{L+1}$. Έτσι, πολύ εύκολα θέτουμε στα διανύσματα των καψουλών του επόμενου επιπέδου τις νέες αναπαραστάσεις που αντιστοιχούν στους νικητές (αφού πρώτα εφαρμόσουμε τη συνάρτηση σύνθλιψης (\en{squash})).\par

Στο σημείο αυτό κρίνεται ωφέλιμο να αναφέρουμε τα εξής:
\begin{itemize}
  \item Στην υλοποίησή μας, υπάρχει (προαιρετικά) η δυνατότητα για κλιμάκωση των αναπαραστάσεων $E^L$ με τα αντίστοιχά τους $SoftSC^L$ προτού εκχωρηθούν στις κάψουλες $C^{L+1}$. Δηλαδή, η πράξη θα ήταν:
  \begin{equation}
    \label{eq:scaleEmb}
    \forall j \in \Omega_{L+1}, \forall i \in \Omega_{L}: ScaledE_{ji}^L \gets E_{ji}^L \ast SoftSC^L_{ji}
  \end{equation}
  Αυτή η εκχώρηση θα λάμβανε χώρα μετά το βήμα \ref{op:method3_max_scale_embeddings} όπου θα έπρεπε να τροποποιήσουμε τη διαδικασία $NewEmb$ να επιστρέφει και τον πίνακα $SoftSC^L$.
  \item Υπάρχει μια παραλλαγή του αλγορίθμου \en{RoWSS} που ακούει στο όνομα \en{RoWL} ο οποίος αντί για τη διαδικασία \en{FindWinnerIndexes} χρησιμοποιεί τη διαδικασία \en{FindWinnerIndexesLength}. Ουσιαστικά, πρόκειται για τη διαδικασία που πραγματοποιεί την επιλογή των εκπροσώπων όχι με κριτήριο την αθροιστική ομοιότητα γραμμής ($SC^L$) αλλά με το μήκος των διανυσμάτων αναπαράστασης ($L_2$ νόρμα). Η μόνη διαφορά στην κύρια διεργασία του αλγορίθμου είναι ότι αντί για την κλήση της διαδικασίας $FindWinnerIndexes$ με όρισμα $\prescript{+}{}{A^{L}}$ γίνεται κλήση στη διαδικασία $FindWinnerIndexesLength$ με όρισμα τη μήτρα $V^L$. Φυσικά, και εδώ υπάρχει το προαιρετικό βήμα της κλιμάκωσης των νέων αναπαραστάσεων με τα στοιχεία του πίνακα $SoftSC^L$.
  \item Όλοι οι αλγόριθμοί μας που χρησιμοποιούν προσοχή (\en{RooMAV}, \en{RoWSS} και \en{RoWL}) υποστηρίζουν μηχανισμό προσοχής πολλών κεφαλών (\en{multi\textendash head attention}). Επειδή όμως η προσθήκη του πιο σύνθετου μηχανισμού δεν αλλάζει τη διαισθητική ερμηνεία των αντίστοιχων αλγορίθμων, η διαδικασία που υλοποιεί τον σύνθετο αυτό μηχανισμό αλλά και οι υπόλοιπες διαδικασίες που πρέπει να τροποποιηθούν ελαφρώς για να τον υποστηρίξουν παρουσιάζονται στο τέλος της ενότητας. 
\end{itemize}

\en{
\begin{algorithm}[H]
  \caption{\gr{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 2 (Αλγόριθμος \en{RoWSS})}}\label{alg:method3_max_rooting}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line
    \Procedure{Main-RoWSS}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $A^L \gets$ \Call{Self-Attention}{$V^L$}
      \State $\prescript{+}{}{A^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap}{$A^L$}
      \State $E^L \gets$ \Call{NewEmb}{$\prescript{+}{}{A^{L}}, V^L$} \Comment{Computes new, context-aware, votes.} 
      \State $Winners^L \gets$ \Call{FindWinnerIndexes}{$\prescript{+}{}{A^{L}}$} \label{op:method3_max_scale_embeddings}
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets E^L_{ji=Winners_j^L}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets E^L_{ji=Winners_j^Lk}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure
    % \item[] % Empty, unnumbered line
    \Procedure{ComputeVotes}{$C^L$}  \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: V_{ji:}^L \gets C_{i:}^L \times W_{ji::}^L$ \Comment{Equiv.: $V^L_{jik_{L+1}} \gets \sum_{k_{L}}^{d^L} C_{ik_L}^L \ast W_{jik_Lk_{L+1}}$}
      \State \Return $V^{L}$ \Comment{Output: $V^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure
    
    \Procedure{Self-Attention}{$V^L$}  \Comment{Input: $V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
      \State $\forall j \in \Omega_{L+1}: A_j^L \gets \frac{V^L_j \times {V_j^L}^T}{\sqrt{d^{L+1}}}$ \Comment{Equiv.: $A^L_{ji_1i_2} \gets \sum_k^{d^{L+1}} V^L_{ji_1k} \ast V^L_{ji_2k}$}
      \State \Return $A^{L}$ \Comment{Output: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

    \Procedure{ComputeNonNegativeAttentionMap}{$A^L$}\Comment{Input: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \State $\forall j \in \Omega_{L+1}:  \prescript{+}{}{A^{L}_j} \gets \mathbf{ReLU}(A^L_j)$ \Comment{Equiv.: $\prescript{+}{}{A^{L}_{ji\tilde{i}}} \gets ReLU(A^L_{ji\tilde{i}})$} \label{op:method3_max_pointwise_relu_this_op_may_be_different}
    \State \Return $\prescript{+}{}{A^{L}}$ \Comment{Output: $\prescript{+}{}{A^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

    \Procedure{NewEmb}{$\prescript{+}{}{A^{L}}, V^L$} \Comment{Input: $\prescript{+}{}{A^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}, V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
    \State $\forall j \in \Omega_{L+1}: E^L_j \gets \prescript{+}{}{A_j^L} \times V_j^L$ \Comment{Equiv.: $E^L_{jik} \gets \sum_{\tilde{i}}^{n^L} \prescript{+}{}{A_{ji\tilde{i}}^L} \ast V^L_{j\tilde{i}k}$} \label{op:method3_max_weighted_sum} % Πες για την ερμηνεία, πρόκειται για σταθμισμένο άθροισμα των votes. Αλλά εσύ το κάνες για κάθε ένα row. Υπολογίζεις μια νέα αναπαράσταση (embeding) που είναι context-aware, δηλαδή σχηματίζεται από όλες τις ψήφους (για την κάψουλα j) που συμφωνούν με την εκάστοτε κάψουλα i (εκπρόσωπος γραμμής). Από εδώ είναι εύκολο να πας λογικά στον επόμενο αλγόριθμο που βρίσκει το max row.
    \State \Return $E^L$ \Comment{Output: $E^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure

    \Procedure{FindWinnerIndices}{$A^L$} \Comment{Input: $A^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \State $\forall j \in \Omega_{L+1}: SC_j^L \gets (\sum_{i_2}^{n^L} A^L_{j:i_2})^T$ \Comment{Equiv.: $SC_{ji}^L \gets \sum_{i_2}^{n^L} A^L_{jii_2}$} \label{op:method3_max_sum_of_vector_columns} % Επίσης, πες που ανήκει το SC (διαστάσεις). Πες ότι το άθροισμα αριστερά είναι ισοδύναμο με το  $\forall j \in \Omega_{L+1}: SC_j^L \gets \sum_{i_1}^{n^L} A^L_{ji_1:}$ επειδή ο πίνακας Α είναι συμμετρικός (άθροισμα γραμμών == άθροισμα στηλών).
    \State \Comment{$SC^L \in \Re^{n^{L+1} \times n^L}$}
    \State $\forall i \in \Omega_L: SoftSC_{:i}^L \gets softmax(SC^L_{:i})$ \Comment{Equiv.: $ SoftSC_{ji}^L \gets \frac{\exp(SC_{ji}^L)}{\sum_j^{n^{L+1}} \exp(SC_{ji}^L)}$} \label{op:method3_max_softmax_like_previous_softmax}
    \State $\forall j \in \Omega_{L+1}: Winners_j^L \gets \underset{i \in [1,n^L]}{\mathrm{argmax}}(SoftSC_j^L)$ \label{op:method3_max_argmax_say_that_input_is_a_column}
    \State \Return $Winners^L$ \Comment{Output: $Winners^L \in \mathbb{Z}^{n^{L+1} \times 1}$}
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}

\subsubsection{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 3 (Αλγόριθμος \en{RoWL})}
Πρόκειται ουσιαστικά για την παραλλαγή του αλγορίθμου \en{RoWSS} που χρησιμοποιεί τη διεργασία $FindWinnerIndexesLength$. Με άλλα λόγια, όπως προαναφέρθηκε, αλλάζει το κριτήριο επιλογής των αναπαραστάσεων - εκπροσώπων από αυτό των $SC^L$ σε αυτό του μήκους των διανυσμάτων αναπαράστασης. Ο αλγόριθμος αυτός προέκυψε φυσικά από την παρατήρηση ότι ψήφοι ($V^L_{ji}$) που έχουν μεγάλο βαθμό ομοιότητας γραμμής ($SC^L_{ji}$) έχουν και μεγάλο μήκος διανύσματος αναπαράστασης (\en{embedding} $E^L_{ji}$). Συνεπώς, επαρκές κριτήριο επιλογής εκπροσώπων είναι το μήκος τους ($L_2$ νόρμα). Όπως και στον αλγόριθμο \en{RoWSS}, η γραμμή \ref{op:method3_max_alternative_softmax_like_previous_softmax} είναι προαιρετική και σκοπό έχει να προκαλέσει ανταγονισμό μεταξύ των καψουλών γονέων. Τέλος, να αναφέρουμε ότι και εδώ υπάρχει η δυνατότητα υπολογισμού της μήτρας $SoftSC^L$ και της κλιμάκωσης των νέων αναπαραστάσεων με αυτή.


\en{
\begin{algorithm}[H]
  \caption{\gr{Αλγόριθμος Δρομολόγησης με Αυτο\textendash προσοχή 3 (Αλγόριθμος \en{RoWL})}}\label{alg:method3_max_len_rooting}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line
    \Procedure{Main-RoWL}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $A^L \gets$ \Call{Self-Attention}{$V^L$}
      \State $\prescript{+}{}{A^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap}{$A^L$}
      \State $E^L \gets$ \Call{NewEmb}{$\prescript{+}{}{A^{L}}, V^L$} \Comment{Computes new, context-aware, votes.} 
      \State $Winners^L \gets$ \Call{FindWinnerIndexesLength}{$V^L$} \label{op:}
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets E^L_{ji=Winners_j^L}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets E^L_{ji=Winners_j^Lk}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure

    \Procedure{FindWinnerIndexesLength}{$V^L$} \Comment{Input: $V^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: Length_{ji}^L \gets {\left\lVert V^L_{ji:}\right\rVert}_2 $ \Comment{Equiv.: $Length_{ji}^L \gets \sqrt{\sum_{k}^{d^{L+1}} (V^L_{jik})^2}$} 
    \State \Comment{$Length^L \in \Re^{n^{L+1} \times n^L}$}
    \State $\forall i \in \Omega_L: SoftLength_{:i}^L \gets softmax(Length^L_{:i})$ \Comment{Equiv.: $ SoftLength_{ji}^L \gets \frac{\exp(Length_{ji}^L)}{\sum_j^{n^{L+1}} \exp(Length_{ji}^L)}$} \label{op:method3_max_len_softmax_like_previous_softmax}
    \State $\forall j \in \Omega_{L+1}: Winners_j^L \gets \underset{i \in [1,n^L]}{\mathrm{argmax}}(SoftLength_j^L)$ \label{op:method3_max_len_argmax_say_that_input_is_a_row}
    \State \Return $Winners^L$ \Comment{Output: $Winners^L \in \Re^{n^{L+1} \times 1}$}
    \EndProcedure
  \end{algorithmic}
  \end{algorithm}
}




\subsection{Αρχιτεκτονική Αποκωδικοποιητή}

Οι αλγόριθμοί μας διαθέτουν δύο εναλλακτικές εκδοχές αποκωδικοποιητών. Ο πρώτος αποκωδικοποιητής που μπορεί να χρησιμοποιηθεί είναι όμοιος με αυτόν που χρησιμοποιείται στο έργο \cite{sabour2017dynamic} και φαίνεται στην εικόνα \ref{fig:method_1_decoder_architecture}. Πρόκειται για έναν απλό αποκωδικοποιητή με τρία πλήρως διασυνδεδεμένα επίπεδα. Στην υλοποίησή μας, κατά τη διάρκεια της εκπαίδευσης, εφαρμόζουμε μια μάσκα και μηδενίζουμε τις κάψουλες που δεν αντιστοιχούν στη σωστή κλάση. Κατά τον έλεγχο (\en{validation}) εφαρμόζουμε μια μάσκα που εκμηδενίζει όλες τις κάψουλες εκτός από αυτή που έχει το μεγαλύτερο μήκος. Ειδικά για το σύνολο δεδομένων \en{MultiMNIST} όπου έχουμε δύο προβλέψεις, εφαρμόζουμε δυο ξεχωριστές μάσκες εξόδου και τα δύο αποτελέσματα που προκύποτουν τα τροφοδοτούμε, ξεχωριστά, στον αποκωδικοποιητή.\par

Ο αλγόριθμός μας έχει τη δυνατότητα να χρησιμοποιήσει έναν ξεχωριστού είδους αποκωδικοποιητή βασισμένο σε συνελικτικά επίπεδα κλιμακωτού βηματισμού (\en{fractionally\textendash strided convolutional layers} ή απλά \en{deconvolution layers}). Πατώντας στις παρατηρήσεις των έργων \cite{shiri2020quick,liu2019fsc,luo2020capsnet} ότι ένας τέτοιος, εξελιγμένος μηχανισμός ανακατασκευής εικόνας παράγει καλύτερα αποτελέσματα, τον υιοθετήσαμε με παρόμοιες παραμέτρους. Συγκεκριμένα, ο δεύτερος αποκωδικοποιητής μας περιλαμβάνει ένα πλήρως διασυνδεδεμένο επίπεδο και τρία επίπεδα κλιμακωτού βηματισμού με βήματισμους (\en{strides}) 2,1 και 1 αντίστοιχα (από την είσοδο του αποκωδικοποιητή προς την έξοδο). Οι άλλες διαστάσεις είναι τέτοιες ώστε να έχουμε ως έξοδο εικόνα στο μέγεθος της εικόνας εισόδου.

\subsection{Συνάρτηση Σφάλματος και Λοιπά Στοιχεία Υλοποίησης}
Η πιο σημαντική διαφορά στα στοιχεία υλοποίησης των μεθόδων αυτής της ενότητας με τη μέθοδο \cite{sabour2017dynamic} είναι στη συνάρτηση σύνθλιψης (\en{squashing function}). Αναλυτικότερα, αυτή η συνάρτηση στις μεθόδους της ενότητας ορίζεται για ένα διάνυσμα εισόδου $x$ ως εξής\cite{mazzia2021efficient}:

\begin{equation}
    squash(x) = (1 - \frac{1}{\exp{\left\lVert x\right\rVert^2}}) \frac{x}{\left\lVert x\right\rVert}
\end{equation}

Πλην αυτής της εξαίρεσης, τα περισσότερα στοιχεία υλοποίησης είναι τέτοια ώστε να επιτρέπουν την άμεση σύγκριση με τον αλγόριθμο στο έργο \cite{sabour2017dynamic}. Για παράδειγμα, χρησιμοποιούμε ίδιο μηχανισμό για την εξασθένηση του ρυθμού μάθησης (\en{learning rate decay}). Σημειώνουμε ότι για την εκπαίδευση του αλγορίθμου χρησιμοποιείται ο βελτιστοποιητής (\en{optimizer}) \en{Adam}. Τέλος, αξίζει να αναφέρουμε ότι έχουμε αυτοματοποιήσει όλες τις διαδικασίες εκπαίδευσης και ελέγχου (\en{validation}) και κατά τη διάρκεια εκτέλεσης, παράγεται αναλυτικό αρχείο καταγραφής των παραμέτρων και των επιδόσεων.

\subsection{Αλγόριθμοι με Μηχανισμούς Αυτοπροσοχής Πολλών Κεφαλών}
% \subsubsection{Διαδικασία Προσοχής Πολλών Κεφαλών}
Στην ενότητα αυτή παρατίθενται οι εκδόσεις των τριών αλγορίθμων μας αλλά με την πιο σύνθετη περίπτωση της προσοχής πολλών κεφαλών (\en{multi\textendash head attention}). Επειδή η διαισθητική ερμηνεία των αλγορίθμων δεν μεταβάλλεται, παρατίθενται εδώ, στο τέλος της ενότητας χωρίς επεξήγηση.

\en{
\begin{algorithm}[h]
  % \renewcommand{\addcontentsline}[3]{}
  \floatname{algorithm}{Procedure}
  \caption{\gr{Διαδικασία Αυτο\textendash Προσοχής Πολλών Κεφαλών (\en{Multi\textendash Head Procedure})}}

  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $Wv^L \in \Re^{d^{L+1}, nh^L, d_v^L}, Wk \in \Re^{d^{L+1}, nh^L, d_k^L},$\\
  \hspace*{\algorithmicindent} $Wq \in \Re^{d^{L+1}, nh^L, d_k^L}, Wo \in \Re^{d_v^L \ast nh^L, d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Optional Trainable Parameters} $bv^L \in \Re^{nh^L \times d_v^L}, bk^L \in \Re^{nh^L \times d_k^L},$\\
  \hspace*{\algorithmicindent} $bq^L \in \Re^{nh^L \times d_q^L}, bo^L \in \Re^{d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line
    
    \Procedure{Multi-Head Self-Attention}{$V^L$}  \Comment{Input: $V^L \in \Re^{n^{L+1} \times n^L \times d^{L+1}}$} 
      \State $Q \gets V^L$
      \State $K \gets V^L$
      \State $V \gets V^L$
      \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: Vp_{j::h}^L \gets V_j^L \times Wv^L_{:h:}$ \Comment{Equiv.: $Vp_{jikh}^L \gets \sum_{k_1}^{d^{L+1}} V^L_{jik_1}\ast Wv^L_{k_1ik}$}
      \State \Comment $Vp^L \in \Re^{n^{L+1} \times n^L \times d_v^L \times nh^L}$
      \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: Qp_{j::h}^L \gets V_j^L \times Wq^L_{:h:}$ \Comment{Equiv.: $Qp_{jikh}^L \gets \sum_{k_1}^{d^{L+1}} V^L_{jik_1}\ast Wq^L_{k_1ik}$}
      \State \Comment $Qp^L \in \Re^{n^{L+1} \times n^L \times d_k^L \times nh^L}$
      \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: Kp_{j::h}^L \gets V_j^L \times Wk^L_{:h:}$ \Comment{Equiv.: $Kp_{jikh}^L \gets \sum_{k_1}^{d^{L+1}} V^L_{jik_1}\ast Wk^L_{k_1ik}$}
      \State \Comment $Kp^L \in \Re^{n^{L+1} \times n^L \times d_k^L \times nh^L}$
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_{L}: Vp_{ji::}^L \gets Vp_{ji::}^L + {bv^L}^T$
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_{L}: Kp_{ji::}^L \gets Kp_{ji::}^L + {bk^L}^T$
      \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_{L}: Qp_{ji::}^L \gets Qp_{ji::}^L + {bq^L}^T$
      \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: Amh_{j::h}^L \gets \frac{Qp^L_{j::h} \times {Kp_{j::h}^L}^T}{\sqrt{d^{L}_k}}$ \Comment{Equiv.: $Amh^L_{ji_1i_2h} \gets \sum_k^{d^{L+1}} Qp^L_{ji_1kh} \ast Kp^L_{ji_2kh}$}
      \State \Return $A^{L}, Vp^L$ \Comment{Output: $Amh^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L} \times nh^L}, Vp^L \in \Re^{n^{L+1} \times n^L \times d_v^L \times nh^L}$}
    \EndProcedure



  \end{algorithmic}
  \end{algorithm}
}

% \subsubsection{Συμπληρωματικές Διαδικασίες για τους Αλγορίθμους Προσοχής Πολλών Κεφαλών}

\en{
\begin{algorithm}[h]
  \floatname{algorithm}{Procedure}
  \caption{\gr{Συμπληρωματικές, Βοηθητικές Διαδικασίες (\en{Multi\textendash Head Helper Procedures})}}\label{alg:method3_multihead_helper_procedures}
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line

    \Procedure{ComputeNonNegativeAttentionMap2}{$Amh^L$}
    \State \Comment{Input: $Amh^{L} \in \Re^{n^{L+1} \times n^{L} \times n^{L} \times nh^L}$}
    \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: \prescript{+}{}{Amh^{L}_{j::h}} \gets \mathbf{ReLU}(Amh^L_{j::h})$ 
    \State \Comment{Equiv.: $\prescript{+}{}{Amh^{L}_{ji\tilde{i}h}} \gets ReLU(Amh^L_{ji\tilde{i}h})$} \label{op:method3_max_alternative_pointwise_relu_this_op_may_be_different_multihead}
    \State \Return $\prescript{+}{}{Amh^{L}}$ \Comment{Output: $\prescript{+}{}{Amh^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L} \times nh^L}$}
    \EndProcedure

    \Procedure{NewEmb2}{$\prescript{+}{}{Amh^{L}}, Vp^L$} 
    \State \Comment{Input: $\prescript{+}{}{Amh^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L} \times nh^L}, Vp^L \in \Re^{n^{L+1} \times n^L \times dv^{L} \times nh^L}$} 
    \State $\forall j \in \Omega_{L+1}, \forall h \in H^L: Ep^L_{j::h} \gets \prescript{+}{}{Amh_{j::h}^L} \times Vp_{j::h}^L$ 
    \State \Comment{Equiv.: $Ep^L_{jikh} \gets \sum_{\tilde{i}}^{n^L} \prescript{+}{}{Amh_{ji\tilde{i}h}^L} \ast V^L_{j\tilde{i}kh}$} \label{op:method3_max_alternative_weighted_sum} % Πες για την ερμηνεία, πρόκειται για σταθμισμένο άθροισμα των votes. Αλλά εσύ το κάνες για κάθε ένα row. Υπολογίζεις μια νέα αναπαράσταση (embeding) που είναι context-aware, δηλαδή σχηματίζεται από όλες τις ψήφους (για την κάψουλα j) που συμφωνούν με την εκάστοτε κάψουλα i (εκπρόσωπος γραμμής). Από εδώ είναι εύκολο να πας λογικά στον επόμενο αλγόριθμο που βρίσκει το max row.
    \State \Comment{$Ep^L \in \Re^{n^{L+1} \times n^L \times d_v^L \times nh^L}$}
    \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: Econcat_{ji}^L \gets Concatenate(E_{ji:h=1}^L, E_{ji:h=2}^L, \dots, E_{ji:h=nh^L}^L)$ 
    \State \Comment{Equiv.: $Econcat_{jik}^L \gets E_{ji\tilde{k}={(k-1) mod nh^L + 1}h={k div nh^L}}^L $} \label{op:weird_method3_alternative_modulo}
    \State \Comment{$Econcat^L \in \Re^{n^{L+1} \times n^L \times d_v^L \ast nh^L}$}
    \State  $\forall j \in \Omega_{L+1}: E_j^L \gets Econcat_j^L \times Wo^L$ \Comment{Equiv.: $E_{jik}^L \gets \sum_{k_2}^{nh^L \ast dv^L} Econcat_{jik_2}^L \ast Wo_{k_2k}^L$}
    \State  $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: E_{ji}^L \gets E_{ji}^L + bo^L$
    \State \Return $E^L$ \Comment{Output: $E^{L} \in \Re^{n^{L+1} \times n^{L} \times d^{L+1}}$}
    \EndProcedure

    \Procedure{AggregateAttentionHeads}{$\prescript{+}{}{Amh^L}$}
    \State \Comment{Input: $\prescript{+}{}{Amh^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L} \times nh^L}$}
    \State $\forall j \in \Omega_{L+1}, \forall i \in \Omega_L: \prescript{+}{}{A^{L}_{ji:}} \gets \sum_h^{nh^L} \prescript{+}{}{Amh^{L}_{ji:h}}$ \Comment{Equiv.: $\prescript{+}{}{A^{L}_{ji_1i_2}} \gets \sum_h^{nh^L} \prescript{+}{}{Amh^{L}_{ji_1i_2h}}$} \label{op:method3_max_alternative_pointwise_relu_this_op_may_be_different}
    \State \Return $\prescript{+}{}{A^{L}}$ \Comment{Output: $\prescript{+}{}{A^{L}} \in \Re^{n^{L+1} \times n^{L} \times n^{L}}$}
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}



% \subsubsection{Αλγόριθμος 1 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (Αλγόριθμος \en{Multihead RooMAV})}
\en{
\begin{algorithm}[h]
  \caption{\gr{Αλγόριθμος 1 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (\en{Multihead RooMAV})}}\label{alg:method3_sum_multihead_rooting}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}, Wv^L \in \Re^{d^{L+1}, nh^L, d_v^L},$ \\
  \hspace*{\algorithmicindent} $Wk \in \Re^{d^{L+1}, nh^L, d_k^L}, Wq \in \Re^{d^{L+1}, nh^L, d_k^L}, Wo \in \Re^{d_v^L \ast nh^L, d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Optional Trainable Parameters} $bv^L \in \Re^{nh^L \times d_v^L}, bk^L \in \Re^{nh^L \times d_k^L},$\\ 
  \hspace*{\algorithmicindent} $bq^L \in \Re^{nh^L \times d_q^L}, bo^L \in \Re^{d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line

    \Procedure{Main-RooMAV-Multihead}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $Amh^L, Vp^L \gets$ \Call{Multi-Head Self-Attention}{$V^L$}
      \State $\prescript{+}{}{Amh^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap2}{$Amh^L$}
      \State $E^L \gets$ \Call{NewEmb2}{$\prescript{+}{}{Amh^{L}}, Vp^L$} \Comment{Computes new, context-aware, votes.}
      \State $\prescript{+}{}{A^{L}}\gets$ \Call{AggregateAttentionHeads}{$\prescript{+}{}{Amh^{L}}$}
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets \sum_i^{n^L} E^L_{ji}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets \sum_i^{n^L} E^L_{jik}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure
    
  \end{algorithmic}
  \end{algorithm}
}

% \subsubsection{Αλγόριθμος 2 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (Αλγόριθμος \en{Multihead RoWSS})}
\en{
\begin{algorithm}[h]
  \caption{\gr{Αλγόριθμος 2 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (\en{Multihead RoWSS})}}\label{alg:method3_max_multihead_rooting}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}, Wv^L \in \Re^{d^{L+1}, nh^L, d_v^L},$\\
  \hspace*{\algorithmicindent} $Wk \in \Re^{d^{L+1}, nh^L, d_k^L}, Wq \in \Re^{d^{L+1}, nh^L, d_k^L}, Wo \in \Re^{d_v^L \ast nh^L, d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Optional Trainable Parameters} $bv^L \in \Re^{nh^L \times d_v^L}, bk^L \in \Re^{nh^L \times d_k^L},$\\
  \hspace*{\algorithmicindent} $bq^L \in \Re^{nh^L \times d_q^L}, bo^L \in \Re^{d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line

    \Procedure{Main-RoWSS-Multihead}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $Amh^L, Vp^L \gets$ \Call{Multi-Head Self-Attention}{$V^L$}
      \State $\prescript{+}{}{Amh^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap2}{$Amh^L$}
      \State $E^L \gets$ \Call{NewEmb2}{$\prescript{+}{}{Amh^{L}}, Vp^L$} \Comment{Computes new, context-aware, votes.}
      \State $\prescript{+}{}{A^{L}}\gets$ \Call{AggregateAttentionHeads}{$\prescript{+}{}{Amh^{L}}$}
      \State $Winners^L \gets$ \Call{FindWinnerIndexes}{$\prescript{+}{}{A^{L}}$}
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets E^L_{ji=Winners_j^L}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets E^L_{ji=Winners_j^Lk}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}

% \subsubsection{Αλγόριθμος 3 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (Αλγόριθμος \en{Multihead RoWL})}
\en{
\begin{algorithm}[h]
  \caption{\gr{Αλγόριθμος 3 με Αυτο\textendash Προσοχή Πολλών Κεφαλών (\en{Multihead RoWL})}}\label{alg:method3_max_len_multihead_rooting}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} DigitCaps $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} $W^L \in \Re^{n^{L+1} \times n^L \times d^L \times d^{L+1}}, Wv^L \in \Re^{d^{L+1}, nh^L, d_v^L},$\\
  \hspace*{\algorithmicindent} $Wk \in \Re^{d^{L+1}, nh^L, d_k^L}, Wq \in \Re^{d^{L+1}, nh^L, d_k^L}, Wo \in \Re^{d_v^L \ast nh^L, d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Optional Trainable Parameters} $bv^L \in \Re^{nh^L \times d_v^L}, bk^L \in \Re^{nh^L \times d_k^L},$\\
  \hspace*{\algorithmicindent} $bq^L \in \Re^{nh^L \times d_q^L}, bo^L \in \Re^{d^{L+1}}$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line

    \Procedure{Main-RoWL-Multihead}{$C^L$} \Comment{Input: $C^L \in  \Re^{n^L \times d^L}$} 
      \State $V^L \gets$ \Call{ComputeVotes}{$C^L$}
      \State $Amh^L, Vp^L \gets$ \Call{Multi-Head Self-Attention}{$V^L$}
      \State $\prescript{+}{}{Amh^{L}} \gets$ \Call{ComputeNonNegativeAttentionMap2}{$Amh^L$}
      \State $E^L \gets$ \Call{NewEmb2}{$\prescript{+}{}{Amh^{L}}, Vp^L$} \Comment{Computes new, context-aware, votes.}
      \State $Winners^L \gets$ \Call{FindWinnerIndexesLength}{$V^L$}
      \State $\forall j \in \Omega_{L+1}: S^{L+1}_j \gets E^L_{ji=Winners_j^L}$ \Comment{Equiv.: $S^{L+1}_{jk} \gets E^L_{ji=Winners_j^Lk}$}
      \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets squash(S_j^{L+1})$
      \State \Return $C^{L+1}$ \Comment{Output: $C^{L+1} \in  \Re^{n^{L+1} \times d^{L+1}}$} 
    \EndProcedure

  \end{algorithmic}
  \end{algorithm}
}

\section{\en{SOM-Caps}}

Σε αυτήν την ενότητα παρουσιάζουμε την τέταρτη μέθοδο, την οποία αναπτύξαμε στην προσπάθειά μας να εξετάσουμε ορισμένες εναλλακτικές προσεγγίσεις των βασικών αρχών των νευρωνικών δικτύων με κάψουλες. Όπως θα δούμε στην συνέχεια, υπό ορισμένες παραμετροποιήσεις του αλγορίθμου μας, αυτός συμπεριφέρεται αρκετά διαφορετικά από τους βασικούς αλγορίθμους δρομολόγησης που χρησιμοποιούνται στα έργα \cite{sabour2017dynamic,hinton2018matrix}.\par

Πηγή έμπνευσης για την ανάπτυξη της μεθόδου αυτής ήταν ο αλγόριθμος για τον σχηματισμό του χάρτη αυτο\textendash οργάνωσης (\en{SOM algorithm}), όπως παρουσιάστηκε στην ενότητα \ref{sec:_SOM}. Η βασική ιδέα είναι ότι η ομαδοποίηση (\en{clustering}) που πραγματοποιεί ο αλγόριθμος \en{SOM}, με τις κατάλληλες τροποποιήσεις, αποτελεί μια κατάλληλη μέθοδος δρομολόγησης μεταξύ των επιπέδων \en{PrimaryCaps} και \en{DigitCaps}. Κατά αντιστοιχία, στην θέση των \en{datapoints} εμείς έχουμε ψήφους (\en{votes}) και στην θέση των κεντροειδών (\en{centroids}) έχουμε τις κάψουλες γονείς. Κατά αυτόν τον τρόπο, η δρομολόγηση ανάγεται στην εύρεση συστάδων στον χώρο των ψήφων και κατόπιν, στην ενημέρωση των \en{DigitCaps} με βάση τα κέντρα βάρους των συστάδων αυτών.\par

Φυσικά, έπρεπε να γίνουν αρκετές τροποποιήσεις στον σειριακό αλγόριθμο \en{SOM} ώστε αυτός να είναι ένας ικανοποιητικός αλγόριθμος δρομολόγησης. Μεταξύ αυτών των αλλαγών, όπως θα δούμε στην συνέχεια, είναι η παραλληλοποίηση της διαδικασίας ενημέρωσης των \textquote{κόμβων} (\en{DigitCaps} στην περίπτωσή μας). Μια ακόμα διαφορά είναι ότι η τοπολογική διάταξη (και για αυτό το λόγο, και η πλευρική απόσταση) δεν είναι τόσο προφανής αφού η διάταξη των κόμβων εξόδου δεν είναι προφανής. Με αυτές τις διαφοροποιήσεις, οδηγούμαστε στον \textquote{αλγόριθμο δρομολόγησης βασισμένο στον \en{SOM}}. Αξίζει στο σημείο αυτό να σημειώσουμε ότι, λόγω της πληθόρας των υπερπαραμέτρων που αλλάζουν σημαντικά την συμπεριφορά του αλγορίθμου, στην ουσία πρόκειται για μια οικογένεια αλγρορίθμων.\par

Στο παρόν κεφάλαιο ξεκινάμε περιγράφοντας τις δύο παραλλαγές της αρχιτεκτονικής του νευρωνικού δικτύου. Έπειτα, συνεχίζουμε με την ανάλυση του καινοτόμου αλγορίθμου δρομολόγησης. Αντιλαμβανόμενοι την πολυπλοκότητα του αλγορίθμου, παροτρύνουμε τον αναγνώστη στο παράρτημα \ref{chap:SOM_appendix} για μια αναλυτικότερη και συνάμα λιγότερο τυπική παρουσίαση του αλγορίθμου. Στην επόμενη ενότητα, αναφερόμαστε σε ορισμένες ποιοτικές διαφορές με τον δυναμικό αλγόριθμο δρομολόγησης μιας και πρωταρχικός σκοπός της παρούσας μεθόδου είναι η επιλεκτική χαλάρωση ορισμένων περιορισμών και η εξέταση των αποτελεσμάτων. Τέλος, γίνεται λόγος σε λοιπά στοιχεία υλοποίησης όπως ο βελτιστοποιητής και η συνάρτηση σφάλματος. 

\subsection{Αρχιτεκτονική Νευρωνικού Δικτύου}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{images/chapter method/forth_method_architecture.pdf}
  \caption{Στο σχήμα παρουσιάζεται η βασική αρχιτεκτονική της τέταρτης μεθόδου. Παρατηρούμε ότι μοιράζεται πολλά κοινά στοιχεία με το δίκτυο του έργου \cite{sabour2017dynamic}. Τα μεγέθη ύψους και πλάτους των χαρτών χαρακτηριστικών είναι υπολογισμένα για το σύνολο δεδομένων \en{MNIST}.\textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
  \label{fig:method_4_architecture}
\end{figure}

Για την τέταρτη μέθοδο αναπτύξαμε δύο αρχιτεκτονικές νευρωνικού δικτύου. Μια βασική αρχιτεκτονική και μια με ενα επιπλέον συνελικτικό επίπεδο που σκοπό έχει να μειώσει το υπολογιστικό κόστος. Η πρώτη από αυτές τις αρχιτεκτονικές απεικονίζεται στο σχήμα \ref{fig:method_4_architecture} και είναι τέτοια ώστε να διευκολύνεται η σύγκριση με το έργο \cite{sabour2017dynamic}. Επειδή αριστερό μέρος του δικτύου είναι ίδιο με αυτό προηγούμενων μεθόδων, δεν θα το περιγράψουμε αναλυτικά. Αρκεί να αναφέρουμε ότι για λόγους περιορισμού της πολυπλοκότητας, το μέγεθος των καψουλών εξόδου (\en{DigitCaps}) είναι ίσο με 8.

Το σημείο στο οποίο παρατηρούμε την μεγαλύτερη διαφορά εντοπίζεται στο επίπεδο εξόδου. Πιο συγκεκριμένα, στο νευρωνικό δίκτυο από κάψουλες με τον αλγόριθμο δρομολόγησης βασισμένο στο \en{SOM} (\en{SOM-Caps}) διακρύνουμε δύο εξόδους: μια για τις κάψουλες τελευταίου επιπέδου (\en{DigitCaps}) και μια για τις \textquote{ομοιότητες} (\en{similarities}). Ενώ η πρώτη έξοδος είναι παρούσα σε όλα τα νευρωνκά δίκτυα με κάψουλες, η δεύτερη έξοδος κάνει την αρχιτεκτονική της τέταρτης μεθόδου να διαφέρει αρκετά από τις υπόλοιπες.\par

Αναφορικά με την δεύτερη έξοδο, αυτή προκύπτει από την σύγκριση των αντίστοιχων προβλέψεων πόζας (ψήφων) της κάθε κάψουλας εξόδου με την κάψουλα αυτή. Με απλά λόγια, κάθε στοιχείο της εξόδου ομοιότητας (\en{similarity}) είναι μια μετρική του πόσο εύστοχες ήταν οι προβλέψεις των \en{PrimaryCaps} στην πρόβλεψη της πόζας της εκάστοτε κάψουλας εξόδου\footnote{Βέβαια, οι προβλέψεις, εφόσον συγκροτούν πυκνές συστάδες, συν\textendash διαμορφώνουν τα διανύσματα των καψουλών εξόδου (σε μικρότερο ή μεγαλύτερο βαθμό, ανάλογα με την παραμετροποίηση). Συνεπώς, οι ομοιότητες είναι και ένας βαθμός συμφωνίας των ψήφων μεταξύ τους. Περισσότερα σχετικά στην επόμενη ενότητα.}. Αυτό είναι και το διάνυσμα από το οποίο παράγεται η κλάση πρόβλεψης (με την πράξη $argmax$).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{images/chapter method/forth_method_architecture_small.pdf}
  \caption{Στο σχήμα παρουσιάζεται η μικρή αρχιτεκτονική της τέταρτης μεθόδου. Αποσκοπεί στην μείωση του αριθμού των \en{PrimaryCapsules} με την προσθήκη ενός ακόμα συνελικτικού επιπέδου στη βασική αρχιτεκτονική. Τα μεγέθη ύψους και πλάτους των χαρτών χαρακτηριστικών είναι υπολογισμένα για το σύνολο δεδομένων \en{MNIST}.\textit{Παράχθηκε από το \href{https://inkscape.org/}{\en{Inkscape}}}.}
  \label{fig:method_4_architecture_small}
\end{figure}

Κλείνοντας την ενότητα αυτή, στο σχήμα \ref{fig:method_4_architecture_small} αναπαριστάνεται η πιο ελαφριά (\en{light}) εκδοχή του \en{SOM-Caps}. Ενώ φαινομενικά η εισαγωγή ενός επιπλέον συνελικτικού επιπέδου θα επιβάρυνε περισσότερο το υπολογιστικό κόστος, η ελάτωση των \en{PrimaryCapsules} που συμμετέχουν στον αλγόριθμο δρομολόγησης βασισμένο στο \en{SOM} συνολικά, μειώνει σημαντικά τον χρόνο εκπαίδευσης και πρόβλεψης του δικτύου.


\subsection{Αλγόριθμος Δρομολόγησης}

Σε αυτήν την ενότητα παρουσιάζουμε την οικογένεια αλγορίθμων που αναπτύξαμε και βασίζονται στον \en{SOM}. Όπως προαναφέραμε, η κεντρική ιδέα είναι η χρήση του αλγορίθμου για την εύρεση των συστάδων στον πολυδιάστατο χώρο των ψήφων. Φυσικά, ανάλογα με την παραμετροποίηση, η συμπεριφορά του αλγορίθμου μπορεί να αλλάξει σημαντικά. Παρόλα αυτά, με μια τέτοια υλοποίηση μας δίνεται η δυνατότητα της επιλεκτικής χαλάρωσης ορισμένων υποθέσεων των νευρωνικών δικτύων από κάψουλες και η παρατήρηση των αποτελεσμάτων αυτής.\par

Όπως είδαμε στην ενότητα της αρχιτεκτονικής του δικτύου, το κυρίαρχο χαρακτηριστικό που την διαφοροποιεί από τις άλλες μεθόδους εντοπίζεται στην έξοδο του αλγορίθμου. Αναλυτικότερα, οι \en{DigitCaps}, στις περισσότερες παραμετροποιήσεις, αποτελούν παράμετροι του δικτύου που ενημερώνονται σε επίπεδο δέσμης (\en{batch}). Συνεπώς, δεν έχουμε ένα ξεχωριστό σύνολο από κάψουλες εξόδου σε κάθε παράδειγμα εισόδου (\en{instance}). Ένα τέτοιο γνώρισμα συνεπάγεται ότι τα διανύσματα \en{DigitCaps} δεν ενσωματώνουν τα μεταβαλλόμενα χαρακτηριστικά του εκάστοτε στιγμιοτύπου \en{equi\textendash variant}.\par

Αντίθετα με την έξοδο \en{DigitCaps}, η έξοδος η οποία χαρακτηρίζει την κάθε εικόνα εισόδου είναι το διάνυσμα με τις ομοιότητες (\en{similarities}). Ποιοτικά, αυτό περιέχει τον μέσο βαθμό συμφωνίας των ψήφων με τα διανύσματα αναπαράστασης των \en{DigitCaps}. Αν θεωρήσουμε ότι τα διανύσματα των καψουλών εξόδου παραμένουν σταθερά, μεγάλος βαθμός συμφωνίας με μια κάψουλα εξόδου συνεπάγεται ότι οι ψήφοι συμφωνούν στο διάνυσμα που χρησιμοποιείται για την αναπαράσταση της συγκεκριμένης κλάσης που η κάψουλα\textendash γονέας αναπαριστά.\par

Φυσικά, υπό διαφορετικές παραμετροποιήσεις (π.χ. μέγεθος δέσμης = 1) ο αλγόριθμος μπορεί να εξάγει διαφορετικά \en{DigitCaps} για κάθε εικόνα εισόδου. Επίσης, με μικρή τροποποίηση, έχει την ίδια δυνατότητα και για μέγεθος δέσμης μεγαλύτερο της μονάδας. Παρόλα αυτά, κάτι τέτοιο είχε δοκιμαστεί και οδηγούσε σε απαγορευτικούς χρόνους εκπαίδευσης αφού για κάθε δείγμα έτρεχε ένας (τροποποιημένος) αλγόριθμος \en{SOM} για την εκ νέου προσαρμογή των \en{DigitCaps}.\par 

Σε κάθε περίπτωση, μια υπόθεση που παραμένει ακλόνητη είναι ο ανταγωνισμός μεταξύ των \en{DigitCaps} για το ποιά θα εξηγήσει την κάθε κάψουλα παιδί. Αυτή που εμφανίζει την μεγαλύτερη συμφωνία, κερδίζει την συγκεκριμένη κάψουλα και το διάνυσμά της τελευταίας θα ληφθεί υπ'όψην στην ενημέρωση της \en{DigitCap}. Στις παραγράφους που ακολουθούν θα γίνουν ακόμα πιο σαφείς οι ποικίλες πτυχές του αλγορίθμου με την περιγραφή των υπερπαραμέτρων που τον ελέγχουν.

\subsubsection{Υπερπαράμετροι Αλγορίθμου}

Ο αλγόριθμος, λόγο της φύσης του, ενσωματώνει μια πληθόρα υπερπαραμέτρων για τον πειραματισμό με τις διάφορες υποθέσεις των νευρωνικών δικτύων από κάψουλες. Για αυτό, πριν γίνει η φορμαλιστική περιγραφή του αλγορίθμου κρίνεται σκόπιμη η περιγραφή της κάθε παραμέτρου που ο χρήστης μπορεί να τροποποιήσει (εξωτερικά, χωρίς την γνώση του κώδικα) καθώς και την επίδραση που αυτές έχουν στην διαμόρφωση του αλγορίθμου. Σημειώνουμε ότι αυτή η πληροφορία, μαζί με τον αναλυτικό αλγόριθμο καταγράφονται και στο παράρτημα \ref{chap:SOM_appendix}.\par

Οι υπερπαράμετροι που μπορεί να ρυθμίσει κανείς κατά την εκτέλεση του αρχείου\footnote{Ο κώδικας για όλες τις μεθόδους είναι αναρτημένος σε \href{https://github.com/abarmper/Capsule_Nets_with_uncertainty}{αυτή} την ιστοσελίδα.} είναι οι εξής:


\begin{description}
  \item[$reduced\_votes$] \hfill \\ Αυτή η παράμετρος, όταν τίθεται ενεργή, αίρει τον περιορισμό του να αντιστοιχούν ξεχωριστοί πίνακες μετασχηματισμού ($W_{ij}^L$) για κάθε κάψουλα επιπέδου $(L+1)$. Με άλλα λόγια, όταν εισάγεται ως όρισμα στην κλήση του εκτελέσιμου αρχείου, παράγονται τόσοι μετασχηματισμοί της εκάστοτε κάψουλας $C_i^L$ όσοι ορίζονται από μια άλλη υπερπαράμετρο, την $m$ (και όχι όσες είναι οι κάψουλες του επόμενου επιπέδου). Έτσι, κάθε κάψουλα $C^{L+1}_j$ έχει την ίδια όψη (\en{view}) των ψήφων. Σε τελική ανάλυση, αφήνεται στον αλγόριθμο ανταγωνιστικής μάθησης (τον αλγόριθμο \en{SOM}) να διαχωρίσει τις ψήφους και να τις συνδέσει με κάποια κλάση.
  
  \item[$m$] \hfill \\ Η παράμετρος αυτή ρυθμίζει τον αριθμό των πινάκων μετασχηματισμού, για κάθε κάψουλα $C_i^L$ (εφόσον η προηγούμενη παράμετρος είναι ενεργή). Έτσι, προκύπτουν $m$ ψήφοι για κάθε $C_i^L$. Συνολικά λοιπόν, κάθε \en{DigitCap} \textquote{βλέπει} $n^L \ast m$ ψήφους.
  
  \item[$radical$] \hfill \\ Η παράμετρος αυτή επηρεάζει άμεσα τον μηχανισμό ενημέρωσης των $C^{L+1}$. Όταν είναι ενεργή, τότε οι κάψουλες γονείς προκύπτουν σαν ένας μέσος όρος των ψήφων που μπορούν και εξηγούν καλύτερα (των ψήφων που κέρδισαν). Αυτό, διαφέρει από τον κλασικό αλγόριθμο \en{SOM} και την δική μας υλοποίηση χωρίς την συγκεκριμένη παράμετρο ενεργή όπου η ενημέρωση των κόμβων γίνεται με την πρόσθεση της διαφοράς τους με το \en{datapoint} που εξηγούν καλύτερα.\footnote{Φυσικά, σε κάθε περίπτωση, η μετρική που χρησιμοποιείται για την επιλογή του νικητή διαφέρει από τον αυθεντικό αλγόριθμο που παρουσιάσαμε στο \ref{sec:_SOM}. Εκεί χρησιμοποιείται η Ευκλείδια απόσταση ενώ εμείς χρησιμοποιούμε το εσωτερικό γινόμενο.}
  
  \item[$lr\_SOM$] \hfill \\ Ο ρυθμός μάθησης του αλγορίθμου \en{SOM-Based Routing}. Καθορίζει το βάρος των ενημερώσεων.
  \item[$r$] \hfill \\ Καθορίζει τον αριθμό των ενημερώσεων που θα γίνουν στα διανύσματα των \en{DigitCaps} όταν ο αλγόριθμος εκπαίδευσης έχει τροφοδοτηθεί με μια δέσμη παραδειγμάτων (\en{batch}). Όσο πιο πολλές ενημερώσεις εκτελούνται και όσο πιο μικρό είναι το μέγεθος της δέσμης τόσο πιο πολύ τα \en{DigitCaps} θα περιέχουν ιδιότητες που αφορούν τις συγκεκριμένες αναπαραστάσεις των αντικειμένων εισόδου (π.χ. προσανατολισμός, χρώμα κτλ.). Στην αντίθετη περίπτωση όπου οι ενημερώσεις είναι λίγες, ο ρυθμός μάθησης \en{SOM} μικρός και το μέγεθος της δέσμης μεγάλο, τα διανύσματα \en{DigitCaps} διαμορφώνονται έτσι ώστε να αναπαριστούν γενικά χαρακτηριστικά των κλάσεων που παραμένουν αμετάβλητα μέσα στα παραδείγματα μιας κλάσης (\en{instance\textendash invariant characteristics}). Σημειώνουμε ότι αν θέσουμε την παράμετρο ίση με τη μονάδα τότε ακολουθούμε την πολιτική \textquote{ο νικητής τα παίρνει όλα} (\en{winner takes it all}).
  \item[$Theta$] \hfill \\ Πρόκειται για μια λίστα με βάρη που καθορίζουν το μέγεθος της \textquote{γειτονιάς} του νικητή\footnote{Στο πλαίσιο του αλγορίθμου \en{SOM} ο νικητής ονομάζεται και \en{BMU (Best Matching Unit)}.} αλλά και το βάρος ενημέρωσης των γειτόνων (συμπεριλαμβανομένου του βάρους ενημέρωσης του νικητή). Με άλλα λόγια, είναι μια διακριτή συνάρτηση γειτνίασης με όρισμα την πλευρική απόσταση (\en{lateral distance}) και έξοδο το βάρος ενημέρωσης. Σημαντική διαφορά με τον αυθεντικό αλγόριθμο \en{SOM}, ωστόσο, είναι στο πως ορίζουμε αυτήν την πλευρική απόσταση (\en{lateral distance}). Αν θεωρήσουμε ότι η κάψουλα $C_j^{L+1}$ είναι το \en{BMU} για την κάψουλα $C_i^L$ τότε η $C^{L+1}_{\grave{j}}$ που είχε την δεύτερη μεγαλύτερη συμφωνία (εσωτερικό γινόμενο) με την συγκεκριμένη κάψουλα παιδί, θα έχει με το \en{BMU} πλευρική απόσταση ίση με 1. Προφανώς, μια τέτοια τοπολογική διάταξη δεν είναι σταθερή αλλά μεταβάλεται από δέσμη σε δέσμη. Για αυτό λέμε ότι ο αλγόριθμός μας δεν διαθέτει κάποια προφανή τοπολογική διάταξη, ούτε και είναι ποτέ σκοπός η δημιουργεία μιας τέτοιας.
  \item[$softmax$] \hfill \\ Αν θέσουμε αυτή τη παράμετρο ενεργή τότε η επιλογή των νικητών δεν είναι μια διαδικασία με μόνο δύο καταστάσεις. Αντιθέτως έχουμε μαλακούς νικητές (\en{soft winners}) που καθορίζονται από τον βαθμό ομοιότητας που εμφανίζουν με την κάθε ψήφο. Συνεπώς, στην διεκδίκηση της κάψουλας $C_i^L$ από τις $C^{L+1}$, όλες \textquote{κερδίζουν} και η συμβολή του διανύσματος  $C_i^L$ στην κάθε κάψουλα $C^{L+1}_j$ θα είναι ανάλογη του βαθμού ομοιότητας μεταξύ τους. Φυσικά, σε μια τέτοια περίπτωση, δεν έχει νόημα η ύπαρξη της γειτονιάς και ο αλγόριθμος διαφοροποιείται αρκετά από τον αυθεντικό \en{SOM}.
  \item[$tanh\_like$] \hfill \\ Η παράμετρος αυτή έχει παρόμοια επίδραση με την παράμετρο $softmax$ και είναι αμοιβαία αποκλεινόμενες (δεν επιτρέπεται και οι δύο να είναι ενεργές). Η διαφορά έγκειται ότι εφαρμόζεται επιπλέον μια οίσθηση και κλιμάκωση προκειμένου η σιγμοειδής συνάρτηση με είσοδο τον βαθμό ομοιότητας και έξοδο το βάρος ενημέρωσης να γίνεται πιο απότομη και ο αλγόριθμος να τείνει (σχεδόν) στην επιλογή σκληρών νικητών (\en{hard winners}).
  \item[$take\_into\_account\_win\_ratio$] \hfill \\ Σε περίπτωση που είναι ενεργή, πραγματοποιείται κλιμάκωση των ενημερώσεων που λαμβάνει μια κάψουλα $C^{L+1}_j$ με βάση το πόσοστό των $C^L_i$ που καλύτερα εξήγησε σε μια δέσμη δεδομένων. Στο σημείο αυτό υπενθυμίζεται ότι οι ενημερώσεις, για λόγους απόδοσης, γίνονται παράλληλα και ανά δέσμη (όχι ανά εικόνα εισόδου).
  \item[$take\_into\_account\_similarity$] \hfill \\ Παρόμοια λειτουργία με την προηγούμενη παράμετρο, μόνο που ο συντελεστής κλιμάκωσης της ενημέρωσης για μια κάψουλα $C_j^{L+1}$ σε αυτή την περίπτωση καθορίζεται από τον μέσο βαθμό ομοιότητας μεταξύ αυτής και των ψήφων που κέρδισε.
  \item[$norm\_type$] \hfill \\ Το είδος της συνάρτησης σύνθλιψης. Σε περίπτωση που είναι μηδέν εφαρμόζεται η κλασική συνάρτηση σύνθλιψης (\en{squash}). Σε περίπτωση που είναι μονάδα, χρησιμοποιείται $tanh$ \en{normalization} για την κανονικοποίηση των ψήφων και \en{unit normalization} για τις $C^{L+1}$.
  \item[$normalize\_votes$] \hfill \\ Χρησιμοποιείται σε περίπτωση που επιθυμούμε να κανονικοποιήσυμε τις ψήφους $V^L$.
  \item[$normalize\_d\_in\_loop$] \hfill \\ Αν η ρύθμιση αυτή είναι ενεργή, τότε οι κάψουλες $C^{L+1}$ κανονικοποιούνται αμέσως μετά από κάθε ενημέρωση. Συνήθως, βοηθάει στην ευστάθεια του αλγορίθμου όταν έχουμε μεγάλο αριθμό επαναλήψεων (παράμετρος $r$).
\end{description}

\subsubsection{Φορμαλιστική Παρουσίαση Αλγορίθμου}

Στην παράγραφο αυτή παρουσιάζουμε σε μια τυπική μορφή τον αλγόριθμο που αναπτύξαμε στην τέταρτη μέθοδο. Τον αλγόριθμο δεν τον συνοδεύουμε με πολλά σχόλια, αφενώς διότι στο παράρτημα \ref{chap:SOM_appendix} έχουμε αναπτύξει ενιά σελίδες περιγράφοντας τον εν λόγο αλγόριθμο και αφετέρου διότι οι μέχρι τώρα περιγραφές επαρκούν για την ποιοτική κατανόησή του.

\en{
\begin{algorithm}[h]
  \setstretch{1.2}
  \caption{\gr{Αλγόριθμος Δρομολόγησης Βασισμένος στον \en{SOM} (\en{SOM-Based Routing})}}\label{alg:method4_som}
  \hspace*{\algorithmicindent} \textbf{Input} PrimaryCaps $C^L \in \Re^{B \times n^L \times d^L}$\\
  \hspace*{\algorithmicindent} \textbf{Output} SimsOut $C^{L+1} \in \Re^{B \times n^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} If $reduced\_votes == True$: $W^L \in \Re^{n^{L} \times d^L \times d^{L+1} \times n^{L+1}}$ \\
  \hspace*{\algorithmicindent} \textbf{Trainable Parameters} If $reduced\_votes == False$: $W^L \in \Re^{n^{L} \times d^L \times d^{L+1} \times m^L}$ \\

  \hspace*{\algorithmicindent} \textbf{Non-Trainable Parameters} $C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$\\
  \hspace*{\algorithmicindent} \textbf{Hyperparameters} $reduced\_votes, m, normalize\_votes, norm\_type, r, radical, \Theta,$\\
  \hspace*{\algorithmicindent} $take\_into\_account\_win\_ratio, take\_into\_account\_similarity, softmax, tanh\_like,$\\
  \hspace*{\algorithmicindent} $normalize\_d\_in\_loop, lr\_SOM$
  \begin{algorithmic}[1]
    % \item[] % Empty, unnumbered line

    \Procedure{SOM-Based Routing}{$C^L$} \Comment{Input: $C^L \in  \Re^{B \times n^L \times d^L}$} 
      \Initializee{$C^{L+1} \gets UniformRandom([-1,+1])$} 
      \State \Comment{$C^{L+1} \in \Re^{n^{L+1} \times d^{L+1}}$}
      \If{$reduced\_votes$}
        \State $\forall b \in [1,B], \forall i \in \Omega_L, \forall j \in \Omega_{L+1}: V^L_{bij:} \gets C^L_{bi:} \times W^L_{i::j}$
      \Else
        \State $\forall b \in [1,B], \forall i \in \Omega_L, \forall j \in \Omega_{L+1}: preV^L_{b(i+n^L\ast (j_m-1))d^{L+1}} \gets C^L_{bi::} \times W^L_{i::j_m}$
        \State $\forall j \in \Omega_{L+1}: V^L_{::j:} \gets preV^L$ \Comment{Copy $n^{L+1} times.$}
      \EndIf
      \State \Comment{$V^L \in \Re^{B \times \underline{n^L}, n^{L+1, d^{L+1}}}}$ 
      \State \Comment{where $\underline{n}^L=n^L$ OR $n^L \ast m$, depending on the value of $reduced\_votes$}

      \If{$normalize\_votes$}
        \If{$norm\_type == 0$}
          \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L, \forall j \in \Omega_{L+1}: V^L_{bij} \gets Squash(V^L_{bij})$
          
        \Else
          \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L, \forall j \in \Omega_{L+1}: V_{bij}^L \gets tanh{\left\lVert V^L_{bij}\right\rVert } \frac{V_{bij}^L}{\left\lVert V_{bij}^L\right\rVert }$
        \EndIf
      \EndIf

      \If{$take\_into\_account\_win\_ratio$}
      \Initializee{$WinCount \gets zeros^{n^{L+1}}$}
      \EndIf
      \algstore{first_part}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
  \setstretch{1.2}
  \begin{algorithmic}[1]
    \algrestore{first_part}
      \For{$r$ iterations}
        \Initializee{$SU^L \gets zeros^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}
        \If{$radical$}
          \State $D^L \gets V^L$ \Comment{$D^L$ stores differences}
        \Else
          \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: D^L_{bi::} \gets V^L_{bi} - C^L$
        \EndIf
        \State \Comment{$D^L \in \Re^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}
        \Initializee{$M^L \gets ones^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}
        \State \Call{IterateOver$\Theta$}{$\Theta$} \Comment{Inner loop, check pseudocode below.}
        \State \Comment{Make update dense.}
        \State $U^L \gets \frac{\sum_b^B \sum_i^{\underline{n}^L} SU^L_{bi}}{B \ast \underline{n}^L}$ \Comment{$U^L \in \Re^{n^{L+1} \times d^{L+1}}$}

        \If{not $take\_into\_account\_win\_ratio$}

          \If{not $radical$}
            \State $C^{L+1} \gets C^{L+1} + U^L$
          \ElsIf{$radical$}
            \State $C^{L+1} \gets \frac{C^{L+1} \ast (r-1) + U^L}{r}$ \Comment{Moving Average}
          \EndIf

          \Else
            \State $WinRatio^L \gets \frac{WinCount^L}{\underline{n}^L \ast B}$
            \State $SoftWinRatio^L \gets softmax(WinRatio^L)$
            \State $\forall j \in \Omega_{L+1}: U^L_j \gets SoftWinRatio_j^L \ast U^L_j$
            \If{not $radical$}
              \State $C^{L+1} \gets C^{L+1} + U^L$
            \ElsIf{$radical$}
              \State $C^{L+1} \gets \frac{C^{L+1} \ast (r-1) + U^L}{r}$
            \EndIf
        \EndIf

        \If{$normalize\_in\_d\_loop$}
          \If{$norm\_type == 0$}
            \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets Squash(C_j^{L+1})$
          \Else
            \State $\forall j \in \Omega_{L+1}: C_j^{L+1} \gets tanh(\left\lVert C_j^{L+1}\right\rVert ) \frac{C_j^{L+1}}{\left\lVert C_j^{L+1}\right\rVert }$
          \EndIf
        \EndIf

      \EndFor
      


      \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: FinalSims^{L+1}_{bi} \gets \sum_k^{d^{L+1}} [V^L_{bi} \ast C^{L+1}]_{bik}$
      \State \Comment{$FinalSims \in \Re^{B \times \underline{n}^L \times n^{L+1}}$}
      \State $\forall b \in [1,B]: SimsOut_b^{L+1} \gets \frac{\sum_i^{\underline{n}^L} FinalSims^{L+1}_{bi}}{\underline{n}^L}$
      \State \Return $SimsOut^{L+1}$ \Comment{$SimsOut^{L+1} \in \Re^{B, n^{L+1}}$, optional output: $C^{L+1}$}
    \EndProcedure
    \algstore{second_part}
  \end{algorithmic}
  \end{algorithm}

\begin{algorithm}
  \setstretch{1.2}
  \begin{algorithmic}
    \algrestore{second_part}
    \Procedure{IterateOver$\Theta$}{$\Theta$}
      \ForAll{$\theta \in \Theta$}
            \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: Sims^L_{bi} \gets \sum_k^{d^L+1} [(V^L_{bi} \ast M^L_{bi}) \ast C^{L+1}]_{bi:k}$
            \State \Comment{$Sims^L \in \Re^{B \times \underline{n}^L, n^{L+1}}$}
            \If{not $softmax$ nor $tanh\_like$}
              \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: Jwinners^L_{bi} \gets \underset{j}{\mathrm{argmax}}(Sims^L_{bi})$
              \State \Comment{$Jwinners^L \in \Re^{B \times \underline{n}^L}$}
              \State $\forall b \in [1,B]: Mwinners^L_b \gets OneHot(Jwinners^L_{b};n^{L+1})$
              
            \ElsIf{$softmax$}
              \State $\forall b \in [1,B]: Mwinners^L_{b} \gets \underset{across j}{SoftMax}(Sims^L_b)$
            \ElsIf{$tanh\_like$}
              \State $\forall b \in [1,B]: Mwinners^L_b \gets (\underset{across j}{SoftMax}(Sims^L_b) - 0.5) \ast 2$
            \EndIf
            \State \Comment{$Mwinners^L \in \Re^{B \times \underline{n}^L \times n^{L+1}}$}

            \If{not $take\_into\_account\_similarity$ nor $take\_into\_account\_win\_ratio$}
              \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: Usparse^L_{bi} \gets (Mwinners^L_{bi} \times D^L_{bi}) \ast lr_SOM \ast \theta$
            \ElsIf{$take\_into\_account\_similarity$}
              \State $SimsSparse^L \gets Sims^L \ast Mwinners^L$
              \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L: Usparse_{bi}^L \gets (SimsSparse^L_{bi} \times D^L_{bi}) \ast lr_SOM$
            \EndIf
            \State \Comment{$Usparse \in \Re^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}
            \If{$take\_into\_account\_win\_ratio$}
              \State $WinCountPartial^L \gets \sum_i^{\underline{n}^L} \sum_b^B Mwinners^L_{bi}$
              \State $WinCount \gets WinCount + WinCountPartial$ \Comment{$WinCount \in \Re^{n^{L+1}}$}

            \EndIf
            \If{not $softmax$ nor $tanh\_like$}
              \State $\forall b \in [1,B], \forall i \in \underline{\Omega}_L, \forall j \in \Omega_{L+1}: M^L_{bij:} \gets M^L_{bij:} - Mwinners^L_{bij}$
              \State \Comment{$M^L \in \Re^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}
            \EndIf
            \State \Comment{Aggregate updates across $\theta$.}
            \State $SU^L \gets SU^L + Usparse^L$ \Comment{$SU^L \in \Re^{B \times \underline{n}^L \times n^{L+1} \times d^{L+1}}$}

      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
}
\subsection{Λοιπά Στοιχεία Υλοποίησης}

Στον αλγόριθμο που περιγράψαμε χρησιμοποιείται ο βελτιστοποιητής \en{nAdam} ενώ οι περισσότερες παράμετροι εκπαίδευσης (π.χ. ρυθμός μάθησης) είναι παραμετροποιήσημες (ακόμα και η αρχιτεκτονική του πρώτου τμήματος του νευρωνικού δικτύου). Να αναφέρουμε ακόμα ότι δεν χρησιμοποιείται κάποιος προγραματισμός για την σταδιακή ελάτωση του ρυθμού εκπαίδευσης. \par

Και σε αυτήν την αρχιτεκτονική έχουμε την δυνατότητα ανακατασκευής της εικόνας εισόδου χρησιμοποιώντας δύο ξεχωριστά είδη αποκωδικοποιητών (ακριβώς ίδια με αυτά της προηγούμενης μεθόδου). Συγκεκριμένα, χρησιμοποιούμε έναν αποκωδικοποιητή όπως αυτόν της πρώτης μεθόδου αλλά και έναν αποκωδικοποιητή βασιζόμενο σε επίπεδα αποσυνέλιξης. Όπως είναι φυσικό, οι αποκωδικοποιητές μεταβάλλονται ελαφρώς ανάλογα με τα δεδομένα εικόνων που καλούνται να ανακατασκευάσουν\footnote{Η συγκεκριμένη μέθοδος μπορεί με την αλλαγή μιας παραμέτρου να δοκιμαστεί στα σύνολα δεδομένων \en{MNIST, Cifar10, FashionMNIST, MultiMNIST} και \en{smallNORB}}.\par

Τέλος, αναφορικά με την συνάρτηση σφάλματος ειδεικά για το σύνολο δεδομένων υποχρεωτικά πρέπει να χρησιμοποιηθεί \en{MarginLoss} το (όπως το ορίσαμε σε προηγούμενες μεθόδους) ενώ για τα υπόλοιπα (όπου έχουμε μια κλάση πρόβλεψης) υπάρχει η δυνατότητα επιλογής ανάμεσα σε αυτό και στο \en{CategoricalCrossEntropyLoss}.\par
