\chapter{Πειραματική Μελέτη}

Στην παρούσα ενότητα παρουσιάζουμε τα αποτελέσματα των πειραμάτων που διενεργήθηκαν στην κάθε μια οικογένεια αλγορίθμων. Παρόλα αυτά, δεν είναι σκοπός η βελτιστοποίηση της απόδοσης (όπως καταγράφεται από τις επιλεγμένες μετρικές) για κάθε αλγόριθμο. Όπως έχουμε αναφέρει, ο σκοπός της παρούσας διπλωματικής είναι διττός: αφενός επιθυμούμε να εξερευνήσουμε την επίδραση της χαλάρωσης ορισμένων υποθέσεων των νευρωνικών δικτύων με κάψουλες στην απόδοσή τους (μέθοδος 1) και αφετέρου να επιλύσουμε το πρόβλημα της κλιμακωσιμότητας προτείνοντας έναν αποδοτικό αλγόριθμο δρομολόγησης (μέθοδος 3). Η τέταρτη, πολυδύναμη, μέθοδος, βρίσκεται στο μεταίχμιο αυτών με τη δυνατότητα τόσο για επιλεκτική χαλάρωση των περιορισμών της εν λόγω τεχνολογίας όσο για μερική βελτίωση του χρόνου εκπαίδευσης. Τέλος, η δεύτερη μέθοδος, λόγο της μεγάλης υπολογιστικής πολυπλοκότητάς της που δεν επέτρεπε τον εκτενή πειραματισμό, περιορίζεται σε δύο σύνολα δεδομένων και αναλαμβάνει τον σκοπό τις σύγκρισης με τις υπόλοιπες μεθόδους μας.\par

Όπως γίνεται αντιληπτό, κυρίως για τους αλγορίθμους της μεθόδου 1 αλλά και για αυτούς της μεθόδου 4, μας ενδιαφέρει περισσότερο η σχετική επίδοση μεταξύ αυτών αφού αυτή φανερώνει αν οι περιορισμοί που επιβάλλονται τελικά συμβάλουν στην καλύτερη γενίκευση του δικτύου ή όχι. Για τον λόγο αυτό, δίνουμε έμφαση στη σύγκριση των επιδόσεων μεταξύ των αλγορίθμων που ανήκουν στην ίδια οικογένεια (ίδια μέθοδο). Βέβαια, για λόγους πληρότητας, επιλέγουμε τους καλύτερους αλγορίθμους από την κάθε μέθοδο και τους συγκρίνουμε στο τέλος του παρόντος κεφαλαίου.\par

Κρίνεται σκόπιμο να αναφερθεί πως σε κάθε περίπτωση, η πειραματική μας μελέτη δεν είναι πλήρης. Ορισμένοι αλγόριθμοι που αναπτύξαμε (και ειδικά αυτοί που απαντώνται στην πολυδύναμη τέταρτη μέθοδο) διαμορφώνονται από μια πληθώρα υπερπαραμέτρων όπου η κάθε μια επιδρά καθοριστικά στην απόδοσή τους. Επιπρόσθετα, οι μειωμένοι υπολογιστικοί πόροι που διαθέτουμε καθιστούν τη διαδικασία πειραματισμού ιδιαιτέρως χρονοβόρα. Συνεπώς, είναι χρήσιμο να έχουμε υπ' όψη ότι οι επιδώσεις που καταγράφουμε πιθανότατα να επιδέχονται βελτίωση.\par

Το παρόν κεφάλαιο ακολουθεί την εξής διάρθρωση:
\begin{enumerate}
    \item Αρχικά γίνεται μια σύντομη παρουσίαση των συνόλων δεδομένων που χρησιμοποιούμε, των μετρικών αλλά και της πλατφόρμας πειραματισμού.
    \item Έπειτα ακολουθούν οι πειραματικές μελέτες της κάθε μεθόδου ξεχωριστά. Τα περιεχόμενα της κάθε τέτοιας υποενότητας διαφέρουν σημαντικά ανάλογα με τον σκοπό της εκάστοτε μεθόδου. Σε γενικές γραμμές όμως, περιλαμβάνουν τα αποτελέσματα που προκύπτουν από την αναζήτηση ικανοποιητικών υπερπαραμέτρων στα διάφορα σύνολα δεδομένων και τη σύγκριση των αλγορίθμων μεταξύ τους.
    \item Τέλος, επιλέγουμε τους καλύτερους αλγορίθμους από διαφορετικές μεθόδους και τους συγκρίνουμε μεταξύ τους αλλά και με άλλες υλοποιήσεις νευρωνικών δικτύων με κάψουλες που συναντώνται στη βιβλιογραφία.
\end{enumerate}

\section{Πλατφόρμα Διεξαγωγής Πειραμάτων, Μετρικές και Σύνολα Δεδομένων} 
Στην ενότητα αυτή κάνουμε λόγο για τα αμετάβλητα στοιχεία που συνθέτουν το περιβάλλον της πειραματικής μας μελέτης. Αυτά περιλαμβάνουν το υπολογιστικό σύστημα στο οποίο διενεργήθηκαν όλα τα πειράματα, τις μετρικές που χρησιμοποιήθηκαν για την εκτίμηση της επίδοσης και τα σύνολα δεδομένων με τα οποία οι αλγόριθμοι τροφοδοτήθηκαν.
\subsection{Πειραματική Πλατφόρμα}
Όλα τα πειράματα εκτελέστηκαν τοπικά, στον προσωπικό υπολογιστή (\en{PC}). Επειδή το σύστημα εκτέλεσης των πειραμάτων επηρεάζει τους χρόνους εκπαίδευσης, κρίνεται σκόπιμη η περιγραφή των δυνατοτήτων του υπολογιστικού μας συστήματος. Από πλευράς υλικού (\en{hardware}) λοιπόν, τα χαρακτηριστικά του είναι τα εξής:
\begin{itemize}
    \item 16GB \en{DDR4 RAM}
    \item \en{AMD Ryzen} 9 3900\en{X, 12-core CPU}
    \item \en{Nvidia RTX 2070 super GPU}
\end{itemize}

Ένα πολύ καλό εργαλείο για την αντιστοίχηση της υπολογιστικής δυνατότητας μιας συσκευής σε μια μετρική για την αντιπαραβολή με τις δυνατότητες άλλων συστημάτων είναι το \en{\it{ai-benchmark}}. Τρέχοντας το σχετικό πρόγραμμα εκτίμησης δυνατοτήτων, λάβαμε, μεταξύ άλλων τα παρακάτω αποτελέσματα:
\begin{itemize}
    \item \en{\it{Device Inference Score: 12150}}
    \item \en{\it{Device Training Score: 12115}}
    \item \en{\it{Device AI Score: 24265}}
\end{itemize}
Βέβαια, το περιβάλλον πειραματισμού απαρτίζεται και από τις εκδόσεις των πακέτων λογισμικού που είναι εγκατεστημένες στο σύστημα. Για αυτό τον σκοπό, στην \href{https://github.com/abarmper/Capsule_Nets_with_uncertainty}{ιστοσελίδα} όπου είναι αναρτημένος ο κώδικας, έχουμε καταγράψει όλα τα απαιτούμενα πακέτα λογισμικού. Ενδεικτικά, τα βασικότερα στοιχεία λογισμικού είναι τα εξής:
\begin{itemize}
    \item \en{Platform: \it{Linux}, Release: \it{5.15.0-48-generic}, Version: \it{20.04.1-Ubuntu}}
    \item \en{CUDA version: \it{11.0}}
    \item \en{cudnn version: \it{8}}
    \item \en{Tensorflow version: \it{2.4.1}}
    \item \en{Pytorch version: \it{1.7.1+cu110}}
\end{itemize}
Για να διευκολύνουμε την αναπαραγωγή πειραμάτων, ενσωματώνουμε και ένα εικονικό περιβάλλον (\en{Docker}). Το σχετικό αρχείο (\en{DockerFileGenericGPU}) δημιουργεί ένα εικονικό περιβάλλον με τις τα απαραίτητα πακέτα λογισμικού (\en{dependences}) που χρειάζεται να είναι εγκατεστημένα για την εκτέλεση των προγραμμάτων.
\subsection{Μετρικές Επίδοσης}

Λόγο του ερευνητικού χαρακτήρα της παρούσας διπλωματικής, μας ενδιαφέρει κυρίως η σύγκριση των μεθόδων μας με τις υπόλοιπες σχετικές μεθόδους που απαντώνται στη βιβλιογραφία. Για τον σκοπό αυτό και με δεδομένο ότι όλες οι εργασίες είναι εργασίες ταξινόμησης, η μετρική της ακρίβειας (\en{accuracy}) είναι η πλέον κατάλληλη μετρική. Η μετρική αυτή ορίζεται από τον λόγο των σωστά ταξινομημένων προβλέψεων προς το σύνολο των προβλέψεων. Με μαθηματικούς όρους δηλαδή, έχουμε:
\begin{equation}
    Accuracy = \frac{\text{\en{Number of Correct Predictions}}}{\text{\en{Total Number of Predictions}}}
\end{equation}
Ειδικά για το σύνολο δεδομένων \en{MultiMNIST} όπου έχουμε δύο προβλέψεις για κάθε δείγμα εισόδου, η μετρική μας ονομάζεται πολλαπλή\textendash Ακρίβεια (\en{multi-Accuracy}). Παρόλα αυτά, σύμφωνα με τον παραπάνω ορισμό (που εστιάζει στον αριθμό των προβλέψεων και όχι των δειγμάτων εισόδου) οι δύο μετρικές είναι ταυτόσημες.\par

Συχνά, αντί για την ακρίβεια, χρησιμοποιείται σαν μετρική το ποσοστιαίο σφάλμα ελέγχου (\en{test error rate\%}). Στην πραγματικότητα, δεν αποτελεί μια ξεχωριστή μετρική αφού ισχύει ότι $$test\_error\_rate = 100 - Accuracy*100\%.$$ \par

Όπως έχει γίνει αντιληπτό από το πρώτο κεφάλαιο της εργασίας, μας ενδιαφέρει να εντοπίσουμε μια αρχιτεκτονική με μικρό υπολογιστικό κόστος. Δύο μετρικές που φανερώνουν την ποσότητα αυτή είναι ο (σχετικός) μέσος χρόνος εκπαίδευσης ενός συνόλου δέσμης και ο αριθμός των εκπαιδευόμενων παραμέτρων ενός μοντέλου (των παραμέτρων δηλαδή που ρυθμίζονται με τον αλγόριθμο της οπισθοδιάδοσης σφάλματος). Συνεπώς, εκτός από την ακρίβεια, οι δύο αυτές μετρικές προστίθενται στα κριτήρια επιλογής των καλύτερων αλγορίθμων.

\subsection{Σύνολα Δεδομένων}
Στις περισσότερες μεθόδους μας χρησιμοποιούμε όλα τα σχετικά σύνολα δεδομένων με τα οποία δοκιμάζονται συνήθως οι αρχιτεκτονικές νευρωνικών δικτύων με κάψουλες. Γεγονός, που μας επιτρέπουν να εξετάσουμε αν τηρούνται οι χαρακτηριστικές ιδιότητες της εν λόγω τεχνολογίας από τα νέα μοντέλα που αναπτύξαμε. Τα σύνολα δεδομένων με τα οποία καταπιανόμαστε είναι τα \en{MNIST}\cite{deng2012mnist}, \en{FashionMNIST}\cite{Xiao2017FashionMNISTAN}, \en{CIFAR10}\cite{CIFAR10}, \en{MultiMNIST}\cite{sabour2017dynamic} και \en{smallNORB}\cite{lecun2004learning}. Για λόγους πληρότητας, στον πίνακα \ref{tab:exp_datasets} φαίνονται τα μοντέλα επιβλεπόμενης μάθησης που επιτυγχάνουν τη μέγιστη ακρίβεια για το κάθε σύνολο δεδομένων (καταγράφονται στην \href{https://paperswithcode.com/}{ιστοσελίδα} τον Οκτώβριο του 2022).


\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c } 
        \hline
        Dataset & Method & Test Error (\%) \\
        \hline 
         MNIST & Heterogeneous ensemble with simple CNN\cite{an2020ensemble} & 0.09 \\ 
         
         FashionMNIST & Fine-Tuning DARTS\cite{tanveer2021fine} & 3.09 \\ 
         
         CIFAR-10 & ViT-H/14\cite{dosovitskiy2020image_is_worth_16} & 0.5 \\ 
         
         MultiMNIST & CapsNet\cite{sabour2017dynamic} & 5.2 \\ 
         
         smallNORB & Heinsen Routing\cite{heinsen2019algorithm} & 0.90 \\ 
         
        \end{tabular}
        }
        \end{center}
        \caption{\label{tab:exp_datasets} Πίνακας που συγκεντρώνει το καλύτερο μοντέλο και την απόδοσή του, για κάθε σύνολο δεδομένων.}
    \end{table}
    \subsubsection{Περιγραφή Συνόλων Δεδομένων \en{smallNORB} και \en{multiMNIST}}
    Τα δύο σύνολα δεδομένων είναι λιγότερο δημοφιλή στην ακαδημαϊκή κοινότητα για αυτό αφιερώνουμε αυτή την παράγραφο για την περιγραφή τους. Και τα δύο εξετάζουν την ιδιότητα των νευρωνικών δικτύων από κάψουλες να γενικεύουν σε νέες οπτικές γωνίες και να εξηγούν εικόνες με σημαντική επικάλυψη.\par

    Αναφορικά με το σύνολο δεδομένων \en{smallNORB}, αυτό περιέχει στερεο\textendash οπτικές εικόνες που απεικονίζουν 50 αντικείμενα (παιχνίδια) τα οποία ανήκουν σε 5 κλάσεις: τετράποδα ζώα, ανθρώπινες φιγούρες, αεροπλάνα, φορτηγά και αυτοκίνητα. Η κάθε κλάση εκπροσωπείται από δέκα φυσικά αντικείμενα, τα μισά εξ'αυτών βρίσκονται στο σύνολο εκπαίδευσης. Το σύνολο δεδομένων δημιουργήθηκε από την στερεοσκοπική λήψη αυτών των αντικειμένων από δύο κάμερες υπό 6 διαφορετικές συνθήκες φωτισμού, 9 διαφορετικά υψόμετρα προβολής (γωνίες 30◦έως 70◦ με βήμα 5◦) και 18 διαφορετικά αζιμούθια (γωνίες 0◦ έως 340◦ με βήμα 20◦). Έτσι, τόσο το σύνολο εκπαίδευσης όσο και το σύνολο ελέγχου αποτελούνται από 24.300 ζευγάρια στερεο\textendash πτικών εικόνων το καθένα. Οι αλγόριθμοί μας, δέχονται κάθε ζεύγος εικόνων σαν ένα δείγμα και στόχος τους είναι να προβλέψουν το απεικονιζόμενο αντικείμενο.\par

    Το σύνολο δεδομένων \en{multiMNIST} δημιουργήθηκε από τους \en{Hinton et al.} κατά την συγγραφή του έργου \cite{sabour2017dynamic}. Στην πραγματικότητα, ο αριθμός των δειγμάτων και τα περιεχόμενα του συνόλου δεν είναι προκαθορισμένα αφού η κάθε υλοποίηση κατασκευάζει δυναμικά το σύνολο αυτό λαμβάνοντας εικόνες από το σύνολο δεδομένων \en{MNIST} (αυτός είναι και ένας λόγος του περιορισμένου πειραματισμού με αυτό το σύνολο δεδομένων στη βιβλιογραφία). Ουσιαστικά, αποτελείται από εικόνες που απεικονίζουν στο ίδιο πλαίσιο, δύο επικαλυπτόμενα ψηφία (με ποσοστό επικάλυψης περίπου 80\%). Όπως είναι λογικό, κάθε ένα τέτοιο δείγμα συνοδεύεται από δύο ετικέτες: μια για κάθε απεικονιζόμενο ψηφίο.
    

\section{Πειραματική Μελέτη Μεθόδου 1}

Στην παρούσα ενότητα πραγματοποιούνται πειράματα στους τέσσερεις αλγορίθμους της μεθόδου 1 για να εκτιμηθεί η επίδοσή τους στα σύνολα δεδομένων \en{MNIST}\cite{deng2012mnist}, \en{FashionMNIST}\cite{Xiao2017FashionMNISTAN}, \en{CIFAR10}\cite{CIFAR10} και \en{smallNORB}\cite{lecun2004learning}. Υπενθυμίζεται ότι όλοι οι αλγόριθμοι της μεθόδου 1 έχουν ίδια αρχιτεκτονική με αυτήν που χρησιμοποιείται στο έργο \cite{sabour2017dynamic} αλλά διαφέρουν στον αλγόριθμο δρομολόγησης ο οποίος βαθμιαία, από τον πρώτο αλγόριθμο στον τέταρτο γίνεται απλούστερος. Συνεπώς, ο ρόλος των πειραμάτων της πρώτης μεθόδου δεν είναι απαραίτητα να προτείνει μια αντικατάσταση του δυναμικού αλγορίθμου δρομολόγησης. Βασικότερος σκοπός είναι να εξεταστεί η επίδραση που έχουν ορισμένες απλουστεύσεις του αλγορίθμου δυναμικής δρομολόγησης (αλγόριθμος \ref{alg:dynam_max_routing}) στη μάθηση (αλγόριθμοι \ref{alg:dynam_argmax_scaled_routing} και \ref{alg:dynam_argmax_routing}) και την επίδοση του αλγορίθμου μετά τη χαλάρωση της υπόθεσης περί φιλτραρίσματος υψηλής, πολυδιάστατης συμφωνίας (αλγόριθμος \ref{alg:dynam_max_routing}).\par

Μεταξύ των τεσσάρων αλγορίθμων που εξετάζονται, συμπεριλαμβάνεται και ο αυθεντικός αλγόριθμος της δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος 1). Αν και αυτός δεν αποτελεί μια δική μας μέθοδο, συμπεριλαμβάνεται στα πειράματα για να διευκολύνει την ισότιμη σύγκριση με τους άλλους αλγορίθμους της μεθόδου. Επίσης, οι υψηλές επιδόσεις που εντοπίζονται στον αλγόριθμο 1 πιστοποιούν την ορθή υλοποίηση της αρχιτεκτονικής του δικτύου που μοιράζονται και οι άλλες τρεις παραλλαγές.\par

Η διάρθρωση των πειραμάτων του κεφαλαίου έχει ως εξής: Για τα σύνολα δεδομένων \en{MNIST} και \en{CIFAR-10} κάνουμε πειράματα για τον εντοπισμό των υπερπαραμέτρων των μεθόδων (ρυθμός μάθησης κ.ο.κ.) με τα καλύτερα αποτελέσματα (χρησιμοποιώντας λίγες εποχές). Χρησιμοποιούμε αυτά τα δύο σύνολα καθώς έχουν μεγάλη ετερογένεια μεταξύ τους. Αφού βρεθούν αυτές οι υπερπαράμετροι, εμβαθύνουμε στο κάθε σύνολο δεδομένων ξεχωριστά εκπαιδεύοντας τους τέσσερεις αλγορίθμους με περισσότερες εποχές και με τις αντίστοιχες παραμετροποιήσεις τους ενώ έπειτα, τους συγκρίνουμε. Στην προ\textendash τελευταία υπο-ενότητα παρουσιάζουμε συγκεντρωτικά τα αποτελέσματα και αναφέρουμε τις παρατηρήσεις μας σχετικά με την επίδραση των υποθέσεών μας στις επιδόσεις. Στην τελευταία υπο-ενότητα παρουσιάζουμε τα συμπεράσματά μας από ορισμένα ειδικά πειράματα που αποσκοπούν να διερευνήσουν την εσωτερική λειτουργία των αλγορίθμων μας.\par

Σχετικά με τις λεπτομέρειες υλοποίησης να αναφέρουμε ότι η συνάρτηση σφάλματος είναι αυτή που έχει οριστεί στην ενότητα \ref{sec:method1_loss_fn}. Δηλαδή, είναι η συνάρτηση σφφάλματος περιθορίου (\en{Margin Loss}) με την προσθήκη του μέσου τετραγωνικού σφάλματος (κλιμακωμένου κατά $0.0005$) στην περίπτωση που χρησιμοποιείται αποκωδικοποιητής. Ο βελτιστοποιητής (\en{optimizer}) που χρησιμοποιούμε είναι ο \en{Adam} (\en{adaptive moment estimation}) ενώ χρησιμοποιούμε και σύστημα ελάττωσης του ρυθμού μάθησης όταν δεν παρατηρείται μείωση του σφάλματος στη διάρκεια ενός προκαθορισμένου αριθμού εποχών (\en{learning rate scheduler with reduce on plateau and patience}). Σε αρκετές περιπτώσεις, είναι απαραίτητη η χρήση ενός συνόλου επικύρωσης (\en{validation set}) το οποίο πάντα αποτελεί το 10\% του συνόλου δεδομένων εκπαίδευσης. Αξίζει να σημειωθεί επίσης ότι για την αποφυγή της υπερεκπαίδυσης (\en{overfitting}) αποθηκεύουμε το μοντέλο με το μικρότερο σφάλμα κατά την διάρκεια της εκπαίδευσης και χρησιμοποιούμε αυτό στο σύνολο ελέγχου.\par

Τέλος, σχετικά με την προεπεξεργασία των συνόλων δεδομένων, αυτή είναι όσο πιο πιστή γίνεται στο έργο \cite{sabour2017dynamic}. Πιο συγκεκριμένα, για το κάθε σύνολο δεδομένων έχουμε:
\begin{description}
    \item[\en{MNIST}] Κανονικοποίηση, ολίσθηση στον κατακόρυφο και οριζόντιο άξονα κατά 2 εικονοστοιχεία το μέγιστο (με \en{zero padding}). Μέγεθος εισόδου: [$1 \times 28 \times 28$]\footnote{Οι αλγόριθμοι της μεθόδου 1 είναι ανεπτυγμένοι στη γλώσσα \en{Pytorch} και συνεπώς, χρησιμοποιείται μια αναπαράσταση δεδομένων τύπου \en{channels-first}}.
    \item[\en{FashionMNIST}] Κανονικοποίηση μόνο. Μέγεθος εισόδου: [$1 \times 28 \times 28$]
    \item[\en{CIFAR10}] Κανικοποίηση για το καθένα από τα τρία κανάλια και πρερικοπή παραθύρων μεγέθους $28 \times 28$ (το αρχικό ύψος και πλάτος είναι $32 \times 32$). Μέγεθος εισόδου [$3 \times 28 \times 28$].
    \item[\en{smallNORB}] Κλιμάκωση σε μέγεθος $48 \times 48$ (από το αρχικό μέγεθος που είναι $96 \times 96$), κάνουμε περικοπή σε ένα παράθυρο μεγέθους $32 \times 32$ και τέλος, προσθέτουμε ταιχαία φωτεινότητα και αντίθεση στο σύνολο εκπαίδευσης (παρόμοια με το έργο \cite{hinton2018matrix}). Επειδή το σύνολο αποτελείται από στερεοπτικές εικόνες, τις στοιβάζουμε δημιουργώντας μια εικόνα με δύο κανάλια. Μέγεθος εισόδου μετά από προεπεξεργασία: [$2 \times 32 \times 32$].
\end{description} 
%  και τέλος, προσθέτουμε ταιχαία φωτεινότητα και αντίθεση στο σύνολο εκπαίδευσης (παρόμοια με το έργο \cite{hinton2018matrix})
Σημειώνουμε ότι κατά τη δημιουργία του συνόλου ελέγχου, το παράθυρο περικοπής είναι κεντραρισμένο στην εικόνα ενώ κατά τη δημιουργία του συνόλου εκπαίδευσης το παράθυρο αυτό είναι τυχαίο για το κάθε δείγμα.

\subsection{Εύρεση Βέλτιστων Υπερπαραμέτρων}

Στα πρώτα πειράματα της μεθόδου 1 διερευνούμε τις υπερπαραμέτρους των αλγορίθμων που εμφανίζουν τα καλύτερα αποτελέσματα. Οι υπερπαράμετροι αυτοί αφορούν τον ρυθμό μάθησης (\en{Learning Rate - Lr}), το σύνολο δέσμης (\en{Batch Size - Bs}), τη χρήση αποκωδικοποιητή (\en{reconstruction}) ή όχι και τον αριθμό επαναλήψεων (\en{Routing Iterations - r}) - με εξαίρεση τον αλγόριθμο \en{Max Routing} ο οποίος δεν είναι επαναληπτικός. Ο αριθμός των εποχών είναι μόλις 30 καθώς τα πειράματα είναι πολλά και οι υπολογιστικοί πόροι περιορισμένοι. Παρόλα αυτά, ο αριθμός των εποχών είναι αρκετός για να βρεθεί μια καλή παραμετροποίηση για τον κάθε αλγόριθμο. Στην επόμενη υποενότητα, θα εκπαιδεύσουμε επιλεκτικά τα μοντέλα για περισσότερες επαναλήψεις. 

\subsubsection{Σύνολο Δεδομένων \en{MNIST}}
Τα πρώτα πειράματα που έγιναν στο σύνολο δεδομένων \en{MNIST} αφορούν τον κλασσικό δυναμικό αλγόριθμο δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}). Τα αποτελέσματα των πειραμάτων παρατίθενται στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg1}.
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.51 \\
            & 64 & 3 & 0.001 & no & 0.37\footnote{\gr{Σκορ πολύ κοντά στο 0.35, όπως παρουσιάζεται στο \cite{sabour2017dynamic} για τις ίδιες παραμέτρους, κάτι που πιστοποιεί την ακεραιότητα της υλοποίησής μας.}} \\
            & 32 & 3 & 0.001 & yes & 0.37 \\
            & 64 & 3 & 0.001 & yes & 0.41 \\
            \midrule
            \multirow{6}{*}{Routing Iterations} & 64 & 1 & 0.001 & no & 0.44 \\
            & 64 & 1 & 0.001 & yes & 0.45 \\
            & 64 & 2 & 0.001 & no & 0.48 \\
            & 64 & 2 & 0.001 & yes & 0.35 \\
            & 64 & 3 & 0.001 & no & 0.37 \\
            & 64 & 3 & 0.001 & yes & 0.41 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & \textbf{0.33} \\
            & 64 & 3 & 0.001 & yes & 0.37 \\
            & 64 & 3 & 0.002 & yes & 0.51 \\
            & 64 & 3 & 0.01 & yes & - \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg1}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}) για 30 εποχές.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Αναφορικά με το μέγεθος της δέσμης, παρατηρείται σημαντική βελτίωση στην περίπτωση που δε χρησιμοποιείται αποκωδικοποιητής. Στην περίπτωση που χρησιμοποιείται δίκτυο αποκωδικοποιητή, έχει ένα μικρό προβάδισμα η ρύθμιση με το μικρότερο μέγεθος δέσμης. Αυτό πιθανότατα οφείλεται στον μικρότερο αριθμό των βημάτων οπισθοδιάδοσης σφάλματος που συμβαίνει κατά τον διπλασιασμό του μεγέθους δέσμης σε συνδειασμό τόσο με την σημαντική αύξηση των παραμέτρων λόγω της προσθήκης αποκωδικοποιητή όσο και με την κλιμάκωση του σφάλματως ανακατασκευής. Συνεπώς, μπορούμε με ασφάλεια να υποθέσουμε ότι εν γένη, ένα σύνολο δέσμης μεγέθους 64 οδηγεί σε καλύτερη εκπαίδευση από ένα σύνολο δέσμης 32 (αρκεί να συνοδεύεται από μεγάλο αριθμό εποχών).
    \item Αναφορικά με τον αριθμό των επαναλήψεων, κατά μέσο όρο παρατηρείται βελτίωση με την αύξηση των επαναλήψεων του αλγορίθμου. Όταν ο αριθμός επαναλήψεων είναι ίσος με τη μονάδα, ουσιαστικά δεν έχουμε κάποιον δυναμικό αλγόριθμο επανάληψης αφού η κάθε κάψουλα \en{DigitCap} προκύπτει από τον μέσο όρο των ψήφων της. Στηριζόμενοι και στα πειράματά του έργου \cite{sabour2017dynamic}, οι τρεις επαναλήψεις είναι η ιδανική παράμετρος για τη μέγιστη επίδοση. Η αύξηση του σφάλματος που παρατηρείται στην περίπτωση της χρήσης ανακατασκευής ίσως να οφείλεται στον μικρό αριθμό εποχών που έχει σαν αποτέλεσμα να μην προλαβαίνει να εκπαιδευτεί πλήρως το δίκτυο αποκωδικοποιητή\footnote{Άλλωστε, στα γραφήματα του σφάλματος επικύρωσης που παράγουμε κατά την εκπαίδευση, είναι προφανές ότι το σφάλμα μειώνεται ακόμα στις 30 εποχές.}.
    \item Στα πειράματα που αφορούν τον ρυθμό μάθησης φαίνεται ότι ένας μικρότερος ρυθμός μάθησης οδηγεί σε καλύτερη εκπαίδευση. Βέβαια, κατά τη διάρκεια της εκπαίδευσης πολλές φορές ο ρυθμός μάθησης μειώθηκε κατά μια ή και δύο κλίμακες μεγέθους από τον \en{learning rate scheduler}. Συνεπώς, σε περισσότερες επαναλήψεις ένας ρυθμός μάθησης με τιμή $0.001$ δε θα αποτελεί πρόβλημα. Το ασφαλές συμπέρασμα λοιπόν είναι ότι ο ρυθμός μάθησης δεν ωφελεί να είναι μεγαλύτερος του $0.001$.
\end{enumerate}

%%% image 1
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 1/train_curve.png}
    \caption{Στο σχήμα παρατηρούμε τις γραφικές παραστάσεις του σφάλματος εκπαίδευσης και ελέγχου (\en{validation} ή \en{test}) κατά τη διάρκεια των 30 εποχών για το μοντέλο με την καλύτερη επίδοση στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg1}.}
    \label{fig:exp_method_1_mnist_alg1}
  \end{figure}
Στην εικόνα \ref{fig:exp_method_1_mnist_alg1} παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.0005, r = 3, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στην 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι ο \en{Argmax Scaled Routing}. Ο αλγόριθμος αυτός κάνει την υπόθεση ότι μια ψήφος είναι αρκετή για τη διαμόρφωση μιας κάψουλας γονέα. Συνεπώς, για τη διαμόρφωση ολόκληρου του επιπέδου \en{DigitCaps} αρκεί να επιλεγούν (με κριτήριο τα βάρη δρομολόγησης) τόσες ψήφοι εκπρόσωποι όσος είναι και ο αριθμός των κλάσεων. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg2} παρατίθενται τα αποτελέσματα των πειραμάτων. Προφανώς, η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.54 \\
            & 64 & 3 & 0.001 & no & 0.53 \\
            & 32 & 3 & 0.001 & yes & 0.52 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 0.53 \\
            & 64 & 2 & 0.001 & yes & \textbf{0.39} \\
            & 64 & 3 & 0.001 & no & 0.53 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 0.45 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            & 64 & 3 & 0.002 & yes & 0.57 \\
            & 64 & 3 & 0.01 & yes & 0.59 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg2}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Scaled Routing} (αλγόριθμος \ref{alg:dynam_argmax_scaled_routing}) για 30 εποχές.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Σχετικά με το πρώτο πείραμα, αν και οι διαφορές δεν είναι μεγάλες, φαίνεται να βοηθάει η αύξηση του μεγέθους δέσμης και η προσθήκη αποκωδικοποιητή κατά την εκπαίδευση.
    \item Σε αντίθεση με τον κλασσικό αλγόριθμο δυναμικής δρομολόγησης, ο βέλτιστος αριθμός επαναλήψεων είναι 2. Όπως αποδεικνύεται στα ειδικά πειράματα (ενότητα \ref{sec:method1_special_experiments}), όσο αυξάνονται οι επαναλήψεις τόσο τα βάρη δρομολόγησης λαμβάνουν ακραίες τιμές (είτε 0 αν δρομολογούν ψήφους σε κάψουλες γονείς που δεν ανήκουν στη σωστή κλάση είτε 1 για κάψουλες παιδιά που ανήκουν στη σωστή κλάση στόχο). Συνεπώς, στις πρώτες επαναλήψεις, μπορεί τα βάρη δρομολόγησης να μην έχουν κορεστεί σε ακραίες τιμές αλλά από νωρίς τα βάρη που αντιστοιχούν στη σωστή κλάση έχουν τις μεγαλύτερες τιμές (αποδεικνύεται στην ενότητα \ref{sec:method1_special_experiments}). Το γεγονός όμως ότι η αύξηση των επαναλήψεων προκαλεί μείωση της επίδοσης στον συγκεκριμένο αλγόριθμο μας προδιαθέτει ότι υπάρχουν πολλές περιπτώσεις σύγχυσης όπου ενώ έτεινε προς τη σωστή κλάση, σε αργότερη επανάληψη άλλαξαν ριζικά τα βάρη ευνοώντας κάποια λανθασμένη κλάση. Η συγκεκριμένη περίπτωση περιγράφεται έμπρακτα στο δεύτερο σχήμα του έργου \cite{hinton2018matrix}. Επίσης, είναι πολύ πιθανό να οφείλεται στο ότι υπάρχουν κάψουλες που έχουν παράξει ψήφους με μικρή συμφωνία για την κάψουλα πρόβλεψης. Κατά τον κορεσμό τους, αυτές θα αποκτήσουν βάρη δρομολόγησης ίσα με τη μονάδα προς άλλες κάψουλες.
    \item Σε αντιστοιχεία με τις παραμέτρους της προηγούμενης μεθόδου, βλέπουμε ότι ένας χαμηλότερος ρυθμός μάθησης βοηθάει την διαδικασία εκπαίδευσης.
\end{enumerate}
% image 2
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 2/train_curve.png}
    \caption{Στο σχήμα παρατηρούμε τις γραφικές παραστάσεις του σφάλματος εκπαίδευσης και ελέγχου (\en{validation} ή \en{test}) κατά τη διάρκεια των 30 εποχών για το μοντέλο με την καλύτερη επίδοση στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg2}.}
    \label{fig:exp_method_1_mnist_alg2}
  \end{figure}
Στην εικόνα \ref{fig:exp_method_1_mnist_alg2} παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.001, r = 2, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στην 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι ο \en{Argmax Routing}. Και αυτός ο αλγόριθμος κάνει την υπόθεση ότι μια ψήφος είναι αρκετή για τη διαμόρφωση μιας κάψουλας γονέα. Συνεπώς, και πάλι, για τη διαμόρφωση ολόκληρου του επιπέδου \en{DigitCaps} αρκεί να επιλεγούν (με κριτήριο τα βάρη δρομολόγησης) τόσες ψήφοι εκπρόσωποι όσος είναι και ο αριθμός των κλάσεων. Υπενθιμίζεται ότι η διαφορά με τον \en{Argmax Scaled Routing} είναι ότι δεν πραγματοποιείται κλιμάκωση των εκπροσώπων με βάση το αντίστοιχο βάρος δρομολόγησης. Αυτό πάει ένα βήμα παραπέρα την υπόθεσή μας αφού πλέον, το μήκος των \en{DigitCaps} που καθορίζει την κλάση πρόβλεψης δεν εξαρτάται από τον δυναμικό αλγόριθμο αλλά από τις επιλεγμένες ψήφους. Με απλά λόγια, ερχόμαστε ακόμα πιο κοντά στον ισχυρισμό ότι τα βάρη δρομολόγησης διαμορφώνονται αποκλειστικά τις ψήφους με μεγάλο μέτρο (χωρίς να δίνεται ιδιαίτερη έμφαση στην πολυδιάστατη σύμπτωση).\par

Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg3} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, όπως και στον προηγούμενο αλγόριθμο, η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.87 \\
            & 64 & 3 & 0.001 & no & \textbf{0.68} \\
            & 32 & 3 & 0.001 & yes & 0.90 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 0.88 \\
            & 64 & 2 & 0.001 & yes & 0.81 \\
            & 64 & 3 & 0.001 & no & 0.68 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 0.99 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            & 64 & 3 & 0.002 & yes & 0.75 \\
            & 64 & 3 & 0.01 & yes & 0.80 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg3}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Routing} (αλγόριθμος \ref{alg:dynam_argmax_routing}) για 30 εποχές.}
\end{table}

Από τα πειράματα του πίνακα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Παρατηρούμε ότι η αύξηση του μεγέθους δέσμης συμβάλει στην εκπαίδευση.
    \item Η αύξηση του ρυθμού επαναλήψεων βοηθάει στον συγκεκριμένο αλγόριθμο την επίδοση. Γεγονός που ενισχύει την τελευταία υπόθεση της αντίστοιχης παρατήρησης του προηγούμενου αλγορίθμου. Δηλαδή, το γεγονός ότι ο αλγόριθμος της μη κλιμάκωσης των ψήφων εκπροσώπων παρουσιάζει βελτίωση στις 3 επαναλήψεις συνεπάγεται ότι για κάθε κάψουλα γονέα επιλέγονται οι σωστές ψήφοι. Παρόλα αυτά, όταν τα βάρη δρομολόγησης φτάνουν στον κορεσμό, ίσως υπάρχουν και άλλες (λίγες) κάψουλες των οποίων οι ψήφοι δεν συμφωνούν με τις άλλες ψήφους της σωστής κλάσης πρόβλεψης (π.χ. επειδή αναπαριστούν μέρη αντικειμένων που δεν εντοπίζονται στην συγκεκριμένη εικόνα εισόδου) αλλά τυχαίνει να συμφωνούν με λίγες ψήφους μιας άλλης κλάσης. Υπό αυτές τις συνθήκες, το αντίστοιχο βάρος δρομολόγησς της κάψουλας θα κορεστεί στη μονάδα πολύ γρήγορα\footnote{Στην ενότητα \ref{sec:method1_special_experiments} παρουσιάζεται μια τέτοια περίπτωση.}. 
    \item Σε αυτόν τον αλγόριθμο απαιτείται λίγο μεγαλύτερος ρυθμός μάθησης (ή περισσότερες εποχές).
\end{enumerate}
%% imaeg 3
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 3/train_curve.png}
    \caption{Στο σχήμα παρατηρούμε τις γραφικές παραστάσεις του σφάλματος εκπαίδευσης και ελέγχου (\en{validation} ή \en{test}) κατά τη διάρκεια των 30 εποχών για το μοντέλο που ακολουθεί τον αλγόριθμο \en{Argmax Routing} που έχει τις υπερπαραμέτρους με την καλύτερη επίδοση, όπως φαίνεται στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg3}.}
    \label{fig:exp_method_1_mnist_alg3}
  \end{figure}
Στην εικόνα \ref{fig:exp_method_1_mnist_alg3} παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.001, r = 3, Reconstruction = no$). Παρατηρούμε ότι ακόμα και στη 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται. Πιθανότητα, το γεγονός αυτό είναι η εήγηση της αύξησης του σφάλματος με τη χρήση του αποκωδικοποιητή.\par

Σε κάθε διάταξη, η μέθοδος αυτή παρουσιάζει χειρότερα αποτελέσματα. Συνεπώς, απορρίπτονται οι υποθέσεις σχετικά με τη συνεισφορά των βαρών δρομολόγησης. Τέλος, το πόρισμα αυτό είναι ήδη αναμφίβολο (και λόγω παλαιότερων πειραμάτων σε 100 εποχές) και άρα, δεν έχει νόημα η δοκιμή του αλγορίθμου αυτού σε όλα τα σύνολα δεδομένων.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο τελευταίος αλγόριθμος που εξετάζουμε σε αυτή τη μέθοδο είναι ο \en{Max Routing}. Υπενθυμίζεται ότι ο αλγόριθμος αυτός απορρίπτει τελείως τον δυναμικό αλγόριθμο δρομολόγησης και μαζί του, την υπόθεση των νευρωνικών δικτύων από κάψουλες περί φιλτραρίσματος με πολυδιάστατη σύμπτωση. Υποστηρίζει ότι ο στον δυναμικό αλγόριθμο δρομολόγησης, η ψήφος (για κάθε \en{DigitCap}) με το μεγαλύτερο μέτρο διαδραματίζει τον μεγαλύτερο ρόλο στη διαμόρφωση του μέσου διανύσματος $s_j$ και με αυτόν τον τρόπο, απλά προσελκύει όσες ψήφους τυχαίνει να συμφωνούν μαζί της. Συνεπώς, υποστηρίζουμε με αυτή τη μέθοδο ότι δεν πρόκειται για ένα φιλτράρισμα συμφωνίας αλλά για μια αδικαιολόγητα περίπλοκη επιλογή μεγίστου διανύσματος.\par

Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg4} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, η δοκιμασία για τον αριθμό των επαναλήψεων δεν έχει νόημα καθώς ο αλγόριθμος δεν είναι επαναληπτικός.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c}
            \toprule
            Experiment & Batch Size & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 0.001 & no & 0.92 \\
            & 64 & 0.001 & no & 0.81 \\
            & 32 & 0.001 & yes & \textbf{0.72} \\
            & 64 & 0.001 & yes & 0.89 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 0.0005 & yes & 1.00 \\
            & 64 & 0.001 & yes & 0.89 \\
            & 64 & 0.002 & yes & 0.90 \\
            & 64 & 0.01 & yes & 0.95 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg4}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Max Routing} (αλγόριθμος \ref{alg:dynam_max_routing}) για 30 εποχές.}
\end{table}

Από τα πειράματα του πίνακα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Από τα αποτελέσματα, δεν μπορούμε να κρίνουμε ποια είναι η καλύτερη επιλογή μεγέθους δέσμης. Εκτιμούμε ότι σε αυτό τον αριθμό εποχών για τον συγκεκριμένο αλγόριθμο, μικρότερος αριθμος δέσμης είναι προτιμιταίος.
    \item Σε αυτόν τον αλγόριθμο η τιμή $0.001$ για τον ρυθμό μάθησης φαίνεται να είναι ιδανική.
\end{enumerate}
%% image 4
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 3/train_curve.png}
    \caption{Στο σχήμα απεικονίζονται οι γραφικές παραστάσεις του σφάλματος εκπαίδευσης και ελέγχου (\en{validation} ή \en{test}) κατά τη διάρκεια των 30 εποχών για το καλύτερο μοντέλο που ακολουθεί τον αλγόριθμο \en{Max Routing}, όπως προκύπτει από τον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg4}.}
    \label{fig:exp_method_1_mnist_alg4}
  \end{figure}
Για άλλη μια φορά, στην εικόνα \ref{fig:exp_method_1_mnist_alg4} παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 32, lr = 0.001, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στη 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται ακόμα.\par

Σε κάθε διάταξη, η μέθοδος αυτή παρουσιάζει χειρότερα αποτελέσματα σε σχέση με τις πρώτες δύο. Συνεπώς, αφενός δε θα γίνει εκτενή μελέτη αυτού σε όλα τα σύνολα δεδομένων και αφετέρου φαίνεται ότι ο αλγόριθμος δρομολόγησης πραγματικά συμβάλει στην εκπαίδευση. Περισσότερα σχετικά πειράματα βρίσκονται στην ενότητα \ref{sec:method1_special_experiments}.

\subsubsection{Σύνολο Δεδομένων \en{CIFAR10}}
Αντίστοιχα με τα πειράματα για το σύνολο \en{MNIST}, παρατίθενται παρακάτω τα πειράματα για το πιο σύνθετο σύνολο δεδομένων \en{CIFAR10}. Οι λίγες εποχές που χρησιμοποιούνται για το συγκεκριμένο σύνολο σε συνδειασμό με την τάση των νευρωνικών δικτύων από κάψουλες να εξηγούν το οτιδήποτε βρίσκεται στην εικόνα (ακόμα και το παρασκήνιο) οδηγούν σε χαμηλές επιδόσεις. Σημειώνουμε ότι κατά αντιστοιχία με το έργο \cite{sabour2017dynamic}, χρησιμοποιήθηκε μια επιπλέον κλάση \en{none-of-the-above} για να δρομολογούν εκεί την ψήφο τους κάψουλες που αναπαριστούν αντικείμενα που δεν εμφανίζονται στην εικόνα. Επίσης, ο αριθμός των τύπων από κάψουλες αυξήθηκε από 32 σε 64 (όπως και στο έργο \cite{sabour2017dynamic}).\par

Ο αριθμός των πειραμάτων είναι μικρότερος από αυτόν για το σύνολο \en{MNIST}. Αυτό διότι, αν και το σύνολο \en{CIFAR10} έχει μεγάλη ετερογένεια με το προηγούμενο, ορισμένα πειράματα ποτέ δεν οδηγούν σε καλύτερα αποτελέσματα (ανεξαρτήτως συνόλου δεδομένων). Τα πειράματα αυτά έχουν αφαιρεθεί (όπως αυτό της δοκιμής του συνόλου δέσμης).

Τα πρώτα πειράματα για το σύνολο δεδομένων \en{CIFAR10} αφορούν τον κλασσικό δυναμικό αλγόριθμο δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}). Τα αποτελέσματα των πειραμάτων παρατίθενται στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg1}. Είναι λιγότερα από πρίν
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{2}{*}{Batch Size} & 32 & 3 & 0.0005 & yes & \textbf{20.60} \\
            & 64 & 3 & 0.0005 & yes & 23.96 \\
            \midrule
            \multirow{7}{*}{Routing Iterations} & 64 & 1 & 0.001 & no & 24.64 \\
            & 64 & 1 & 0.001 & yes & 23.19 \\
            & 64 & 2 & 0.001 & no & 29.25* \\
            & 64 & 2 & 0.001 & yes & 26.23* \\
            & 64 & 3 & 0.001 & no & 31.24* \\
            & 64 & 3 & 0.001 & yes & 29.15* \\
            \midrule
            \multirow{2}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 23.96 \\
            & 64 & 3 & 0.001 & yes & 29.15* \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg1}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}) για 30 εποχές. Οι αριθμοί με αστερίσκο αναφέρονται σε περιπτώσεις αστάθειας του αλγορίθμου εκπαίδευσης.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{itemize}
    \item Αναφορικά με το μέγεθος της δέσμης, παρατηρήθηκε ότι χαμηλότερο μέγεθος (32) εξαλείφει την όποια αστάθεια στον αλγόριθμο εκπαίδευσης. Για αυτό και η καλύτερη επίδοση επιτυγχάνεται με αυτό.
    \item Αναφορικά με τον αριθμό των επαναλήψεων, παρατηρούμε ότι η αύξησή τους επιδεινώνει την αστάθεια. Το γεγονός ότι οι καλύτερες παραμετροποιήσεις είναι για 3 επαναλήψεις σε συνδειασμό με τα πειράματα στο προηγούμενο σύνολο δεδομένων υποδεικνύουν ότι αυτή είναι η βέλτιστη υπερπαράμετρος.
    \item Για άλλη μια φορά, χαμηλότερη τιμή ρυθμού μάθησης βελτιώνει την επίδοση και μειώνει την πιθανότητα αστάθειας.
\end{itemize}


% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος αλγόριθμος είναι ο \en{Argmax Scaled Routing}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg2} παρατίθενται τα αποτελέσματα των πειραμάτων. Να επαναλάβουμε ότι και για αυτό το σύνολο δεδομένων η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 28.10 \\
            & 64 & 2 & 0.001 & yes & \textbf{27.46} \\
            & 64 & 3 & 0.001 & no & 29.65 \\
            & 64 & 3 & 0.001 & yes & 31.02 \\
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 29.06 \\
            & 64 & 3 & 0.001 & yes & 31.02 \\
            & 64 & 3 & 0.002 & yes & 31.60 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg2}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Scaled Routing} (αλγόριθμος \ref{alg:dynam_argmax_scaled_routing}) για 30 εποχές.}
\end{table}

Τα ανωτέρω πειράματα, επιβεβαιώνουν τα συμπεράσματα των πειραμάτων στο σύνολο δεδομένων \en{MNIST}. Βλέπουμε ότι οι 2 επαναλήψεις είναι η βέλτιστη ρύθμιση για τον συγκεκριμένο αλγόριθμο ενώ επίσης, συνήθως βοηθάει η ελάττωση του ρυθμού μάθησης στην τιμή $0.0005$. Σημειώνουμε ότι ο αλγόριθμος αυτός (όπως και όλοι οι αλγόριθμοι εκτός του προηγούμενου), ουδέποτε εμφάνισε την οποιαδήποτε αστάθεια κατά την εκπαίδευση.


% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι όπως και πριν ο \en{Argmax Routing}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg3} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων (με τη δοκιμή για 1 επανάληψη να μην έχει ουσία).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 38.96 \\
            & 64 & 2 & 0.001 & yes & 37.96 \\
            & 64 & 3 & 0.001 & no & 38.06 \\
            & 64 & 3 & 0.001 & yes & 38.49 \\
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 38.97 \\
            & 64 & 3 & 0.001 & yes & 38.49 \\
            & 64 & 3 & 0.002 & yes & \textbf{37.83} \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg3}Πίνακας που περιέχει τα πειράματα που έγιναν στο σύνολο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Routing} (αλγόριθμος \ref{alg:dynam_argmax_routing}) για 30 εποχές.}
\end{table}

Για άλλη μια φορά επιβεβαιόνωνται οι παρατηρήσεις που έγιναν στο σύνολο δεδομένων \en{MNIST} για τον αντίστοιχο αλγόριθμο. Φαίνεται ότι οι 3 επαναλήψεις οδηγούν σε καλύτερη επίδοση αφού σε αυτή τη ρύθμιση επιτυγχάνεται η καλύτερη επίδοση.

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Κλείνοντας τα πειράματα για την εύρεση των βέλτιστων υπερπαραμέτρων, αναφέρουμε τα πειράματα που έγιναν στον αλγόριθμο \en{Max Routing} με το σύνολο δεδομένων \en{CIFAR10}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg4} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, όπως και στον προηγούμενο αλγόριθμο, η δοκιμασία για τον αριθμό των επαναλήψεων δεν έχει νόημα καθώς ο αλγόριθμος δεν είναι επαναληπτικός.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c}
            \toprule
            Experiment & Batch Size & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 0.0005 & yes & \textbf{37.19} \\
            & 64 & 0.002 & yes & 37.60 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg4}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Max Routing} (αλγόριθμος \ref{alg:dynam_max_routing}) για 30 εποχές.}
\end{table}

Από τα δύο περιάματα αποδεικνύεται ότι ο μικρότερος ρυθμός μάθησης βοηθάει γενικότερα στην εκπαίδευση των αλγορίθμων μας.


\subsection{Επιλεκτική Εμβάθυνση Πειραμάτων και Σύγκριση}
Για τους δύο αλγορίθμους της μηθόδου 1 με την καλύτερη επίδοση (αλγόριθμοι \ref{alg:dynam_routing} και \ref{alg:dynam_argmax_scaled_routing}) πραγματοποιήθηκαν εκτενέστερα πειράματα με περισσότερες εποχές, για όλα τα σύνολα δεδομένων και σε μεγαλύτερο μέγεθος δέσμης (εφόσον ήταν εφικτό). Τα νεα πειράματα αυτά, μαζί με τα παλαιότερα πειράμτα από τις άλλες δύο μεθόδους (για 100 εποχές), τα παρουσιάζουμε συγκεντρωτικά στον πίνακα \ref{tab:method_1_all_datasets}.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c c}
            \toprule
            Dataset & Algorithm & Bs & r & Lr & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{8}{*}{MNIST} &Classic& 64 & 3 & 0.0005 & yes & 0.41 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 0.43 \\
            &Classic& 128 & 3 & 0.0005 & yes & \textbf{0.31}* \\
            &Argmax Scaled& 128 & 2 & 0.001 & yes & 0.37* \\
            &Classic& 32 & 3 & 0.0005 & yes & 0.35 \\
            &Argmax Scaled& 32 & 2 & 0.001 & yes & 0.42 \\
            &Argmax& 64 & 3 & 0.001 & yes & 0.60 \\
            &Max& 32 & 0 & 0.001 & yes & 0.75 \\
            \midrule
            \multirow{2}{*}{FashionMNIST} &Classic& 64 & 3 & 0.0005 & yes & 6.94 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 7.00 \\
            \midrule
            \multirow{2}{*}{CIFAR10} &Classic& 64 & 3 & 0.0005 & yes & 21.64 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 22.16 \\
            \midrule
            \multirow{2}{*}{SmallNORB} &Classic& 64 & 3 & 0.0005 & yes & 17.01 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 16.49 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_all_datasets}Πίνακας που περιέχει τα πειράματα που έγιναν σε όλα τα σύνολα δεδομένων στους δυο αλγορίθμους με την καλύτερη επίδοση. Επίσης, περιέχονται για λόγoυς σύγκρισης ορισμένα πειράματα των δύο αλγορίθμων με τη λιγότερο καλή επίδοση (όπως μετρήθηκε στην προηγούμενη ενότητα). Σημειώνουμε ότι με τον όρο \en{Classic} αναφερόμαστε στον αλόριθμο δυναμικής δρομολόγησης με συμφωνία. Επίσης, τα αποτελέσματα με αστερίσκο (*) προέκυψαν μετά από εκπαίδευση σε 150 εποχές.}
\end{table}
%% image 5 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 5/train_curve.png}
    \caption{Στο σχήμα παρατηρούμε τις γραφικές παραστάσεις του σφάλματος εκπαίδευσης και ελέγχου (\en{validation}) κατά τη διάρκεια των 150 εποχών για το μοντέλο που ακολουθεί τον αλγόριθμο \en{Dynamic Routing Between Capsules}. Είναι βέβαιο ότι παρατηρείται (\en{overfitting}) αλλά όχι σε βαθμό που να επηρρεάζει την απόδοση στο σύνολο ελέγχου.}
    \label{fig:exp_method_1_best_alg}
  \end{figure}

Το πιο βέβαιο συμπέρασμα που μπορούμε να εξάγουμε από τα ανωτέρω αποτελέσματα είναι ότι ο αλγόριθμός μας \en{Argmax Scaled Routing} δεν παρουσιάζει έντονες διαφορές στην επίδοση σε σχέση με τον κλασσικό αλγόριθμο δρομολόγησης με συμφωνία. Αν εξαιρέσουμε τα σύνολα δεδομένων \en{CIFAR10} και \en{SmallNORB} όπου ο αριθμός των εποχών δεν επαρκεί για την σύγκλισή τους, στα υπόλοιπα σύνολα δεδομένων οι διαφορές στις επιδόσεις είναι μικρές.\par

Η παρόμοια ακρίβεια μεταξύ των δύο αλγορίθμων σημαίνει ότι η εξαγωγή ενός διανύσματος για κάθε κάψουλα \en{DigitCap} ως το σταθμισμένο άθροισμα των ψήφων δεν οφελεί πρακτικά την επίδοση του δικτύου\footnote{Παρόλα αυτά, το σταθμισμένο άθροισμα μπορεί να οφελεί την σύληψη των παραμέτρων στιγμιοτύπου της. Αυτό το διερευνούμε στην ενότητα \ref{sec:method1_special_experiments}}. Συνεπώς, τον πρωτεύοντα ρόλο τον έχουν τα βάρη δρομολόγησης που κλιμακώνουν τις ψήφους. Αυτά είναι ένα επαρκές κριτήριο για την επιλογή της ψήφου που θα δρομολογηθεί, κλιμακωμένη στο επόμενο επίπεδο. Το γεγονός αυτό μπορούμε να το εκμεταλευτούμε μειώνωντας τις επαναλήψεις και γλιτώνωντας υπολογιστικό κόστος. Αντί για την μια περιττή επανάληψη θα μπορούσαμε να εισάγουμε ένα παραπάνω συνελικτικό επίπεδο που, όπως έχει αποδηχθεί στο κεφάλαιο \ref{chap:related_work}, βελτιώνει την ακρίβεια.\par 

Ένα ακόμα ερώτημα που προκύπτει φυσικά είναι αν το δίκτυο του αλγορίθμου \en{Argmax Scaled Routing} μαθαίνει τα βάρη του ώστε να ανταποκρίνεται με βέλτιστο τρόπο στον νέο μας αλγόριθμο ή αν εξ'αρχής ο κλασσικός αλγόριθμος με συμφωνία έχει την ιδιότητα που περιγράφουμε (δηλαή ότι το μέγιστο βάρος δρομολόγησς και η αντίστοιχη ψήφος - ψήφος \textquote{εκπρόσωπος} - αρκούν για την δρομολόγηση). Το ερώτημα αυτό το απαντάμε στο \ref{sec:method1_special_experiments} ύστερα από την ιενέργεια κατάλληλων ειδεικών πειραμάτων.\par

Ένα τελευταίο σημείο της σύγκρισης των δύο αλγορίθμων είναι ο χρόνος επίδοσης. Αναλυτικότερα, ο αλγόριθμος δυναμικής δρομολόγησης με συμφωνία (στον πίνακα αναγράφεται ως \en{Classic}), για μέγεθος δέσμης ίσο με 64 είναι κατά 3 δευτερόλεπτα πιο αργός από τον αλγόριθμο \en{Argmax Scaled Routing} (χρόνοι 30 και 30 δευτερόλεπτα αντίστοιχα). Σημειώνουμε ότι όλοι οι αλγόριθμοι έχουν ίσο αριθμό παραμέτρων, δηλαδή είναι $8,227k$ (οκτώ εκατομμύρια 227 χιλιάες παράμετροι). Εξ' αυτών, οι $6,816k$ αφορούν τον κωικοποιητή ενώ οι $1,411k$ τον αποκωδικοποιητή.\par  

Σε σύγκριση με τον απόλυτο αλγόριθμο \en{Max Rooting} της πρώτης μεθόδου, φαίνεται πως το φιλτράρισμα με πολυδιάστατη συμφωνία πραγματικά βοηθάει τις επιδόσεις του μοντέλου χωρίς να εισάγει επιπλέον παραμέτρους. Προφανώς λοιπόν, η υπόθεση ότι ο κλασσικός αλγόριθμος δρομολόγησης μπορεί να μην προσφαίρει κάτι παραπάνω από την απλή δρομολόγηση της μέγιστης σε μήκος ψήφου (για κάθε κάψουλα γονέα) δεν είναι ορθή. Τέλος, σε σύγκριση με τον αλγόριθμο \en{Argmax Routing}, παρατηρούμε ότι υπάρχουν οφέλη από την χρήση των βαρών δρομολόγησης όχι μόνο ως κριτίριο για την επιλογή των ψήφων εκπροσώπων αλλά ως μέγεθος για την επιλογή της κλάσης εξόδου (αφού διαμορφώνει άμεσα το μήκος των \en{DigitCaps}).

\section{Ειδικά Πειράματα Μεθόδου 1}
\label{sec:method1_special_experiments}
Στην ενότητα αυυτή πραγματοποιούνται ειδικά πειράματα τα οποία στόχο έχουν να διαφωτήσουν την εσωτερική λειτουργεία των νευρωνικών δικτύων με κάψουλες. Επίσης, στόχο έχουν να πιστοποιήσουν ότι οι προτεινόμενοι αλγόριθμοι πληρούν τις χαρακτηριστικές ιδιότητες της τεχνολογίας αλλά και να αποδείξουν ή να καταρρίψουν ορισμένες υποθέσεις αυτών.

\subsection{Τι Μαθαίνει να Αναπαριστά η Κάθε Διάσταση του Διανύσματος \en{DigitCap}}
% Πειράματα ανακατασκευής
Ένα συνηθισμένο πείραμα της τεχνολογίας νευρωνικών δικτύων με κάψουλες είναι αυτό στο οποίο χρησιμοποιείται ο ανακατασκευαστής για να διαπιστωθεί τι αναπαριστά η κάθε διάσταση του διανύσματος \en{DigitCap}. Υπενθυμίζουμε ότι ένα από τα χαρακτηριστικά των νευρωνικών δικτύων είναι η δυνατότητα αποδόμησης του απεικονιζόμενου αντικειμένου και η ενθυλάκωση των παραμέτρων στιγμιοτύπου του στην αντίστοιχη ενεργή κάψουλα του τελευταίου επιπέδου. Παραδείγματα παραμέτρων στιγμιοτύπου είναι η πόζα η φωτεινότητα αλλά και άλλα χαρακτηριστικά που αφορούν το συγκεκριμένο στιγμιότυπο. Στο πείραμα αυτό, χρησιμοποιώντας τον ανακατασκευαστή μπορούμε να έχουμε μια εικόνα του τι αναπαριστά το κάθε στοιχείο μιας κάψουλας τελευταίου επιπέδου.\par

Το συγκεκριμένο πείραμα ονομάζεται τυπικά πείραμα διαταραχής (\en{perturbation testing}). Πρακτικά, τροφοδοτούμε σε ένα εκπαιδευμένο - με την χρήση αποκωδικοποιητή - μοντέλο νευρωνικού δικτύου από κάψουλες και λαμβάνουμε το διάνυσμα της κάψουλας \en{DigitCap} με την μεγαλύτερη τιμή ενεργοποίησης (με το μεγαλύτερο μήκος). Στην συνέχεια, μεταβάλουμε τις τιμές του διανύσματος (την κάθε μια ξεχωριστά) και τροφοδοτούμε το περιαγμένο διάνυσμα στον αποκωδικοποιητή. Παρατηρώντας την επίδραση των μεταβολών της κάψουλας στην ανακατασκευασμένη εικόνα, είμαστε σε θέση να  κατανοήσουμε το χαρακτηριστικό που η συγκεκριμένη θέση του διανύσματος εισόδου κωδικοποιεί. Πρωτού παρουσιάσουμε τα αποτελέσματα του πειράματος, σημειώνουμε ότι οι μεταβολές (\en{perturbations}) αν ανήκουν στο διάστημα $[-0.25, 0.25]$ και έχουν βήμα 0.05. Με αυτόν τον τρόπο, προκύπτει ότι για ένα διάνυσμα (κάψουλα) που έχει διάσταση 16, έχουμε 16 πειράματα για 11 μεταβολές.\par

% Σχήμα για το 4 perturbations new (image 6)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/chapter experiments/method 1/image 6/perturbations_19.png}
    \caption{\en{Perturbation tests} στο σύνολο δεδομένων \en{MNIST} στην κάψουλα \en{DigitCap} που αναπαριστά την κλάση 4 σε δίκτυο που εκπαιδεύτηκε με την χρήση του δυναμικού αλγορίθμου δρομολόγησης.}
    \label{fig:exp_method_1_special_perturb_1}
  \end{figure}

  % Σχήμα original vs output (image 6)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/chapter experiments/method 1/image 6/original_vs_reconstruction.pdf}
    \caption{Αντιπαραβολή της εικόνας εισόδου (πρώτη γραμμή) με την ανακατασκευασμένη εικόνα (δεύτερη γραμμή). Βλέπουμε ότι το δίκτυο μπορεί να λειτουργήσει ικανοποιητικά και ως αυτοκωδικοποιητής (\en{autoencoder}). Η θολούρα των εικόνων ανακατασκευής βελτιώνεται με την αύξηση των εποχών.}
    \label{fig:exp_method_1_special_perturb_many_1}
  \end{figure}

Στο σχήμα \ref{fig:exp_method1_special_perturb_1} παρατίθενται τα αποτελέσματα του πειράματος για το σύνολο \en{MNIST} και για τον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία. Αν και οι λίγες εποχές σε συνδειασμό με τον μικρό παράγοντα βαρύτητας του σφάλματος ανακατασκευής δεν έχουν επιτρέψει τον πλήρη σχηματισμό του αποκωδικοποιητή, είναι εμφανές ότι οι κάψουλες του τελευταίου επιπέδου (και συγκεκριμένα η κάψουλα 4 στο σχήμα) καταφέρνουν να συλάβουν τις συγκεκριμένες παραμέτρους στιγμιοτύπου των απεικονιζόμενων αντικειμένων. Για παράδειγμα, για το ψηφίο 4, η δέκατητρίτη διάσταση κωδικοποιεί τον προσανατολισμό του ψηφίου. Η δέκατη τέταρτη την γωνία μεταξύ δύο ευθυγράμων τμημάτων που συνθέτουν το ψηφίο 4 κ.ο.κ.\par


Αξίζει να σημειώσουμε ότι για κάθε κάψουλα, η κάθε διάσταση του διανύσματός της κωδικοποιεί διαφορετικές ιδιότητες του αντικειμένου που αναπαριστά. Παρόλα αυτά, ορισμένες διαστάσεις (π.χ. η δέκατη τρίτη) φαίνεται να έχουν καθολική ερμηνεία.\par

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 7/perturbations_19.png}
    \caption{\en{Perturbation tests} στο σύνολο δεδομένων \en{MNIST} στην κάψουλα \en{DigitCap} που αναπαριστά την κλάση 4 σε δίκτυο που εκπαιδεύτηκε με την χρήση του αλγορίθμου \en{Argmax Scaled Routing}. Ενώ χρησιμοποιείται ο ίδιος αποκωδικοποιητής, το δίκτυο του κωδικοποιητή με τον συγκεκριμένο αλγόριθμο δρομολόγησης αδυνατεί να συλλάβει (\en{encapsulate}) τις παραμέτρους στιγμιοτύπου των απεικονίσεων εισόδου.}
    \label{fig:exp_method_1_special_perturb_2}
  \end{figure}
%Σχήμα για το 4 από τον φάκελο perturbations argmax scaled (image 7)
Με μεγάλη περιέργεια πραγματοποιήσαμε την συγκεκριμένη δοκιμασία για τους υπόλοιπους αλγορίθμους της μεθόδου 1. Αρχικά, για τον αλγόριθμο \en{Argmax Scaled Routing}, φαίνεται πως μετά από την εκπαίδευσή του στον ίδιο, περιορισμένο αριθμό εποχών με τον κλασσικό αλγόριθμο, αδυνατεί σε μεγάλο βαθμό να συλλάβει τις ιδιότητες που αφορούν την αναπαράσταση των στιγμιοτύπων των αντικειμένων εισόου (\en{equi\textendash variant parameters}). Για αυτό, στο σχήμα \ref{fig:exp_method_1_special_perturb_2} δεν διακρίνεται έντονα κάποια γραμμή του οποίου τα στοιχεία να μεταβάλλονται κατά προβλέψιμο τρόπο.\par

Από αυτό το πείραμα μπορούμε να συμπεράνουμε ότι στην πραγματικότητα, το σταθμισμένο άθροισμα των ψήφων μπορεί να μην επιρρεάζει σηματνικά την επίδοση αλλά διαδραματίζει καθοριστικό ρόλο στην ορθή λειτουργία των καψουλών. Φαίνεται ότι όλες μαζί οι συμφωνούντες ψήφοι συνθέτουν τις ιδιότητες του στιγμιοτύπου.\par

Η επίδοση στην παρούσα δοκιμασία γίνεται ακόμα πιο δύσκολη καθώς απομακρυνόμαστε από τον κλασικό αλγόριθμο δρομολόγησης με συμφωνία. Στο σχήμα \ref{fig:exp_method_1_special_perturb_3} παρατίθεται το αποτέλεσμα της δοκιμασίας για τον αλγόριθμο \en{Max Routing}.\par

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 8/perturbations_19.png}
    \caption{\en{Perturbation tests} στο σύνολο δεδομένων \en{MNIST} στην κάψουλα \en{DigitCap} που αναπαριστά την κλάση 4 σε δίκτυο που εκπαιδεύτηκε με την χρήση του αλγορίθμου \en{Max Routing}. Η ανακατασκευή δεν είναι ακριβής λόγω της φύσης του αλγορίθμου δρομολόγησης που δεν παράγει \textquote{ισομεταβλητές} (\en{equi-variant}) αναπαραστάσεις.}
    \label{fig:exp_method_1_special_perturb_3}
  \end{figure}
% Σχήμα από φάκελο perturbations μαχ ροθτινγ (image 8)

Τέλος οδηγηθήκαμε σε μια μοναική για την βιβλιογραφία δοκιμή εφαρμόζοντας το πείραμα στον κλασικό αλγόριθμο αλλά εκπαιδευμένο με το σύνολο δεδομένων \en{Fashion-MNIST}. Τα αποτελέσματα για τις περισσότερες διαστάσεις του διανύσματος της κάψουλας που αναπαριστά την κλάση υπόδημα δεν μπορούν να περιγραφούν με σαφή τρόπο. Παρόλα αυτά για ορισμένες διαστάσεις, οι μεταβολές είναι εμφανείς. Για παράδειγμα, στο σχήμα \ref{fig:exp_method_1_special_perturb_4} η διάσταση 6 κωδικοποιεί το ύψος του τακουνιού.
% Βάλε σχήμα από fashion mnist φάκελο (image 9)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter experiments/method 1/image 9/perturbations_11_sixth_dim.png}
    \caption{\en{Perturbation tests} στο σύνολο δεδομένων \en{MNIST} στην έκτη διάσταση της κάψουλας \en{DigitCap} που αναπαριστά την κλάση \textquote{υπόδημα} σε δίκτυο που εκπαιδεύτηκε με την χρήση του αλγορίθμου \en{Classic Routing}. Στην εικόνα είναι εμφανές ότι η διάσταση αυτή κωδικοποιεί το ύψος του τακουνιού.}
    \label{fig:exp_method_1_special_perturb_4}
  \end{figure}

\subsection{Κριτήριο Επιλογής Ψήφων Αλγορίθμου Δυναμικής Δρομολόγησης με Συμφωνία}
% Εκπαίδευση στον κλασικό αλγόριθμο, δοκιμή στον αλγόριθμο max, argmax, argmax scaled.
% Το δίκτυο προσαρμόζεται στον νέο αλγόριθμο ή μαθαίνει έτσι και στην κλασική περίπτωση?

Οι αλγόριθμοι που παρουσιάσαμε στην μέθοδο 1 έχουν όλοι τους τον ίδιο αριθμό βαρών. Το μόνο που μεταβάλεται είναι ο αλγόριθμος δρομολόγησης των ψήφων από το επίπεδο \en{PrimaryCaps} επίπεδο \en{DigitCaps}. Το γεγονός αυτό μας επιτρέπει την εφαρμογή των αλγορίθμων δρομολόγησης \ref{alg:dynam_argmax_scaled_routing}, \ref{alg:dynam_argmax_routing} και \ref{alg:dynam_max_routing} στο μοντέλο που έχει εκπαιδυτεί με την χρήση του αλγορίθμου \ref{alg:dynam_routing}. Κάθε μια από τις τρείς παραλλαγές του αλγορίθμου δρομολόγησης εξετάζει και ένα κριτήριο επιλογής εκπροσώπων. Θέλουμε να διαπιστώσουμε αν το κριτήριο αυτό είναι επαρκές για την δρομολόγηση των ψήφων ή όχι.\par

Πρακτικά, στο πείραμα αυτό χρησιμοποιούμε τα βάρη του μοντέλου που εκπαιδεύτηκε στο δυναμικό αλγόριθμο δρομολόγησης με συμφωνία και είχε την καλύτερη επίδοση ($0.31\%$ \en{Test Error}). Τα βάρη αυτά τα φορτώουμε στα μοντέλα των τριών υπολοίπων αλγορίθμων και έπειτα εξετάζουμε τις επιδόσεις τους. Τα αποτελέσματα των πειραμάτων παρουσιάζονται στον πίνακα \ref{tab:special_experiments_algorithms_on_classic_model}. \par

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c}
            \toprule
            Algorithm Used for Training & Algorithm Used for Testing & r & Loss (with reconstruction) & Test Error (\%)  \\ 
            \midrule
            \multirow{4}{*}{Dynamic Routing (Classic)} & Classic & 3 & 0.0007 & 0.31 \\
            & Argmax Scaled & 2 & 0.0064 & \textbf{0.30} \\
            & Argmax  & 3 & 0.0113 & 0.51 \\
            & Max & -1 & 2.6008 & 30.78 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:special_experiments_algorithms_on_classic_model}Πειράματα στο \en{MNIST} με την εκπαίδευση του αλγορίθμου στον κλασσικό, δυναμικό αλγόριθμο ρομολόγησης με συμφωνία και τον έλεγχο του μοντέλου αυτού με τη χρήση των τεσσάρων αλγορίθμων δρομολόγησης.}
\end{table}

Τα αποτελέσματα είναι τα αναμενόμενα για τους αλγορίθμους \en{Argmax} και \en{Max Routing}. Επίσης αναμενόμενες είναι όλες οι απώλειες (\en{Losses}) που προκύπτουν από το άθροισμα του σφάλματος περιθωρίου (\en{margin loss}) και του σφάλματος ανακατασκευής (\en{mean square error}). Άλλωστε, όπως διαπιστώσαμε στα πειράματα που προηγήθηκαν, ο κλασικός αλγόριθμος κάνει την καλύτερη ανακατασκευή αφού μόνο αυτός καταφέρνει να αποδομεί την εικόνα στις παραμέτρους στιγμιοτύπου (το επιτυγχάνει μέσα από την πράξη του σταθμισμένου αθροίσματος που δεν παρατηρείται στους άλλους αλγορίθμους).\par 

Το αξιοσημείωτο από τον πίνακα των αποτελεσμάτων είναι η επίδοση του αλγορίθμου \en{Argmax Scaled Routing} με βάρη που προέκυψαν από τον αλγόριθμο \en{Dynamic Routing} κατά την εκπαίδευσή του. Αν και η διαφορά είναι μικρή, το γεγονός αυτό συνεπάγεται ότι κατά την πρόβλεψη (\en{inference}) είναι προτιμότερο ένα κριτήριο επιλογής κάψουλας που θα δρομολογεί αυτή που αντιστοιχεί στο μέγιστο βάρος δρομολόγησης. Αυτό το γρήγορο κριτήριο οδηγεί σε καλύτερη επίδοση. Είναι λοιπόν προφανής ο ρόλος των βαρών δρομολόγησης στην επιλογή των κλάσεων. Αυτά εντοπίζουν την συμφωνία μεταξύ των ψήφων (που προκύπτουν από τα εκπαιδευμένα βάρη) και πριμοδοτούν την κλάση όπου υπάρχει μεγάλη συμφωνία με το να λαμβάνουν πολύ μεγάλη τιμή. Σε τελική ανάλυση, ο σημαντικός ρόλος των βαρών δρομολόγησης στην επιλογή της κλάσης προδίδεται από την χαμηλή επίδοση του αλγορίθμου \en{Argmax Routing} στον οποίο οι εκπρόσωποι δεν κλιμακώννται από το αντίστοιχο βάρος δρομολόγησης.\par



\subsection{Συμφωνία Ψήφων}
% Πειράματα με confusion matrix
Στην ενότητα αυτή θέλουμε να εξετάσουμε την υπόθεση των νευρωνικών δικτύων με κάψουλες περί φιλτραρίσματος πολυδιάστατης σύμπτωσης. Αναλυτικότερα, χρησιμοποιούμε το καλύτερο μοντέλο μας που εκπαιδεύτηκε στον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία και εξετάζουμε αν η κλάση πρόβλεψης είναι αυτή που εμφανίζει την μεγαλύτερη συμφωνία μεταξύ των ψήφων. Αν κάτι τέτοιο ισχύει, αποτελεί ακόμα μια επιβεβαίωση των θεμελιωδών υποθέσεων των νευρωνικών δικτύων με κάψουλες.\par

Αναλυτικότερα, για το πείραμά μας τροφοδοτούμε το μοντέλο με ένα παράδειγμα εισόδου της επιλογής μας και έπειτα συγκρίνουμε (με εσωτερικό γινόμενο) τις ψήφους που αντιστοιχούν στην κάθε κάψουλα γονέα μεταξύ τους. Έτσι, παράγεται ένας πίνακς προσοχής (\en{attention matrix}) για κάθε \en{DigitCap}. Έπειτα, λαμβάνουμε την μέση τιμή των στοιχείων του κάθε πίνακα προσοχής ξεχωριστά και έχουμε 10 ομοιότητες ψήφων: μια για κάθε κάψουλα γονέα. Σημειώνουμε ότι όλες τις ψήφους τις κλιμακώνουμε πριν υπολογίσουμε τα εσωτερικά γινόμενα ώστε να έχουν όλες μοναδιαίο μήκος.\par

Με μαθηματικούς όρους και χρησιμοποιώντας τον ενισχυμένο συμβολισμό (\en{notation}) του προηγούμενου κεφαλαίου έχουμε:
\begin{equation}
    \forall j \in \Omega_{L+1}: Similarity^L_{j} \gets \hat{V}^L_{j:} \times \hat{V}^{LΤ}_{j:}
\end{equation}
όπου το $\hat{V}^L$ συμβολίζει τις ψήφους αφότου (η κάθε μια) διαιρεθεί με την νόρμα της ώστε να έχει μοναδιαίο μήκος. Ισχύει $\hat{V}^L \in \Re^{ n^{L+1} \times n^L \times d^{L+1}}$.
% image 10
\begin{figure}[h]
    \centering
    \includegraphics[trim={8cm 0 7.5cm 0},clip, width=0.98\textwidth]{images/chapter experiments/method 1/image 10/barplot_combined.png}
    \caption{Γραφικές παραστάσεις που απεικονίζουν την μέση συμφωνία ψήφων που έχουν προκύψει από τον αλγόριθμο δρομολόγησης με συμφωνία όταν αυτός τροφοδοτείται με εικόνες που περιέχουν τα νούμερα 4, 1 και 9 (από αριστερά προς τα δεξιά).}
    \label{fig:exp_method_1_special_vote_sim_1}
  \end{figure}

Από τα πειράματα αυτά, λαμβάνουμε τα αποτελέσματα που φαίνονται στην εικόνα \ref{fig:exp_method_1_special_vote_sim_1} παρατηρούμε ότι η συμφωνία παίζει καθοριστικό ρόλο στην δρομολόγηση των καψουλών αφού μέσω αυτής διαμορφώνονται τα βάρη δρομολόγησης. Στην εικόνα \ref{fig:exp_method_1_special_vote_sim_1}, στα δεξιά, συμπεριλαμβάνουμε και μια σπάνια περίπτωση όπου η μέση συμφωνία είναι μεγαλύτερη σε μη ορθή κλάση. Αυτό, όταν η διαφορά είναι μικρή, δεν οδηγεί απαραίτητα σε λανθασμένη πρόβλεψη (διότι έχουμε λάβει την μέση συμφωνία).\par
% image 11
\begin{figure}[h]
    \centering
    \includegraphics[ width=0.9\textwidth]{images/chapter experiments/method 1/image 11/barplot_number_9_batch_0.png}
    \caption{Γραφική παράσταση που απεικονίζει την μέση συμφωνία ψήφων που έχουν προκύψει από τον αλγόριθμο δρομολόγησης \en{Argmax Scaled Routing} όταν αυτός τροφοδοτείται με εικόνες που περιέχουν το νούμερο 9. Παρατηρούμε ότι η μέση συμφωνία της σωστής κλάσης είναι η πιο χαμηλή.}
    \label{fig:exp_method_1_special_vote_sim_2}
  \end{figure}
Δοκιμάσαμε το ίδιο πείραμα και σε μοντέλο εκπαιδευμένο στον αλγόριθμο \en{Argmax Routing} (βλέπε σχήμα \ref{fig:exp_method_1_special_vote_sim_2}). Όπως είναι αναμενόμενο, ο αλγόριθμος αδυνατώντας να αποδομήσει αποδοτικά την εικόνα, παράγει ψήφους που δεν έχουν μεγάλη μέση συμφωνία μεταξύ τους. Μάλιστα, φαίνεται στις περισσότερες περιπτώσεις να ισχύει το ανάποδο: δηλαδή η κλάση με την μικρότερη μέση συμφωνία φένεται να είναι η σωστή\footnote{Παραπέμπουμε τον αναγνώστη στην ιστοσελίδα του κώδικα για περισσότερα αποτελέσματα.}. Από άλλα πειράματα που δεν περιλαμβάνονται προέκυψε ότι ο συγκεκριμένος αλγόριθμος προτιμά να παράγει ψήφους για την σωστή κλάση όπου οι περισσότερες να μην εμφανίζουν υψηλή συμφωνία (εξού και η χαμηλή τιμή μέσης συμφωνίας). Αντίθετα, προτιμά, μεταξύ των ψήφων που προορίζονται για την σωστή κλάση να υπάρχουν ελάχιστες με την μέγιστη δυνατή συμφωνία (ώστε να \textquote{κερδίσουν} το ένα και μεγαλύτερο βάρος δρομολόγησης). Φυσικά, κάτι τέτοιο είναι απολύτως λογικό αφού μια είναι η ψήφος που θα προωθηθεί στο επόμενο επίπεδο. Από την στιγμή που δεν λαμβάνεται σαν αποτέλεσμα το σταθμισμένο άθροισμα και δεν υπάρχει φόβος τα διανύσματα να ακυρώσουν το ένα το άλλο (\en{cancel each other out}) δεν αποτελεί πρόβλημα η μικρή μέση συμφωνία.\par

\subsection{Κατανομή των Ψήφων}
% Πειράματα με τα γραφήματα με τις μπάρες.
Συνεχίζοντας τα ειδεικά πειράματα, θα θέλαμε να διερενύσουμε την υπόθεση σύμφωνα με την οποία, ο δυναμικός αλγόριθμος δρομολόγησης μπορεί να εκφυλιστεί στην επιλογή της μέγιστης σε μήκος ψήφου. Ισοδύναμα, θέλουμε να ελέγξουμε αν το μοντέλο διαμορφώνει τέτοια εκπαιδευόμενα βάρη ώστε να μη διαδραματίζει τόσο σημαντικό ρόλο η συμφωνία μεταξύ των ψήφων όσο το μήκος των ψήφων.\par

Από την μειωμένη επίδοση του αλγορίθμου \en{Max Routing}, ακόμα και όταν για την εκπαίδευση έχει χρησιμοποιηθεί ο αλγόριθμος \en{Classic Routing}, είμαστε προδιατεθημένοι να πιστέψουμε ότι ο ανωτέρω ισχυρισμός δεν ευσταθεί. Πράγματι, τα πειράματα που διενεργούνται σε αυτή τη παράγραφο επιβεβαιώνουν την προδιάθεσή μας.\par
% image 12
% \begin{figure}[h]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.3\linewidth, height=0.15\textheight]{images/chapter experiments/method 1/image 12/barplot_for_class_1_unscaled_True.pdf}
%     \caption{Γραφική παράσταση του μέσου μήκους των ψήφων ανά κλάση.}
%     \label{fig:exp_method_1_special_vote_dist_1}
%     \end{minipage}
    
%     \begin{minipage}{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.3\linewidth, height=0.15\textheight]{images/chapter experiments/method 1/image 12/mean_var_for_class_1_unscaled_True.pdf}
%     \caption{Γραφική παράσταση του μέσου μήκους των ψήφων ανά κλάση. Στο διάγραμμα εμφανίζεται και η διασπορά με κόκκινες, κάθετες γραμμές.}
%     \label{fig:exp_method_1_special_vote_dist_2}
%     \end{minipage}
    
%   \end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[ width=0.9\textwidth]{images/chapter experiments/method 1/image 12/barplot_for_class_1_unscaled_True.pdf}
    \caption{Γραφική παράσταση του μέσου μήκους των ψήφων ανά κλάση.}
    \label{fig:exp_method_1_special_vote_dist_1}
  \end{figure}

\begin{figure}[h]

    \centering
    \includegraphics[width=0.9\textwidth]{images/chapter experiments/method 1/image 12/mean_var_for_class_1_unscaled_True.pdf}
    \caption{Γραφική παράσταση του μέσου μήκους των ψήφων ανά κλάση. Στο διάγραμμα εμφανίζεται και η διασπορά με κόκκινες, κάθετες γραμμές.}
    \label{fig:exp_method_1_special_vote_dist_2}
    
  \end{figure}

Οπως φαίνεται από τις εικόνες \ref{fig:exp_method_1_special_vote_dist_1} και \ref{fig:exp_method_1_special_vote_dist_2} το μέτρο των αρχικών ψήφων (πρωτού κλιμακωθούν από τα βάρη δρομολόγησης) δεν φαίνεται να συσχετίζεται με κανέναν τρόπο με την κλάση πρόβλεψης. Να σημειώσουμε ότι τα εξής αποτελέσματα δεν αφορούν ένα μεμονομένο παράδειγμα αλλά υπολογίζονται από τις ψήφους που προκύπτουν από (σχεδόν) όλα τα παραδείγματα της επιλεγμένης κλάσης στο σύνολο δεδομενων ελέγχου. Για αυτό άλλωστε και η διασπορά είναι τόσο μεγάλη.

\subsection{Κατανομή των Βαρών Δρομολόγησης}
% Ιστόγραμμα καθώς αυξάνονται οι επαναλήψεις.
Φαίνεται ότι τον καθοριστικό ρόλο της επίδοσης τον έχουν τα βάρη δρομολόγησης. Μάλιστα, από τα μέχρι τώρα πειράματα έχει φανεί πως η επιλογή της κλάσης στην οποία αντιστοιχεί η μέγιστη τιμή βάρους δρομολόγησης μπορεί να έχει καλύτερη επίδοση από τον κλασικό αλγόριθμο δρομολόγησης. Θέλωντας να εμβαθύνουμε στην κατανομή των βαρών δρομολόγησης μετά από τρείς επαναλήψεις, δημιουργούμε τρείς γραφικές παραστάσεις όπου έχουμε συγκεντρώσει τα βάρη δρομολόγησης από πάρα πολλά παραδείγματα του συνόλου ελέγχου που απεικονίζουν ένα συγκεκριμένο ψηφίο. Τα βάρη αυτά τα χωρίζουμε ανάλογα με το σε ποιά κάψουλα \en{DigitCap} απευθύνονται.\par
% image 13
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/chapter experiments/method 1/image 13/barplot_for_class_7.png}
    \caption{Γραφική παράσταση του της μέσης τιμής των βαρών δρομολόγησης ανά κλάση.}
    \label{fig:exp_method_1_special_weight_dist_1}
  \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/chapter experiments/method 1/image 13/mean_var_for_class_7.png}
    \caption{Γραφική παράσταση της μέσης τιμής των βαρών δρομολόγησης ανά κλάση. Στο διάγραμμα εμφανίζεται και η διασπορά με κόκκινες, κάθετες γραμμές.}
    \label{fig:exp_method_1_special_weight_dist_2}
  \end{figure}

Από τα σχήματα \ref{fig:exp_method_1_special_weight_dist_1} και \ref{fig:exp_method_1_special_weight_dist_2} φαίνεται ότι η διασπορά είναι μεγαλύτερη για τα βάρη που αντιστοιχούν στις σωστές κάψουλες. Δεν μπορούμε όμως να διακρίνουμε την σωστή κλάση κοιτώντας τις μέσες τιμές. Αν όμως λάβουμε την μέγιστη τιμή των βαρών δρομολόγησης για την κάθε κάψουλα του τελευταίου επιπέδου, διαπιστώνουμε ότι πάντα η σωστή κλάση έχει βάρη δρομολόγησης με την μεγαλύτερη τιμή (βλέπε σχήμα \ref{fig:exp_method_1_special_weight_dist_3}).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/chapter experiments/method 1/image 13/Max_for_class_7.png}
    \caption{Γραφική παράσταση της τιμής του μεγίστου βάρους δρομολόγησης ανά κλάση.}
    \label{fig:exp_method_1_special_weight_dist_3}
  \end{figure}

\subsection{Ιστογράμματα των Σταθμισμένων Ψήφων ανα Αριθμό Επαναλήψεων}
Το τελευταίο πείραμα που πραγματοποιείται εξετάζει την κατανομή των μηκών των σταθμισμένων (από το αντίστοιχο βάρος δρομολόγησης) ψήφων, καθώς αυξάνονται οι επαναλήψεις του αλγορίθμου δυναμικής δρομολόγησης. Χρησιμοποιώντας τον συμβολισμό που εισαγάγαμε στο προηγούμενο κεφάλαιο, υπολογίζουμε τα ιστογράμματα των διανυσμάτων $V^L_{ij} \ast R^L_{ij}$ ομαδοποιημένων κατά κλάση $j$. Έτσι προκύπτουν τα σχήματα \ref{fig:exp_method_1_special_hist_r_3} , \ref{fig:exp_method_1_special_hist_r_2} και \ref{fig:exp_method_1_special_hist_r_1} μετά από 3, 2 και 1 επαναναλήψεις αντίστοιχα. Προφανώς, στην πρώτη επανάληψη τα μήκη εξαρτώνται από τους πίνακες μετασχηματισμού αφού τα βάρη δρομολόγησης είναι όλα ίσα. Σε επόμενες επαναλήψεις όμως, τα βάρη δρομολόγησης επιρρεάζουν το μήκος των ψήφων με το να αποσιωπούν τις ψήφους σε κλάσεις με μικρή συμφωνία (που δεν αντιστοιχούν στην σωστή κλάση πρόβλεψης) και με το να ενισχύουν ορισμένες ψήφους που δρομολογούνται στην σωστή κλάση. Σημειώνουμε ότι τα ιστογράμματα βρίσκονται σε λογαριθμική κλίμακα και για την παραγωγή τους λήφθηκαν υπόψη τα περισσότερα δείγματα του συνόλου ελέγχου που αφορούν την συγκεκριμένη κλάση στόχο (ο λόγος που δεν λήφθηκαν όλα υπόψη είναι καθαρά υπολογιστικός).
% image 14
\begin{figure}[h]
    \centering
    \includegraphics[trim={14cm 0 13cm 0},clip, width=0.99\textwidth]{images/chapter experiments/method 1/image 14/hists_for_class_1_r_3.png}
    \caption{Ιστογράμματα του μήκους των ψήφων σταθμισμένων από τα αντίστοιχά τους βάρη δρομολόγησης μετά την τρίτη επανάληψη του δυναμικού αλγροίθμου δρομολόγησης.}
    \label{fig:exp_method_1_special_hist_r_3}
  \end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[trim={14cm 0 13cm 0},clip, width=0.99\textwidth]{images/chapter experiments/method 1/image 14/hists_for_class_1_r_2.png}
    \caption{Ιστογράμματα του μήκους των ψήφων σταθμισμένων από τα αντίστοιχά τους βάρη δρομολόγησης μετά την δεύτερη επανάληψη του δυναμικού αλγροίθμου δρομολόγησης.}
    \label{fig:exp_method_1_special_hist_r_2}
  \end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[trim={14cm 0 13cm 0},clip, width=0.99\textwidth]{images/chapter experiments/method 1/image 14/hists_for_class_1_r_1.png}
    \caption{Ιστογράμματα του μήκους των ψήφων σταθμισμένων από τα αντίστοιχά τους βάρη δρομολόγησης μετά την πρώτη επανάληψη του δυναμικού αλγροίθμου δρομολόγησης.}
    \label{fig:exp_method_1_special_hist_r_1}
  \end{figure}


\section{Πειραματική Μελέτη Μεθόδου 2}
Σε αυτή την ενότητα παρουσιάζουμε ορισμένα από τα πειράματα που πραγματοποιήθηκαν στον αλγόριθμο δρομολόγησης βασισμένο στον \en{EM}. Συγκεκριμένα, πειραματιστήκαμε με δύο υλοποιήσεις στην γλώσσα \en{tensorflow}. Η πρώτη υλοποίηση είναι αυτή της \en{IBM} και περιγράφεται αναλυτικά στο έργο \cite{gritzman2019avoiding}. Η δεύτερη αποτελεί την αυθεντική υλοποίηση του έργου \cite{hinton2018matrix} και εντοπίζεται σε αυτή την \href{https://git.informatik.uni-hamburg.de/0moin/google-research/-/commits/master/capsule_em}{ιστοσελίδα}. Αν και η δεύτερη, πολύπλοκη υλοποίηση από την ομάδα της \en{Google Brain} επέτρεπε την εκπαίδευση και δοκιμή μόνο στο σύνολο δεδομένων \en{smallNORB} με τις κατάλληλες επεκτάσεις επιδιώξαμε να εκπαιδεύσουμε το δίκτυο και στο σύνολο δεδομένων \en{MNIST}. Προφανώς, όλη η προεπεξεργασία των συνόλων δεδομένων είναι ίδια με αυτή του έργου \cite{hinton2018matrix}. Επίσης, κατασκευάσαμε εκ νέου ένα αρχείο \en{Docker} για την κατασκευή εικονικού περιβάλοντος με όλες τις κατάλληλες εκδόσεις λογισμικού. Να σημειώσουμε ότι καμία από τις δύο υλοποιήσεις δεν ήταν σε πλήρη μορφή καθώς δεν προορίζονταν για εκπαίδευση. Χρειάστικε να γίνουν αρκετές αλλαγές προκειμένου να προσαρμοστούν στις δικές μας απαιτήσεις.\par

Πρωτού παρουσιάσουμε τα πειράματα που έγιναν στην μέθοδο δύο, να αναφέρουμε ότι πειραματιστίκαμε για λίγες εποχές και με ορισμένες άλλες επιλογές που παρέχουν οι υλοποιήσεις. Παρόλα αυτά, τα συμπεράσματα ήταν ασαφή για τις λίγες εποχές εκπαίδευσης. Μερικές από τις παραμετροποιήσεις που μπορούν να δοκιμαστούν είναι ο αριθμός των καψουλών του επιπέδου \en{Primary Capsules}, το μέγεθος των πυρήνων των πρώτων επιπέδων και ο αριθμός των φίλτρων τους, την προσθήκη επιπλέον συνελικτικών επιπέδων με ή χωρίς κάψουλες, την χρήση του μηχανισμού \en{dropout}, την χρήση ενός συνόλου ίδιων μοντέλων εκπαιδευμένων στο ίδιο σύνολο για την δοκιμή στο σύνολο ελέγχου (\en{ensemble}) κτλ.\par

Παρακάτω θα παρουσιάσουμε τα πειράματα που έγιναν σε περιορισμένο αριθμό επαναλήψεων για δύο διαφορετκούς αριθμούς επαναλήψεων (2 και 3) του αλγορίθμου δρομολόγησης. Αν και το μοντέλο δεν έχει πολλές εκπαιδευόμενες παραμέτρους ($310k$) η υπολογιστική του πολυπλοκότητα είναι περίπου πέντε φορές μεγαλύτερη (\en{0.401 operations per batch versus 0.086 operations per batch}). Αυτό δεν μας επέτρεψε να διενεργήσουμε εκτενή πειράματα καθώς και επίσης να χρησιμοποιήσουμε μεγάλο μέγεθος δέσμης.

\subsection{\en{SmallNORB}}
Για να διερευνήσουμε την επίδραση του αριθμού των επαναλήψεων στην ακρίβεια του αλγορίθμου στο σύνολο \en{SmallNORB} εκπαιδεύσαμε το μοντέλο με τον αλγόριθμο δρομολόγησης βασισμένο στον \en{EM} (αλγόριθμος \ref{alg:em_routing}) για 15000 βήματα με σύνολο δέσμης ίσο με 8 (το μέγιστο δυνατό) το οποίο ισοδυναμεί με 5 επόχές. Τα αποτελέσματα που λάβαμε παρουσιάζονται στον πίνακα \ref{tab:method_2_smallNORB_rout_iter}.
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c}
            \toprule
            Dataset & Routing Iterations & Test Error (\%) \\ 
            \midrule
            SmallNORB & 2 & \textbf{14.04} \\
            SmallNORB & 3 & 63.49 \\
            \bottomrule
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_2_smallNORB_rout_iter}Πίνακας στον οποίο φαίνεται η επίδραση του αριθμού των επαναλήψεων στην ακρίβεια όπως μετράται από το σύνολο ελέγχου \en{SmallNORB}, όταν χρησιμοποιούνται πολύ λίγες εποχές για την εκπαίδευση του μοντέλου.} 
\end{table}

Όπως προδίδουν τα αποτελέσματα, ειδεικά όταν χρησιμοποιείται μικρό σύνολο δέσμης, ο αλγόριθμος είναι πιθανό να βρεθεί σε αστάθεια. Φυσικά, γνωστά προβλήματα του αλγορίθμου \en{EM} όπως το \en{variance collapse} δεν διευκολύνουν την διαδικασία εκπαίδευσης με αποτέλεσμα αυτή να γίνειται όλο και πιο ασταθής καθώς αυξάνονται οι επαναλήψεις. Στις εικόνες ... και ... παρουσιάζονται το συνολικό σφάλμα και η ακρίβεια (μέγιστη ακρίβεια είναι 1) για τις δύο περιπτώσεις του πίνακα \ref{tab:method_2_smallNORB_rout_iter} (με κόκκινο απεικονίζεται η εκπαίδευση για 2 επαναλήψεις ενώ με μπλέ για 3). Παρατηρούμε ότι στην περίπτωση των 3 επαναλήψεων ο αλγόριθμος μπαίνει σε κατάσταση ευστάθειας και δεν ανακάμπτει μέχρι το τέλος της εκπαίδευσης.
% image 1


\subsection{\en{MNIST}}

Συνεχίζουμε με τη διερεύνηση της επιρροής του αριθμού των επαναλήψεων στην ακρίβεια του αλγορίθμου στο σύνολο δεδομένων \en{MNIST}. Και πάλι εκπαιδεύσαμε το μοντέλο με τον αλγόριθμο δρομολόγησης βασισμένο στον \en{EM} (αλγόριθμος \ref{alg:em_routing}) για 15000 βήματα με σύνολο δέσμης ίσο με 8 το οποίο ισοδυναμεί σε αυτή τη περίπτωση με μόλις 4 επόχές. Τα αποτελέσματα που λάβαμε εμφάνιζαν (λόγω και του μικρού συνόλου δέσμης) μεγάλο βαθμό αστάθειας και συνεπώς δεν ήταν αξιοσημείωτα.

\subsection{Δοκιμή στο \en{SmallNORB} με \en{Pretrained Model}}
Τέλος, δοκιμάσαμε την επίδοση του αλγορίθμου στο σύνολο δεδομένων \en{SmallNORB} με την χρήση ενός προ\textendash εκπαιδευμένου μοντέλου (από το διαδίκτυο) με βάρη που σχηματίστικαν κατά την διάρκεια πολύ περισσότερων εποχών. Τα αποτελέσματα, παρουσιάζονται στον πίνακα \ref{fig:method_2_last_matrix} και είναι ίδια με αυτά που παρουσιάζονται στο σχετικό έργο \cite{hinton2018matrix} επιβεβαιώνωντας έτσι ότι οι αλλαγές που προκαλέσαμε στον κώδικα δεν επιβάρυναν την επίδοση. Στον σχετικό πίνακα συγκρίνονται οι δύο χρησιμοποιούμενες υλοποιήσεις του αλγορίθμου. Όπως φαίνεται, η υλοποίηση που χρησιμοποιούμε (μαζί με τις αλλαγές μας που δεν επηρεάζουν τον βασικό αλγόριθμο) εμφανίζει την καλύτερη επίδοση (μετρημένη με την χρήση του \en{pre\textendash trained} μοντέλου).\par

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c}
            \toprule
            Implementation & Framework & r & Test Error (\%) \\ 
            \midrule
            Hinton et al.\cite{hinton2018matrix} & tensorflow & 3 & 1.8 (1.4*) \\
            Hinton et al. + Our modifications & tensorflow & 3 & \textbf{1.8 (1.3*)} \\
            Matrix Capsules IBM \cite{gritzman2019avoiding}& tensorflow & 2 & 4.6 \\
            Matrix Capsules IBM \cite{gritzman2019avoiding}& tensorflow & 3 & 6.3 \\
            \bottomrule
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_2_last_matrix}Πίνακας στον οποίο συγκρίνονται οι επιδόσεις των δύο χρησιμοποιούμενων υλοποιήσεων της μεθόδου 2 (\ref{alg:em_routing}) στο σύνολο δεδομένων \en{SmallNORB}. Για την δεύτερη υλοποίηση, δεν υπήρχε διαθέσιμο κάποιο προ\textendash εκπαιδευμένο μοντέλο για να δοκιμάσουμε τα επικαλούμενα αποτελέσματα. Σημειώνουμε ότι το αστεράκι σημαίνει ότι η πρόβλεψη προκύπτει από την μέση τιμή των προβλέψεων τυχαίων παραθύρων (\en{random crops}) της ίδιας εικόνας. Η βελτιωμένη επίδοση μετά τις δικές μας αλλαγές πιθανότητα δεν οφείλεται σε αυτές αλλά στο καλύτερο, \en{pre-trained} μοντέλο που χρησιμοποιούμε.} 
\end{table}


\section{Πειραματική Μελέτη Μεθόδου 3}

Στην ενότητα αυτή θα πειραματιστούμε με τους γρήγορους αλγορίθμους δρομολόγησης με μηχανισμό αυτοπροσοχής που παρουσιάσαμε στην ενότητα \ref{sec:method_3}. Υπενθυμίζεται ότι στην ενότητα αυτή παρουσιάστικαν τέσσερεις αλγόριθμοι καθώς και οι παραλλαγές τριών εξ' αυτών για την περίπτωση που χρησιμοποιείται μηχανισμός πολυκέφαλης προσοχής (\en{multiheadd attention}). Οι τέσσερεις αλγόριθμοι και οι τρείς πολυκέφαλες παραλλαγές τους που παρουσιάστηκαν στην σχετική μέθοδο είναι με την σειρά οι εξής: 
\begin{itemize}
    \item \textquote{Απλοικός Αλγόριθμος Δρομολόγησης με Αυτο\textendash Προσοχή} (αλγόριθμος \ref{alg:method3_stupid_routing})
    \item \textquote{Αλγόριθμος \en{RooMAV}} (αλγόριθμος \ref{alg:method3_sum_routing})
    \item \textquote{Αλγόριθμος \en{RoWSS}} (αλγόριθμος \ref{alg:method3_max_rooting})
    \item \textquote{Αλγόριθμος \en{RoWL}} (αλγόριθμος \ref{alg:method3_max_len_rooting})
    \item \textquote{Αλγόριθμος \en{Multihead RooMAV}} (αλγόριθμος \ref{alg:method3_sum_multihead_rooting})
    \item \textquote{Αλγόριθμος \en{Multihead RoWSS}} (αλγόριθμος \ref{alg:method3_max_multihead_rooting})
    \item \textquote{Αλγόριθμος \en{Multihead RoWL}} (αλγόριθμος \ref{alg:method3_max_len_multihead_rooting})
\end{itemize}

Ο πρώτος στην λίστα, απλοικός αλγόριθμος μιας και αποτελεί έργο των \en{Mazzia V. et al.}\cite{mazzia2021efficient} στο οποίο παρουσιάζονται εκτενή πειράματά του, δεν συμμετέχει στο πειραμταικό μέρος αυτής της ενότητας. Αντίθετα, οι άλλοι (δικοί μας) αλγόριθμοι δοκιμάζονται με πειράματα των οποίον στόχος είναι η πιστοποίηση μιας γρήγορης, κλιμακώσιμης αρχιτεκτονικής νευρωνικών δικτύων με κάψουλες. Για τα πειράμτα αυτά, χρησιμοποιούνται όλα τα απαραίτητα σύνολα δεδομένων που σε ένα βαθμό ελέγχουν την τήρηση των ιδιοτήτων των νευρψνικών δικτύων με κάψουλες. Τα σύνολα δεδομεων είναι τα \en{MNIST}, \en{CIFAR10}, \en{SmallNORB} και \en{multiMNIST} με τα τελευταία δύο να εξετάζουν την ικανότητα του νευρωνικού δικτύου από κάψουλες να γενικεύει σε νέες οπτικές γωνίες (\en{SmallNORB}) και να χειρίζεται αποδοτικά μερική επικάλυψη των εικόνων (\en{multiMNIST}) αντίστοιχα.\par

Εστιάζοντας ακόμα περισσότερο στα σύνολα δεδομένων που χρησιμοποιούνται στα πειράματα της τρίτης μεθόδου, η προεπεξεργασία τους διαφέρει από αυτή που παρουσιάσαμε στα περιάματα της μεθόδου 1. Αναλυτικότερα, για κάθε σύνολο δεδομένων κάνουμε τα εξής:
\begin{description}
    \item 
\end{description}
Αναφορικά με τις λεπτομέρειες υλοποίησης, χρησιμοποιείται ο βελτιστοποιητής \en{nAdam} ενώ σε λίγες περιπτώσεις δοκιμάζεται ο βελτιστοποιητής \en{Adam} (όπου γίνεται η χρήση του θα αναγράφεται με ρητό τρόπο). Στις περιπτώσεις που χρησιμοποιείται ανακατασκευαστής, εκτός από το σφάλμα \en{Margin Loss} έχουμε και το σφάλμα που προκύπτει από την ανακατασκευή (\en{MSE loss}). Τα δύο σφάλματα αθροίζονται και διαμορφώνουν το συνολικό σφάλμα εκπαίδευσης, όπως στην περίπτωση της μεθόδου \ref{sec:method_1}. Η διαφορά έγκυται στο ότι το σφάλμα ανακατασκευής κλιμακώνεται πρωτού αθροιστεί με το σφάλμα \en{Margin loss} με τον παράγοντα 0.2. Επιπλέον, αξίζει να αναφέρουμε ότι σε ορισμένες περιπτώσεις δοκιμάζουμε εκθετικό πρόγραμμα μείωσης του ρυθμού μάθησης (\en{exponential decay})τον οποίο και θέτουμε στην τιμή $0.001$.\par

Στις επόμενες υπο\textendash ενότητες παρουσιάζοται τα πειράματα που έγιναν με τους διαθέσιμους αλγορίθμους. Δυστυχώς, οι περιορισμένοι πόροι δεν μας επέτρεψαν να κάνουμε μια εξονυχιστική αναζήτηση στον χώρο των υπερπαραμέτρων για την κάθε μέθοδο ξεχωριστά και για το κάθε σύνολο δεδομένων. Άλλωστε, σκοπός μας σε αυτή τη μέθοδο είναι να παρακινήσουμε την έρευνα προς μια υποσχόμενη αρχιτεκτονική που αναπτύξαμε στην τρίτη μέθοδο καταδηκνύοντας αρκετά καλά αποτελέσματα που μπορούν να βελτιωθούν σημαντικά με περεταίρω αναζήτηση των βέλτιστων υπερπαραμέτρων.\par

\subsection{Αναζήτηση στον Χώρο των Υπερπαραμέτρων}
Στην υποενότητα αυτή παρουσιάζουμε ορισμένα από τα πειράματα που έγιναν στο πλαίσιο αναζήτησης των βέλτιστων υπερπαραμέτρων για τους αλγορίθμους μας. Οι υπερπαράμετροι για τις οποίες αναζητήσαμε την βέλτιστη τιμή συμπεριλαμβάνουν την χρήση ή μη πολυκέφαλης προσοχής, τη χρήση ή μη ανακατασκευαστή (αλλά και το είδος του ανακατασκευαστή) αλλά και ειδεικές - για τον κάθε αλγόριθμο - υπερπαραμέτρους όπως η κλιμάκωση των εκπροσώπων με το αντίστοιχο σκόρ ομοιότητας (\en{Similarity Score}) και η ομαλή μεγιστοποίηση ή όχι των \en{similarity scores}.\par

Το σύνολο στο οποίο γίνεται η αναζήτηση των υπερπαραμέτρων είναι το \en{SmallNORB}. Επιλέξαμε το συγκεκριμένο σύνολο καθώς τα περισσότερα νευρωνικά δίκτυα με κάψουλες εξετάζουν την επίδοσή τους σε αυτόν γεγονός που επιτρέπει την σύγκριση των επιδόσεών τους. Επίσης, είναι ένα πιό σύνθετο σύνολο δεδομένων από το \en{MNIST} γεγονός που διακρύνει άμεσα τις παραμετροποιήσεις που οδηγούν σε μοντέλα με μέτριες επιδόσεις.\par

Δυστυχώς, οι υπολογιστικοί πόροι που έχουμε στην διαθεσιμότητά μας δεν μας επιτρέπουν να πειραματιστούμε με την εναλλακτική αρχιτεκτονική των πρώτων επιπέδων (αυτή δηλαδή που ακολουθεί και το έργο \cite{hinton2018matrix} και όχι αυτή που παρουσιάσαμε στο σχήμα \ref{fig:method_1_architecture}). Αυτό συμβαίνει διότι η διαδικασία εξαγωγής καψουλών που ακολουθείται στο έργο \en{Dynamic Routing Between Capsules}\cite{sabour2017dynamic} παράγει πολύ μεγάλο αριθμό από \en{Primary Capsules} οι οποίες λόγω του αλγορίθμου δρομολόγησής μας, θα πρέπει να συγκριθούν μεταξύ τους (μέσω των παραγώμενων ψήφων τους). Συνεπώς, τα παρακάτω αποτελέσματα αφορούν την αρχιτεκτονική \en{DepthConv}. Σε κάθε περίπτωση, μια τέτοια υλοποίηση θα ξέφευγε από τους σκοπούς της μεθόδου για την ανάπτυξη μιας κλιμακώσιμης (\en{scalable}) και γρήγορης αρχιτεκτονικής νευρωνικών δικτύων με κάψουλες.\par

Σαν μια προσπάθεια ελάτωσης της υπερπροσαρμογής του δικτύου στα δεδομένα εκπαίδευσης, εφαρμόσαμε μετά τα πρώτα δύο συνελικτικά επίπεδα με παράμετρο $p_{value} 0.2$, όπως ήθιστε για την περίπτωση των συνελικτικών δικτύων.\par

Εκτός από τα πειράματα στις παραμέτρους που εμφανίζονται στους πίνακες που ακολουθούν, έγιναν επιπλέον μελέτες για την μη γραμμική συνάρτηση που εφαρμόζεται σημειακά στα στοιχεία των πινάκων προσοχής. Συγκεκριμένα, περίπου το 40\% των πειραμάτων που παρατίθενται σε αυτή την ενότητα επαναλήφθηκαν για την μη γραμμική συνάρτηση \en{Leaky ReLU} (αντί για τη συνάρτηση \en{ReLU}). Τα αποτελέσματα των πειραμάτων αυτών, εφόσον είναι καλύτερα, εμφανίζονται με αστερίσκο (*).\par

Τέλος, σημειώνουμε ότι έγινε εκτενής πειραματική μελέτη (15 περίπου πειράματα) για το είδος και τον αριθμό των επιπέδων κανονικοποίησης που χρησιμοποιούνται στα πρώτα επίπεδα του δικτύου. Ειδικά για το σύνολο \en{SmallNORB}, λόγω της ιδιαίτερης φύσης του, είναι ωφέλιμο να γίνεται αξιοποίηση της τεχονολίας κανονικοποίησης που εφαρμόζεται ξεχωριστά σε κάθε παράδειγμα και σε κάθε κανάλι αυτού (\en{Instance Normalization}). Τελικά βρέθηκε ότι η χρήση \en{Instance Normalization} μετά τα πρώτα τρία συνελικτικά επίπεδα ενισχύει την επίδοση ενώ μετά το τέταρτο συνελικτικό επίπεδο παρατηρείται βελτίωση με τη χρήση επιπέδου κανονικοποίησης κατά δέσμες \en{Batch Normalization}.

\subsubsection{Πειραματικά Αποτελέσματα Μεθόδου \en{RooMAV}}
Στην παράγραφο αυτή παρουσιάζουμε τα αποτελέσματα από τα πειράματα που πραγματοποιήθηκαν στον αλγόριθμο \en{RooMAV} (αλγόριθμος \ref{alg:method3_sum_routing}) σε διάφορες παραμετροποιήσεις του αλγορίθμου. Σημειώνουμε ότι οι δοκιμές αυτές δεν περιλαμβάνουν παραμέτρους όπως ο ρυθμός μάθησης και τη τιμή κλιμάκωσης του σφάλματος ανακατασκευής. Επίσης, σημειώνουμε ότι χρησιμοποιούμε σχετικά μικρό μέγεθος δέσμης (με τιμή 8) και εκπαιδεύουμε για 30 εποχές.\par 

Φυσικά, καλύτερα αποτελέσματα μπορούν να προκύψουν με την διερεύνηση επιπλέον υπερπαραμέτρων (π.χ. σχετικών με τον περιορισμό της υπερπροσαρμογής) και την εκπαίδευση σε περισσότερες εποχές. Άλλωστε, σκοπός των περιαμάτων αυτών δεν είναι η εύρεση της καλύτερης παραμετροποίησης αλλά η απόδειξη ότι οι αλγόριθμοι που έχουμε αναπτύξει είναι ικανοί, με λιγότερη πολυπλοκότητα, να ανταγωνιστούν τους αλγορίθμους που ανέπτυξε η ομάδα του \en{Hinton G.} αλλά και άλλους αλγορίθμους δρομολόγησης νευρωνικών δικτύων με κάψουλες της βιβλιογραφίας.\par

Στον πίνακα \ref{tab:method_3_hyper_tuning_RooMAV} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Για το κάθε πείραμα έχουμε καταγράψει τον αριθμό παραμέτρων του κωδικοποιητή αλλά και τον χρόνο για την εκτέλεση μιας πλήρους εποχής και ενός βήματος. Επιπρόσθετα, έχουμε καταγράψει μεταξύ άλλων, τον τύπο του αποκωδικοποιητή όταν χρησιμοποιούμε έναν. Τα δύο είδη είναι ο πλήρως διασυνδεδεμένος (\en{FC}) με $5,289.9k$ και ο αποκωδικοποιητής με επίπεδα αποσυνέλιξης (\en{Decon.}) που έχει $229.5k$ παραμέτρους. Σημειώνουμε ότι όταν ο αριθμός των κεφαλών είναι μονάδα ουσιαστικά προστίθενται δύο πλήρως διασυνδεδεμένα επίπεδα (βλέπε ενότητα \ref{sec:transformers}).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
             Epoch T. (s) & Step T. (ms) & Param. Count & Heads & Recon. (type) & Test Error (\%) \\ 
            \midrule
            34 & 11 & 151.8k & 0 & yes (FC) &  2.2222 \\
            45 & 15 & 151.8k & 0 & yes (Decon.) &  2.0123 \\
            28 & 09 & 151.8k & 0 & no &  2.3086 \\
            36 & 12 & 152.8k & 1 & yes (FC) &  1.9095 \\
            47 & 16 & 152.8k & 1 & yes (Decon.) &  1.7901 \\
            31 & 10 & 152.8k & 1 & no &  2.2428 \\
            36 & 12 & 152.8k & 2 & yes (FC) &  1.7984 \\
            47 & 16 & 152.8k & 2 & yes (Decon.) &  \textbf{1.4403}\\ % lrelu: 2.0535
            31 & 10 & 152.8k & 2 & no &  1.5761 <<-\\ % lrelu: 2.0041
            \bottomrule
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_3_hyper_tuning_RooMAV}Αποτελέσματα πειραμάτων για την εύρεση των βέλτιστων υπερπαραμέτρων του αλγορίθμου \en{RooMAV} αρχιτεκτονικής \en{DepthConv} στο σύνολο δεδομένων \en{SmallNORB}. Τα πειράματα αυτά πραγματοποιήθηκαν για 30 εποχές με μέγεθος δέσμης ίσο με 8.} 
\end{table}

Τα αποτελέσματα είναι αρκετά ενθαρυντικά, ειδικά αν αναλογιστεί κανείς ότι η καλύτερη υλοποίηση της ομάδας του \en{Hinton et al.} στο συγκεκριμένο dataset έχει ποσοστό σφάλματος (\en{test error}) ίσο με $1.8$. Συγκριτικά, ο απλοικός αλγόριθμος δρομολόγησης με προσοχή (αλγόριθμος \ref{alg:method3_stupid_routing}) από τον οποίο εμπνεύστηκε η τρίτη μέθοδος έχει ποσοστό σφάλματος $2.54$. Επίσης, πρέπει να λάβουμε υπόψη ότι τα αποτελέσματα αυτά έχουν προκύψει από μόλις 30 εποχές χωρίς πολλά αντίμετρα υπερπροσαρμογής (\en{overfitting}).\par

Από τα αποτελέσματα των πειραμάτων για τον αλγόριθμο \en{RooMAV} είναι εμφανές ότι η προσθήκη κεφαλών προσοχής (\en{attention heads}) συμβάλει στην επίδοση του δικτύου χωρίς να αυξάνεται σημαντικά ο αριθμός των παραμέτρων. Μάλιστα, από τις δύο στις τρείς κεφαλές προσοχής ο αριθμός των παραμέτρων δεν αυξάνεται αφού διαιρούμε το μέγεθος αναπαράστασης των διανυσμάτων της κάθε κεφαλής (\en{Q, K, V}) δια δύο. Δηλαδή, με τους όρους της ενότητας \ref{sec:method_3}, είναι $d_k^L = d_v^L = d^{L+1}//2$.\par

Επίσης, μπορούμε να παρατηρήσουμε ότι η χρήση αποκωδικοποιητή με επίπεδα αποσυνέλιξης (\en{fractionally strided convolutional layers}) αφενώς αυξάνει σε πιο μεγάλο βαθμό την επίδοση από άλλους κωδικοποιητές και αφετέρου δεν επιβαρύνει σημαντικά το υπολογιστικό κόστος αφού προστίθενται $229.5k$ παράμετροι (σε αντίθεση με τον αποκωδικοποιητή \en{FC} που προσθέτει $5,289.9k$ παραμέτρους).\par

Τέλος, στο σχήμα απεικονίζονται οι γραφικές παραστάσεις του σφάλματος στα σύνολα εκπαίδευσης και επαλήθευσης για το μοντέλο με την καλύτερη επίδοση. Παρατηρούμε ότι η χρήση μικρού συνόλου δέσμης σε συνδειασμό με την έλειψη μεθόδων αποτροπής υπερπροσαρμογής (πέρα από τον ανακατασκευαστή και τα επίπεδα \en{Dropout}) οδηγεί σε βαθμιαία υπερπροσαρμογή των βαρών στο σύνολο εκπαίδευσης αλλά και σε αστάθεια.

\subsubsection{Πειραματικά Αποτελέσματα Μεθόδου \en{RoWSS}}

Συνεχίζοντας την πειραματική μελέτη της μεθόδου 3, επόμενος αλγόριθμος στην σειρά είναι ο \en{RoWSS} (αλγόριθμος \ref{alg:method3_max_rooting}). Σε αυτόν τον αλγόριθμο, τα πειράματα είναι περισσότερα καθώς θέλουμε να εξετάσουμε, εκτός από τις υπερπαραμέτρους που πειράζαμε στην προηγούμενη παράγραφο, επιπέον υπερπαραμέτρους που δεν ήταν διαθέσιμοι στον προηγούμενο αλγόριθμο (εκ φύσεως). Αναλυτικότερα, αυτές οι παραλλαγές είναι η κλιμάκωση των εκπροσώπων με τις τιμές των αντίστοιχων σκορ ομοιότητας (\en{ScaleEMB}) αλλά και η εφαρμογή της συνάρτησης \en{Softmax} στα σκορ ομοιότητας (\en{SoftSC}) \footnote{Βλέπε αλγόριθμο \ref{alg:method3_max_rooting} γραμμή \ref{op:method3_max_softmax_like_previous_softmax}.}.\par

Τα σχετικά πειράματα παρατίθενται στον πίνακα \ref{tab:method_3_hyper_tuning_RoWSS}. Σημειώνουμε ότι τα αποτελέσματα με (*) έχουν προκύψει από την χρήση της μη γραμμικής συνάρτησης \en{Leaky ReLU} καθότι, μετά από πειράματα βρέθεικε πως συνέβαλε στην επίδοση\footnote{Θα ήταν αδύνατο να ενσωματώσουμε όλα τα πειράματα στο παρόν έργο. Σε περίπτωση που ορισμένα δε βρίσκονται στην \href{https://github.com/abarmper/Capsule_Nets_with_uncertainty}{ιστοσελίδα} του κώδικά, μπορούν να παρασχεθούν μετά από αίτημα μαζί με τα αρχεία βαρών των μοντέλων.}.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c c c}
            \toprule
            Epoch (s) & Step (ms) & Param. Count & Heads & SoftSC & ScaleEmb & Recon. (type) & Test Error (\%) \\ 
            \midrule
            35 & 11 & 151.8k & 0 & yes & no & yes (FC) & 1.9959 \\%(δοκιμή LRelu)\\auta sto telos % 9
            34 & 11 & 151.8k & 0 & no & no & yes (FC) & 2.0412 <<-\\ % sto telos % 9.25
            35 & 12 & 151.8k & 0 & yes & yes & yes (FC) & 3.0206 \\
            35 & 11 & 151.8k & 0 & no & yes & yes (FC) & 2.1564 \\

            45 & 15 & 151.8k & 0 & yes & no & yes (Decon.) & 1.7984 <<-\\ % lrelu:1.8930 (1.8477)(2.5350)\\
            45 & 15 & 151.8k & 0 & no & no & yes (Decon.) & 1.7407 <<-\\%lrelu:1.8354 (2.3498) (2.0165)\\
            46 & 15 & 151.8k & 0 & yes & yes & yes (Decon.) & 2.5885 \\
            45 & 15 & 151.8k & 0 & no & yes & yes (Decon.) & 2.8642 \\

            29 & 09 & 151.8k & 0 & yes & no & no & 2.1280 <<-\\%lrelu:2.4403\\
            28 & 09 & 151.8k & 0 & no & no & no & 1.5309 <<-\\%(δοκιμή LRelu)\\
            29 & 10 & 151.8k & 0 & yes & yes & no & 2.3498 \\
            29 & 10 & 151.8k & 0 & no & yes & no & 2.2346 \\

            37 & 12 & 152.8k & 1 & yes & no & yes (FC) & 2.0741 <<-\\%(θέλω μικρότερο του 1.99)\\
            37 & 12 & 152.8k & 1 & no & no & yes (FC) & 2.2881 <<-\\%(θέλω μικρότερο του 2.04)\\
            37 & 12 & 152.8k & 1 & yes & yes & yes (FC) & 2.8848 \\
            37 & 12 & 152.8k & 1 & no & yes & yes (FC) & 1.9218  \\

            48 & 16 & 152.8k & 1 & yes & no & yes (Decon.) & 2.7860 <<-\\%%(θέλω < του 1.79)\\
            48 & 16 & 152.8k & 1 & no & no & yes (Decon.) & 2.8971 <<-\\%(θέλω < του 1.74)
            48 & 16 & 152.8k & 1 & yes & yes & yes (Decon.) & 2.2757 \\
            48 & 16 & 152.8k & 1 & no & yes & yes (Decon.) & 2.1152 \\

            31 & 10 & 152.8k & 1 & yes & no & no & 1.6626 <<-\\%(θέλω μικρότερο του 1.53)
            31 & 10 & 152.8k & 1 & no & no & no & 2.0041 <<-\\%%(θέλω μικρό)
            31 & 10 & 152.8k & 1 & yes & yes & no & 2.0370 \\
            31 & 10 & 152.8k & 1 & no & yes & no & 2.9877 \\

            37 & 12 & 152.8k & 2 & yes & no & yes (FC) & 2.9712 <<-\\ %(θέλω < 1.99) 
            37 & 12 & 152.8k & 2 & no & no & yes (FC) & 2.3909 <<- \\%(θέλω < 2.04) 
            38 & 12 & 152.8k & 2 & yes & yes & yes (FC) & 2.8272 \\
            37 & 13 & 152.8k & 2 & no & yes & yes (FC) & 2.8313 \\

            48 & 16 & 152.8k & 2 & yes & no & yes (Decon.) & 1.5597 <<-\\ %(θέλω < του 1.79) (δοκ. LRelu) 
            48 & 16 & 152.8k & 2 & no & no & yes (Decon.) & 1.8807 <<-\\ %(θέλω < του 1.74)(δοκιμή LRelu)  
            49 & 16 & 152.8k & 2 & yes & yes & yes (Decon.) & 3.4774 \\
            49 & 16 & 152.8k & 2 & no & yes & yes (Decon.) & 2.1276  \\

            31 & 10 & 152.8k & 2 & yes & no & no & 2.5720 <<-\\
            31 & 10 & 152.8k & 2 & no & no & no & 2.4198 <<-\\
            31 & 10 & 152.8k & 2 & yes & yes & no & 2.5679 \\
            31 & 10 & 152.8k & 2 & no & yes & no & 2.3292 \\

            \bottomrule
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_3_hyper_tuning_RoWSS}Αποτελέσματα εκτενών πειραμάτων για την εύρεση των βέλτιστων υπερπαραμέτρων του αλγορίθμου \en{RoWSS} αρχιτεκτονικής \en{DepthConv} στο σύνολο δεδομένων \en{SmallNORB}. Πραγματοποιήθηκαν για 30 εποχές με μέγεθος δέσμης ίσο με 8.} 
\end{table}

Από τα ανωτέρω αποτελέσματα είναι ασφαλές να υποθέσουμε ότι η κλιμάκωση των εκπροσώπων δεν βοηθάει την επίδοση του δικτύου. Εν αντιθέση, προκαλεί μεγαλύτερη αστάθεια στο δίκτυο κατά την εκπαίδευση. \par

Αναφορικά με τον αριθμό των κεφαλών και το είδος του αποκωδικοποιητή, οι παρατηρήσεις είναι παρόμοιες με αυτές του αλγορίθμου \en{RooMAV}. Δηλαδή, και εδώ η αύξηση των κεφαλών γενικά οδηγεί σε καλύτερη επίδοση χωρίς ουσιαστική επιβάρυνση της πολυπλοκότητας. Επιπλέον, η χρησιμότητα του δικτύου αποκωδικοποιητή είναι λιγότερο προφανής. Ένας πιθανός λόγος είναι ότι σε διατάξεις του αλγορίθμου η τιμή κλιμάκωσης του σφάλματος ανακατασκευής δεν είναι η κατάλληλη.

\subsubsection{Πειραματικά Αποτελέσματα Μεθόδου \en{RoWL}}
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c c c}
            \toprule
            Epoch (s) & Step (ms) & Param. Count & Heads & SoftSC & ScaleEmb & Recon. (type) & Test Error (\%) \\ 
            \midrule
            34 & 11 & 151.8k & 0 & yes & no & yes (FC) &  2.3992 \\% 
            34 & 11 & 151.8k & 0 & no & no & yes (FC) & 2.2757 \\
            35 & 12 & 151.8k & 0 & yes & yes & yes (FC) & 2.2881 \\
            35 & 11 & 151.8k & 0 & no & yes & yes (FC) & 2.2593  \\

            45 & 15 & 151.8k & 0 & yes & no & yes (Decon.) & 2.5103 <<-\\ %
            45 & 15 & 151.8k & 0 & no & no & yes (Decon.) & 1.7613 <<-\\%
            46 & 15 & 151.8k & 0 & yes & yes & yes (Decon.) & 2.3868 \\
            45 & 15 & 151.8k & 0 & no & yes & yes (Decon.) 2.2263  \\

            29 & 09 & 151.8k & 0 & yes & no & no & 2.0658 <<-\\% δοκιμή relu
            28 & 09 & 151.8k & 0 & no & no & no & 1.8107 <<-\\% δοκιμή relu
            29 & 10 & 151.8k & 0 & yes & yes & no & 2.2675 \\% 20.5
            29 & 10 & 151.8k & 0 & no & yes & no & 2.5597 \\% 20.75

            37 & 12 & 152.8k & 1 & yes & no & yes (FC) & 2.4280 <<-\\% 21.0
            37 & 12 & 152.8k & 1 & no & no & yes (FC) & 2.0658 <<-\\%

            48 & 16 & 152.8k & 1 & yes & no & yes (Decon.) & 2.8272 <<-\\%
            48 & 16 & 152.8k & 1 & no & no & yes (Decon.) & 1.8477 <<-\\%

            31 & 10 & 152.8k & 1 & yes & no & no & 2.1235 <<-\\%
            31 & 10 & 152.8k & 1 & no & no & no & 2.1523 <<-\\%

            37 & 12 & 152.8k & 2 & yes & no & yes (FC) 2.6996  <<-\\ % 
            37 & 12 & 152.8k & 2 & no & no & yes (FC) & 2.1317 <<-\\% 

            48 & 16 & 152.8k & 2 & yes & no & yes (Decon.) & 2.2058 <<-\\ % 
            48 & 16 & 152.8k & 2 & no & no & yes (Decon.) & 2.2675 <<-\\

            31 & 10 & 152.8k & 2 & yes & no & no & 2.2099 <<-\\%
            31 & 10 & 152.8k & 2 & no & no & no & 2.1276 <<-\\%

            \bottomrule
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_3_hyper_tuning_RoWL}Αποτελέσματα πειραμάτων για την εύρεση των βέλτιστων υπερπαραμέτρων του αλγορίθμου \en{RoWL} αρχιτεκτονικής \en{DepthConv} στο σύνολο δεδομένων \en{SmallNORB}. Όλα τα πειράματα της ενότητας πραγματοποιήθηκαν για 30 εποχές με μέγεθος δέσμης ίσο με 8.} 
\end{table}

\subsection{Παρατηρήσεις}
Επιλέγουμε τους ... για επιλεκτική εμβάυνση
\subsection{Επιλεκτική Εμβάθυνση στα Σύνολα Δεδομένων }
\section{Πειραματική Μελέτη Μεθόδου 4}
\section{Σύγκριση Πειραματικών Αποτελεσμάτων Μεθόδων}
% Για κάθε dataset ξεχωριστά. Όχι νέα πειράματα.