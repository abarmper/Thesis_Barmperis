\chapter{Πειραματική Μελέτη}

Στην παρούσα ενότητα παρουσιάζουμε τα αποτελέσματα των πειραμάτων που διενεργήθηκαν στην κάθε μια οικογένεια αλγορίθμων. Παρόλα αυτά, δεν είναι σκοπός η βελτιστοποίηση της απόδοσης (όπως καταγράφεται από τις επιλεγμένες μετρικές) για κάθε αλγόριθμο. Όπως έχουμε αναφέρει, ο σκοπός της παρούσας διπλωματικής είναι διττός: αφενός επιθυμούμε να εξερευνήσουμε την επίδραση της χαλάρωσης ορισμένων υποθέσεων των νευρωνικών δικτύων με κάψουλες στην απόδοσή τους (μέθοδος 1) και αφετέρου να επιλύσουμε το πρόβλημα της κλιμακωσιμότητας προτείνοντας έναν αποδοτικό αλγόριθμο δρομολόγησης (μέθοδος 3). Η τέταρτη, πολυδύναμη, μέθοδος, βρίσκεται στο μεταίχμιο αυτών με τη δυνατότητα τόσο για επιλεκτική χαλάρωση των περιορισμών της εν λόγω τεχνολογίας όσο για μερική βελτίωση του χρόνου εκπαίδευσης. Τέλος, η δεύτερη μέθοδος, λόγο της μεγάλης υπολογιστικής πολυπλοκότητάς της που δεν επέτρεπε τον εκτενή πειραματισμό, περιορίζεται σε δύο σύνολα δεδομένων και αναλαμβάνει τον σκοπό τις σύγκρισης με τις υπόλοιπες μεθόδους μας.\par

Όπως γίνεται αντιληπτό, κυρίως για τους αλγορίθμους της μεθόδου 1 αλλά και για αυτούς της μεθόδου 4, μας ενδιαφέρει περισσότερο η σχετική επίδοση μεταξύ αυτών αφού αυτή φανερώνει αν οι περιορισμοί που επιβάλλονται τελικά συμβάλουν στην καλύτερη γενίκευση του δικτύου ή όχι. Για τον λόγο αυτό, δίνουμε έμφαση στη σύγκριση των επιδόσεων μεταξύ των αλγορίθμων που ανήκουν στην ίδια οικογένεια (ίδια μέθοδο). Βέβαια, για λόγους πληρότητας, επιλέγουμε τους καλύτερους αλγορίθμους από την κάθε μέθοδο και τους συγκρίνουμε στο τέλος του παρόντος κεφαλαίου.\par

Κρίνεται σκόπιμο να αναφερθεί πως σε κάθε περίπτωση, η πειραματική μας μελέτη δεν είναι πλήρης. Ορισμένοι αλγόριθμοι που αναπτύξαμε (και ειδικά αυτοί που απαντώνται στην πολυδύναμη τέταρτη μέθοδο) διαμορφώνονται από μια πληθώρα υπερπαραμέτρων όπου η κάθε μια επιδρά καθοριστικά στην απόδοσή τους. Επιπρόσθετα, οι μειωμένοι υπολογιστικοί πόροι που διαθέτουμε καθιστούν τη διαδικασία πειραματισμού ιδιαιτέρως χρονοβόρα. Συνεπώς, είναι χρήσιμο να έχουμε υπ' όψη ότι οι επιδώσεις που καταγράφουμε πιθανότατα να επιδέχονται βελτίωση.\par

Το παρόν κεφάλαιο ακολουθεί την εξής διάρθρωση:
\begin{enumerate}
    \item Αρχικά γίνεται μια σύντομη παρουσίαση των συνόλων δεδομένων που χρησιμοποιούμε, των μετρικών αλλά και της πλατφόρμας πειραματισμού.
    \item Έπειτα ακολουθούν οι πειραματικές μελέτες της κάθε μεθόδου ξεχωριστά. Τα περιεχόμενα της κάθε τέτοιας υποενότητας διαφέρουν σημαντικά ανάλογα με τον σκοπό της εκάστοτε μεθόδου. Σε γενικές γραμμές όμως, περιλαμβάνουν τα αποτελέσματα που προκύπτουν από την αναζήτηση ικανοποιητικών υπερπαραμέτρων στα διάφορα σύνολα δεδομένων και τη σύγκριση των αλγορίθμων μεταξύ τους.
    \item Τέλος, επιλέγουμε τους καλύτερους αλγορίθμους από διαφορετικές μεθόδους και τους συγκρίνουμε μεταξύ τους αλλά και με άλλες υλοποιήσεις νευρωνικών δικτύων με κάψουλες που συναντώνται στη βιβλιογραφία.
\end{enumerate}

\section{Πλατφόρμα Διεξαγωγής Πειραμάτων, Μετρικές και Σύνολα Δεδομένων} 
Στην ενότητα αυτή κάνουμε λόγο για τα αμετάβλητα στοιχεία που συνθέτουν το περιβάλλον της πειραματικής μας μελέτης. Αυτά περιλαμβάνουν το υπολογιστικό σύστημα στο οποίο διενεργήθηκαν όλα τα πειράματα, τις μετρικές που χρησιμοποιήθηκαν για την εκτίμηση της επίδοσης και τα σύνολα δεδομένων με τα οποία οι αλγόριθμοι τροφοδοτήθηκαν.
\subsection{Πειραματική Πλατφόρμα}
Όλα τα πειράματα εκτελέστηκαν τοπικά, στον προσωπικό υπολογιστή (\en{PC}). Επειδή το σύστημα εκτέλεσης των πειραμάτων επηρεάζει τους χρόνους εκπαίδευσης, κρίνεται σκόπιμη η περιγραφή των δυνατοτήτων του υπολογιστικού μας συστήματος. Από πλευράς υλικού (\en{hardware}) λοιπόν, τα χαρακτηριστικά του είναι τα εξής:
\begin{itemize}
    \item 16GB \en{DDR4 RAM}
    \item \en{AMD Ryzen} 9 3900\en{X, 12-core CPU}
    \item \en{Nvidia RTX 2070 super GPU}
\end{itemize}

Ένα πολύ καλό εργαλείο για την αντιστοίχηση της υπολογιστικής δυνατότητας μιας συσκευής σε μια μετρική για την αντιπαραβολή με τις δυνατότητες άλλων συστημάτων είναι το \en{\it{ai-benchmark}}. Τρέχοντας το σχετικό πρόγραμμα εκτίμησης δυνατοτήτων, λάβαμε, μεταξύ άλλων τα παρακάτω αποτελέσματα:
\begin{itemize}
    \item \en{\it{Device Inference Score: 12150}}
    \item \en{\it{Device Training Score: 12115}}
    \item \en{\it{Device AI Score: 24265}}
\end{itemize}
Βέβαια, το περιβάλλον πειραματισμού απαρτίζεται και από τις εκδόσεις των πακέτων λογισμικού που είναι εγκατεστημένες στο σύστημα. Για αυτό τον σκοπό, στην \href{https://github.com/abarmper/Capsule_Nets_with_uncertainty}{ιστοσελίδα} όπου είναι αναρτημένος ο κώδικας, έχουμε καταγράψει όλα τα απαιτούμενα πακέτα λογισμικού. Ενδεικτικά, τα βασικότερα στοιχεία λογισμικού είναι τα εξής:
\begin{itemize}
    \item \en{Platform: \it{Linux}, Release: \it{5.15.0-48-generic}, Version: \it{20.04.1-Ubuntu}}
    \item \en{CUDA version: \it{11.0}}
    \item \en{cudnn version: \it{8}}
    \item \en{Tensorflow version: \it{2.4.1}}
    \item \en{Pytorch version: \it{1.7.1+cu110}}
\end{itemize}
Για να διευκολύνουμε την αναπαραγωγή πειραμάτων, ενσωματώνουμε και ένα εικονικό περιβάλλον (\en{Docker}). Το σχετικό αρχείο (\en{DockerFileGenericGPU}) δημιουργεί ένα εικονικό περιβάλλον με τις τα απαραίτητα πακέτα λογισμικού (\en{dependences}) που χρειάζεται να είναι εγκατεστημένα για την εκτέλεση των προγραμμάτων.
\subsection{Μετρικές Επίδοσης}

Λόγο του ερευνητικού χαρακτήρα της παρούσας διπλωματικής, μας ενδιαφέρει κυρίως η σύγκριση των μεθόδων μας με τις υπόλοιπες σχετικές μεθόδους που απαντώνται στη βιβλιογραφία. Για τον σκοπό αυτό και με δεδομένο ότι όλες οι εργασίες είναι εργασίες ταξινόμησης, η μετρική της ακρίβειας (\en{accuracy}) είναι η πλέον κατάλληλη μετρική. Η μετρική αυτή ορίζεται από τον λόγο των σωστά ταξινομημένων προβλέψεων προς το σύνολο των προβλέψεων. Με μαθηματικούς όρους δηλαδή, έχουμε:
\begin{equation}
    Accuracy = \frac{\text{\en{Number of Correct Predictions}}}{\text{\en{Total Number of Predictions}}}
\end{equation}
Ειδικά για το σύνολο δεδομένων \en{MultiMNIST} όπου έχουμε δύο προβλέψεις για κάθε δείγμα εισόδου, η μετρική μας ονομάζεται πολλαπλή\textendash Ακρίβεια (\en{multi-Accuracy}). Παρόλα αυτά, σύμφωνα με τον παραπάνω ορισμό (που εστιάζει στον αριθμό των προβλέψεων και όχι των δειγμάτων εισόδου) οι δύο μετρικές είναι ταυτόσημες.\par

Συχνά, αντί για την ακρίβεια, χρησιμοποιείται σαν μετρική το ποσοστιαίο σφάλμα ελέγχου (\en{test error rate\%}). Στην πραγματικότητα, δεν αποτελεί μια ξεχωριστή μετρική αφού ισχύει ότι $$test\_error\_rate = 100 - Accuracy*100\%.$$ \par

Όπως έχει γίνει αντιληπτό από το πρώτο κεφάλαιο της εργασίας, μας ενδιαφέρει να εντοπίσουμε μια αρχιτεκτονική με μικρό υπολογιστικό κόστος. Δύο μετρικές που φανερώνουν την ποσότητα αυτή είναι ο (σχετικός) μέσος χρόνος εκπαίδευσης ενός συνόλου δέσμης και ο αριθμός των εκπαιδευόμενων παραμέτρων ενός μοντέλου (των παραμέτρων δηλαδή που ρυθμίζονται με τον αλγόριθμο της οπισθοδιάδοσης σφάλματος). Συνεπώς, εκτός από την ακρίβεια, οι δύο αυτές μετρικές προστίθενται στα κριτήρια επιλογής των καλύτερων αλγορίθμων.

\subsection{Σύνολα Δεδομένων}
Στις περισσότερες μεθόδους μας χρησιμοποιούμε όλα τα σχετικά σύνολα δεδομένων με τα οποία δοκιμάζονται συνήθως οι αρχιτεκτονικές νευρωνικών δικτύων με κάψουλες. Γεγονός, που μας επιτρέπουν να εξετάσουμε αν τηρούνται οι χαρακτηριστικές ιδιότητες της εν λόγω τεχνολογίας από τα νέα μοντέλα που αναπτύξαμε. Τα σύνολα δεδομένων με τα οποία καταπιανόμαστε είναι τα \en{MNIST}\cite{deng2012mnist}, \en{FashionMNIST}\cite{Xiao2017FashionMNISTAN}, \en{CIFAR10}\cite{CIFAR10}, \en{MultiMNIST}\cite{sabour2017dynamic} και \en{smallNORB}\cite{lecun2004learning}. Για λόγους πληρότητας, στον πίνακα \ref{tab:exp_datasets} φαίνονται τα μοντέλα επιβλεπόμενης μάθησης που επιτυγχάνουν τη μέγιστη ακρίβεια για το κάθε σύνολο δεδομένων (καταγράφονται στην \href{https://paperswithcode.com/}{ιστοσελίδα} τον Οκτώβριο του 2022).


\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c } 
        \hline
        Dataset & Method & Test Error (\%) \\
        \hline 
         MNIST & Heterogeneous ensemble with simple CNN\cite{an2020ensemble} & 0.09 \\ 
         
         FashionMNIST & Fine-Tuning DARTS\cite{tanveer2021fine} & 3.09 \\ 
         
         CIFAR-10 & ViT-H/14\cite{dosovitskiy2020image_is_worth_16} & 0.5 \\ 
         
         MultiMNIST & CapsNet\cite{sabour2017dynamic} & 5.2 \\ 
         
         smallNORB & Heinsen Routing\cite{heinsen2019algorithm} & 0.90 \\ 
         
        \end{tabular}
        }
        \end{center}
        \caption{\label{tab:exp_datasets} Πίνακας που συγκεντρώνει το καλύτερο μοντέλο και την απόδοσή του, για κάθε σύνολο δεδομένων.}
    \end{table}
    \subsubsection{Περιγραφή Συνόλων Δεδομένων \en{smallNORB} και \en{multiMNIST}}
    Τα δύο σύνολα δεδομένων είναι λιγότερο δημοφιλή στην ακαδημαϊκή κοινότητα για αυτό αφιερώνουμε αυτή την παράγραφο για την περιγραφή τους. Και τα δύο εξετάζουν την ιδιότητα των νευρωνικών δικτύων από κάψουλες να γενικεύουν σε νέες οπτικές γωνίες και να εξηγούν εικόνες με σημαντική επικάλυψη.\par

    Αναφορικά με το σύνολο δεδομένων \en{smallNORB}, αυτό περιέχει στερεο\textendash οπτικές εικόνες που απεικονίζουν 50 αντικείμενα (παιχνίδια) τα οποία ανήκουν σε 5 κλάσεις: τετράποδα ζώα, ανθρώπινες φιγούρες, αεροπλάνα, φορτηγά και αυτοκίνητα. Η κάθε κλάση εκπροσωπείται από δέκα φυσικά αντικείμενα, τα μισά εξ'αυτών βρίσκονται στο σύνολο εκπαίδευσης. Το σύνολο δεδομένων δημιουργήθηκε από την στερεοσκοπική λήψη αυτών των αντικειμένων από δύο κάμερες υπό 6 διαφορετικές συνθήκες φωτισμού, 9 διαφορετικά υψόμετρα προβολής (γωνίες 30◦έως 70◦ με βήμα 5◦) και 18 διαφορετικά αζιμούθια (γωνίες 0◦ έως 340◦ με βήμα 20◦). Έτσι, τόσο το σύνολο εκπαίδευσης όσο και το σύνολο ελέγχου αποτελούνται από 24.300 ζευγάρια στερεο\textendash πτικών εικόνων το καθένα. Οι αλγόριθμοί μας, δέχονται κάθε ζεύγος εικόνων σαν ένα δείγμα και στόχος τους είναι να προβλέψουν το απεικονιζόμενο αντικείμενο.\par

    Το σύνολο δεδομένων \en{multiMNIST} δημιουργήθηκε από τους \en{Hinton et al.} κατά την συγγραφή του έργου \cite{sabour2017dynamic}. Στην πραγματικότητα, ο αριθμός των δειγμάτων και τα περιεχόμενα του συνόλου δεν είναι προκαθορισμένα αφού η κάθε υλοποίηση κατασκευάζει δυναμικά το σύνολο αυτό λαμβάνοντας εικόνες από το σύνολο δεδομένων \en{MNIST} (αυτός είναι και ένας λόγος του περιορισμένου πειραματισμού με αυτό το σύνολο δεδομένων στη βιβλιογραφία). Ουσιαστικά, αποτελείται από εικόνες που απεικονίζουν στο ίδιο πλαίσιο, δύο επικαλυπτόμενα ψηφία (με ποσοστό επικάλυψης περίπου 80\%). Όπως είναι λογικό, κάθε ένα τέτοιο δείγμα συνοδεύεται από δύο ετικέτες: μια για κάθε απεικονιζόμενο ψηφίο.
    

\section{Πειραματική Μελέτη Μεθόδου 1}

Στην παρούσα ενότητα πραγματοποιούνται πειράματα στους τέσσερεις αλγορίθμους της μεθόδου 1 για να εκτιμηθεί η επίδοσή τους στα σύνολα δεδομένων \en{MNIST}\cite{deng2012mnist}, \en{FashionMNIST}\cite{Xiao2017FashionMNISTAN}, \en{CIFAR10}\cite{CIFAR10} και \en{smallNORB}\cite{lecun2004learning}. Υπενθυμίζεται ότι όλοι οι αλγόριθμοι της μεθόδου 1 έχουν ίδια αρχιτεκτονική με αυτήν που χρησιμοποιείται στο έργο \cite{sabour2017dynamic} αλλά διαφέρουν στον αλγόριθμο δρομολόγησης ο οποίος βαθμιαία, από τον πρώτο αλγόριθμο στον τέταρτο γίνεται απλούστερος. Συνεπώς, ο ρόλος των πειραμάτων της πρώτης μεθόδου δεν είναι απαραίτητα να προτείνει μια αντικατάσταση του δυναμικού αλγορίθμου δρομολόγησης. Βασικότερος σκοπός είναι να εξεταστεί η επίδραση που έχουν ορισμένες απλουστεύσεις του αλγορίθμου δυναμικής δρομολόγησης (αλγόριθμος \ref{alg:dynam_max_routing}) στη μάθηση (αλγόριθμοι \ref{alg:dynam_argmax_scaled_routing} και \ref{alg:dynam_argmax_routing}) και την επίδοση του αλγορίθμου μετά τη χαλάρωση της υπόθεσης περί φιλτραρίσματος υψηλής, πολυδιάστατης συμφωνίας (αλγόριθμος \ref{alg:dynam_max_routing}).\par

Μεταξύ των τεσσάρων αλγορίθμων που εξετάζονται, συμπεριλαμβάνεται και ο αυθεντικός αλγόριθμος της δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος 1). Αν και αυτός δεν αποτελεί μια δική μας μέθοδο, συμπεριλαμβάνεται στα πειράματα για να διευκολύνει την ισότιμη σύγκριση με τους άλλους αλγορίθμους της μεθόδου. Επίσης, οι υψηλές επιδόσεις που εντοπίζονται στον αλγόριθμο 1 πιστοποιούν την ορθή υλοποίηση της αρχιτεκτονικής του δικτύου που μοιράζονται και οι άλλες τρεις παραλλαγές.\par

Η διάρθρωση των πειραμάτων του κεφαλαίου έχει ως εξής: Για τα σύνολα δεδομένων \en{MNIST} και \en{CIFAR-10} κάνουμε πειράματα για τον εντοπισμό των υπερπαραμέτρων των μεθόδων (ρυθμός μάθησης κ.ο.κ.) με τα καλύτερα αποτελέσματα (χρησιμοποιώντας λίγες εποχές). Χρησιμοποιούμε αυτά τα δύο σύνολα καθώς έχουν μεγάλη ετερογένεια μεταξύ τους. Αφού βρεθούν αυτές οι υπερπαράμετροι, εμβαθύνουμε στο κάθε σύνολο δεδομένων ξεχωριστά εκπαιδεύοντας τους τέσσερεις αλγορίθμους με περισσότερες εποχές και με τις αντίστοιχες παραμετροποιήσεις τους ενώ έπειτα, τους συγκρίνουμε. Στην προ\textendash τελευταία υπο-ενότητα παρουσιάζουμε συγκεντρωτικά τα αποτελέσματα και αναφέρουμε τις παρατηρήσεις μας σχετικά με την επίδραση των υποθέσεών μας στις επιδόσεις. Στην τελευταία υπο-ενότητα παρουσιάζουμε τα συμπεράσματά μας από ορισμένα ειδεικά πειράματα που αποσκοπούν να διερευνήσουν την εσωτερική λειτουργία των αλγορίθμων μας.\par

Σχετικά με τις λεπτομέρειες υλοποίησης να αναφέρουμε ότι η συνάρτηση σφάλματος είναι αυτή που έχει οριστεί στην ενότητα \ref{sec:method1_loss_fn}. Δηλαδή, είναι η συνάρτηση σφφάλματος περιθορίου (\en{Margin Loss}) με την προσθήκη του μέσου τετραγωνικού σφάλματος (κλιμακωμένου κατά $0.0005$) στην περίπτωση που χρησιμοποιείται αποκωδικοποιητής. Ο βελτιστοποιητής (\en{optimizer}) που χρησιμοποιούμε είναι ο \en{Adam} (\en{adaptive moment estimation}) ενώ χρησιμοποιούμε και σύστημα ελάττωσης του ρυθμού μάθησης όταν δεν παρατηρείται μείωση του σφάλματος στη διάρκεια ενός προκαθορισμένου αριθμού εποχών (\en{learning rate scheduler with reduce on plateau and patience}). Σε αρκετές περιπτώσεις, είναι απαραίτητη η χρήση ενός συνόλου επικύρωσης (\en{validation set}) το οποίο πάντα αποτελεί το 10\% του συνόλου δεδομένων εκπαίδευσης. Αξίζει να σημειωθεί επίσης ότι για την αποφυγή της υπερεκπαίδυσης (\en{overfitting}) αποθηκεύουμε το μοντέλο με το μικρότερο σφάλμα κατά την διάρκεια της εκπαίδευσης και χρησιμοποιούμε αυτό στο σύνολο ελέγχου.\par

Τέλος, σχετικά με την προεπεξεργασία των συνόλων δεδομένων, αυτή είναι όσο πιο πιστή γίνεται στο έργο \cite{sabour2017dynamic}. Πιο συγκεκριμένα, για το κάθε σύνολο δεδομένων έχουμε:
\begin{description}
    \item[\en{MNIST}] Κανονικοποίηση, ολίσθηση στον κατακόρυφο και οριζόντιο άξονα κατά 2 εικονοστοιχεία το μέγιστο (με \en{zero padding}). Μέγεθος εισόδου: [$1 \times 28 \times 28$]\footnote{Οι αλγόριθμοι της μεθόδου 1 είναι ανεπτυγμένοι στη γλώσσα \en{Pytorch} και συνεπώς, χρησιμοποιείται μια αναπαράσταση δεδομένων τύπου \en{channels-first}}.
    \item[\en{FashionMNIST}] Κανονικοποίηση μόνο. Μέγεθος εισόδου: [$1 \times 28 \times 28$]
    \item[\en{CIFAR10}] Κανικοποίηση για το καθένα από τα τρία κανάλια και πρερικοπή παραθύρων μεγέθους $28 \times 28$ (το αρχικό ύψος και πλάτος είναι $32 \times 32$). Μέγεθος εισόδου [$3 \times 28 \times 28$].
    \item[\en{smallNORB}] Κλιμάκωση σε μέγεθος $48 \times 48$ (από το αρχικό μέγεθος που είναι $96 \times 96$), κάνουμε περικοπή σε ένα παράθυρο μεγέθους $32 \times 32$ και τέλος, προσθέτουμε ταιχαία φωτεινότητα και αντίθεση στο σύνολο εκπαίδευσης (παρόμοια με το έργο \cite{hinton2018matrix}). Επειδή το σύνολο αποτελείται από στερεοπτικές εικόνες, τις στοιβάζουμε δημιουργώντας μια εικόνα με δύο κανάλια. Μέγεθος εισόδου μετά από προεπεξεργασία: [$2 \times 32 \times 32$].
\end{description} 
%  και τέλος, προσθέτουμε ταιχαία φωτεινότητα και αντίθεση στο σύνολο εκπαίδευσης (παρόμοια με το έργο \cite{hinton2018matrix})
Σημειώνουμε ότι κατά τη δημιουργία του συνόλου ελέγχου, το παράθυρο περικοπής είναι κεντραρισμένο στην εικόνα ενώ κατά τη δημιουργία του συνόλου εκπαίδευσης το παράθυρο αυτό είναι τυχαίο για το κάθε δείγμα.

\subsection{Εύρεση Βέλτιστων Υπερπαραμέτρων}

Στα πρώτα πειράματα της μεθόδου 1 διερευνούμε τις υπερπαραμέτρους των αλγορίθμων που εμφανίζουν τα καλύτερα αποτελέσματα. Οι υπερπαράμετροι αυτοί αφορούν τον ρυθμό μάθησης (\en{Learning Rate - Lr}), το σύνολο δέσμης (\en{Batch Size - Bs}), τη χρήση αποκωδικοποιητή (\en{reconstruction}) ή όχι και τον αριθμό επαναλήψεων (\en{Routing Iterations - r}) - με εξαίρεση τον αλγόριθμο \en{Max Routing} ο οποίος δεν είναι επαναληπτικός. Ο αριθμός των εποχών είναι μόλις 30 καθώς τα πειράματα είναι πολλά και οι υπολογιστικοί πόροι περιορισμένοι. Παρόλα αυτά, ο αριθμός των εποχών είναι αρκετός για να βρεθεί μια καλή παραμετροποίηση για τον κάθε αλγόριθμο. Στην επόμενη υποενότητα, θα εκπαιδεύσουμε επιλεκτικά τα μοντέλα για περισσότερες επαναλήψεις. 

\subsubsection{Σύνολο Δεδομένων \en{MNIST}}
Τα πρώτα πειράματα που έγιναν στο σύνολο δεδομένων \en{MNIST} αφορούν τον κλασσικό δυναμικό αλγόριθμο δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}). Τα αποτελέσματα των πειραμάτων παρατίθενται στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg1}.
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.51 \\
            & 64 & 3 & 0.001 & no & 0.37\footnote{\gr{Σκορ πολύ κοντά στο 0.35, όπως παρουσιάζεται στο \cite{sabour2017dynamic} για τις ίδιες παραμέτρους, κάτι που πιστοποιεί την ακεραιότητα της υλοποίησής μας.}} \\
            & 32 & 3 & 0.001 & yes & 0.37 \\
            & 64 & 3 & 0.001 & yes & 0.41 \\
            \midrule
            \multirow{6}{*}{Routing Iterations} & 64 & 1 & 0.001 & no & 0.44 \\
            & 64 & 1 & 0.001 & yes & 0.45 \\
            & 64 & 2 & 0.001 & no & 0.48 \\
            & 64 & 2 & 0.001 & yes & 0.35 \\
            & 64 & 3 & 0.001 & no & 0.37 \\
            & 64 & 3 & 0.001 & yes & 0.41 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & \textbf{0.33} \\
            & 64 & 3 & 0.001 & yes & 0.37 \\
            & 64 & 3 & 0.002 & yes & 0.51 \\
            & 64 & 3 & 0.01 & yes & - \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg1}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}) για 30 εποχές.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Αναφορικά με το μέγεθος της δέσμης, παρατηρείται σημαντική βελτίωση στην περίπτωση που δε χρησιμοποιείται αποκωδικοποιητής. Στην περίπτωση που χρησιμοποιείται δίκτυο αποκωδικοποιητή, έχει ένα μικρό προβάδισμα η ρύθμιση με το μικρότερο μέγεθος δέσμης. Αυτό πιθανότατα οφείλεται στον μικρότερο αριθμό των βημάτων οπισθοδιάδοσης σφάλματος που συμβαίνει κατά τον διπλασιασμό του μεγέθους δέσμης σε συνδειασμό τόσο με την σημαντική αύξηση των παραμέτρων λόγω της προσθήκης αποκωδικοποιητή όσο και με την κλιμάκωση του σφάλματως ανακατασκευής. Συνεπώς, μπορούμε με ασφάλεια να υποθέσουμε ότι εν γένη, ένα σύνολο δέσμης μεγέθους 64 οδηγεί σε καλύτερη εκπαίδευση από ένα σύνολο δέσμης 32 (αρκεί να συνοδεύεται από μεγάλο αριθμό εποχών).
    \item Αναφορικά με τον αριθμό των επαναλήψεων, κατά μέσο όρο παρατηρείται βελτίωση με την αύξηση των επαναλήψεων του αλγορίθμου. Όταν ο αριθμός επαναλήψεων είναι ίσος με τη μονάδα, ουσιαστικά δεν έχουμε κάποιον δυναμικό αλγόριθμο επανάληψης αφού η κάθε κάψουλα \en{DigitCap} προκύπτει από τον μέσο όρο των ψήφων της. Στηριζόμενοι και στα πειράματά του έργου \cite{sabour2017dynamic}, οι τρεις επαναλήψεις είναι η ιδανική παράμετρος για τη μέγιστη επίδοση. Η αύξηση του σφάλματος που παρατηρείται στην περίπτωση της χρήσης ανακατασκευής ίσως να οφείλεται στον μικρό αριθμό εποχών που έχει σαν αποτέλεσμα να μην προλαβαίνει να εκπαιδευτεί πλήρως το δίκτυο αποκωδικοποιητή\footnote{Άλλωστε, στα γραφήματα του σφάλματος επικύρωσης που παράγουμε κατά την εκπαίδευση, είναι προφανές ότι το σφάλμα μειώνεται ακόμα στις 30 εποχές.}.
    \item Στα πειράματα που αφορούν τον ρυθμό μάθησης φαίνεται ότι ένας μικρότερος ρυθμός μάθησης οδηγεί σε καλύτερη εκπαίδευση. Βέβαια, κατά τη διάρκεια της εκπαίδευσης πολλές φορές ο ρυθμός μάθησης μειώθηκε κατά μια ή και δύο κλίμακες μεγέθους από τον \en{learning rate scheduler}. Συνεπώς, σε περισσότερες επαναλήψεις ένας ρυθμός μάθησης με τιμή $0.001$ δε θα αποτελεί πρόβλημα. Το ασφαλές συμπέρασμα λοιπόν είναι ότι ο ρυθμός μάθησης δεν ωφελεί να είναι μεγαλύτερος του $0.001$.
\end{enumerate}

Στην εικόνα ... παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.0005, r = 3, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στην 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι ο \en{Argmax Scaled Routing}. Ο αλγόριθμος αυτός κάνει την υπόθεση ότι μια ψήφος είναι αρκετή για τη διαμόρφωση μιας κάψουλας γονέα. Συνεπώς, για τη διαμόρφωση ολόκληρου του επιπέδου \en{DigitCaps} αρκεί να επιλεγούν (με κριτήριο τα βάρη δρομολόγησης) τόσες ψήφοι εκπρόσωποι όσος είναι και ο αριθμός των κλάσεων. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg2} παρατίθενται τα αποτελέσματα των πειραμάτων. Προφανώς, η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.54 \\
            & 64 & 3 & 0.001 & no & 0.53 \\
            & 32 & 3 & 0.001 & yes & 0.52 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 0.53 \\
            & 64 & 2 & 0.001 & yes & \textbf{0.39} \\
            & 64 & 3 & 0.001 & no & 0.53 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 0.45 \\
            & 64 & 3 & 0.001 & yes & 0.51 \\
            & 64 & 3 & 0.002 & yes & 0.57 \\
            & 64 & 3 & 0.01 & yes & 0.59 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg2}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Scaled Routing} (αλγόριθμος \ref{alg:dynam_argmax_scaled_routing}) για 30 εποχές.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Σχετικά με το πρώτο πείραμα, αν και οι διαφορές δεν είναι μεγάλες, φαίνεται να βοηθάει η αύξηση του μεγέθους δέσμης και η προσθήκη αποκωδικοποιητή κατά την εκπαίδευση.
    \item Σε αντίθεση με τον κλασσικό αλγόριθμο δυναμικής δρομολόγησης, ο βέλτιστος αριθμός επαναλήψεων είναι 2. Όπως αποδεικνύεται στα ειδικά πειράματα (ενότητα \ref{sec:method1_special_experiments}), όσο αυξάνονται οι επαναλήψεις τόσο τα βάρη δρομολόγησης λαμβάνουν ακραίες τιμές (είτε 0 αν δρομολογούν ψήφους σε κάψουλες γονείς που δεν ανήκουν στη σωστή κλάση είτε 1 για κάψουλες παιδιά που ανήκουν στη σωστή κλάση στόχο). Συνεπώς, στις πρώτες επαναλήψεις, μπορεί τα βάρη δρομολόγησης να μην έχουν κορεστεί σε ακραίες τιμές αλλά από νωρίς τα βάρη που αντιστοιχούν στη σωστή κλάση έχουν τις μεγαλύτερες τιμές (αποδεικνύεται στην ενότητα \ref{sec:method1_special_experiments}). Το γεγονός όμως ότι η αύξηση των επαναλήψεων προκαλεί μείωση της επίδοσης στον συγκεκριμένο αλγόριθμο μας προδιαθέτει ότι υπάρχουν πολλές περιπτώσεις σύγχυσης όπου ενώ έτεινε προς τη σωστή κλάση, σε αργότερη επανάληψη άλλαξαν ριζικά τα βάρη ευνοώντας κάποια λανθασμένη κλάση. Η συγκεκριμένη περίπτωση περιγράφεται έμπρακτα στο δεύτερο σχήμα του έργου \cite{hinton2018matrix}. Επίσης, είναι πολύ πιθανό να οφείλεται στο ότι υπάρχουν κάψουλες που έχουν παράξει ψήφους με μικρή συμφωνία για την κάψουλα πρόβλεψης. Κατά τον κορεσμό τους, αυτές θα αποκτήσουν βάρη δρομολόγησης ίσα με τη μονάδα προς άλλες κάψουλες.
    \item Σε αντιστοιχεία με τις παραμέτρους της προηγούμενης μεθόδου, βλέπουμε ότι ένας χαμηλότερος ρυθμός μάθησης βοηθάει την διαδικασία εκπαίδευσης.
\end{enumerate}

Στην εικόνα ... παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.001, r = 2, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στην 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι ο \en{Argmax Routing}. Και αυτός ο αλγόριθμος κάνει την υπόθεση ότι μια ψήφος είναι αρκετή για τη διαμόρφωση μιας κάψουλας γονέα. Συνεπώς, και πάλι, για τη διαμόρφωση ολόκληρου του επιπέδου \en{DigitCaps} αρκεί να επιλεγούν (με κριτήριο τα βάρη δρομολόγησης) τόσες ψήφοι εκπρόσωποι όσος είναι και ο αριθμός των κλάσεων. Υπενθιμίζεται ότι η διαφορά με τον \en{Argmax Scaled Routing} είναι ότι δεν πραγματοποιείται κλιμάκωση των εκπροσώπων με βάση το αντίστοιχο βάρος δρομολόγησης. Αυτό πάει ένα βήμα παραπέρα την υπόθεσή μας αφού πλέον, το μήκος των \en{DigitCaps} που καθορίζει την κλάση πρόβλεψης δεν εξαρτάται από τον δυναμικό αλγόριθμο αλλά από τις επιλεγμένες ψήφους. Με απλά λόγια, ερχόμαστε ακόμα πιο κοντά στον ισχυρισμό ότι τα βάρη δρομολόγησης διαμορφώνονται αποκλειστικά τις ψήφους με μεγάλο μέτρο (χωρίς να δίνεται ιδιαίτερη έμφαση στην πολυδιάστατη σύμπτωση).\par

Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg3} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, όπως και στον προηγούμενο αλγόριθμο, η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 3 & 0.001 & no & 0.87 \\
            & 64 & 3 & 0.001 & no & \textbf{0.68} \\
            & 32 & 3 & 0.001 & yes & 0.90 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 0.88 \\
            & 64 & 2 & 0.001 & yes & 0.81 \\
            & 64 & 3 & 0.001 & no & 0.68 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 0.99 \\
            & 64 & 3 & 0.001 & yes & 0.83 \\
            & 64 & 3 & 0.002 & yes & 0.75 \\
            & 64 & 3 & 0.01 & yes & 0.80 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg3}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Routing} (αλγόριθμος \ref{alg:dynam_argmax_routing}) για 30 εποχές.}
\end{table}

Από τα πειράματα του πίνακα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Παρατηρούμε ότι η αύξηση του μεγέθους δέσμης συμβάλει στην εκπαίδευση.
    \item Η αύξηση του ρυθμού επαναλήψεων βοηθάει στον συγκεκριμένο αλγόριθμο την επίδοση. Γεγονός που ενισχύει την τελευταία υπόθεση της αντίστοιχης παρατήρησης του προηγούμενου αλγορίθμου. Δηλαδή, το γεγονός ότι ο αλγόριθμος της μη κλιμάκωσης των ψήφων εκπροσώπων παρουσιάζει βελτίωση στις 3 επαναλήψεις συνεπάγεται ότι για κάθε κάψουλα γονέα επιλέγονται οι σωστές ψήφοι. Παρόλα αυτά, όταν τα βάρη δρομολόγησης φτάνουν στον κορεσμό, ίσως υπάρχουν και άλλες (λίγες) κάψουλες των οποίων οι ψήφοι δεν συμφωνούν με τις άλλες ψήφους της σωστής κλάσης πρόβλεψης (π.χ. επειδή αναπαριστούν μέρη αντικειμένων που δεν εντοπίζονται στην συγκεκριμένη εικόνα εισόδου) αλλά τυχαίνει να συμφωνούν με λίγες ψήφους μιας άλλης κλάσης. Υπό αυτές τις συνθήκες, το αντίστοιχο βάρος δρομολόγησς της κάψουλας θα κορεστεί στη μονάδα πολύ γρήγορα\footnote{Στην ενότητα \ref{sec:method1_special_experiments} παρουσιάζεται μια τέτοια περίπτωση.}. 
    \item Σε αυτόν τον αλγόριθμο απαιτείται λίγο μεγαλύτερος ρυθμός μάθησης (ή περισσότερες εποχές).
\end{enumerate}

Στην εικόνα ... παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 64, lr = 0.001, r = 3, Reconstruction = no$). Παρατηρούμε ότι ακόμα και στη 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται. Πιθανότητα, το γεγονός αυτό είναι η εήγηση της αύξησης του σφάλματος με τη χρήση του αποκωδικοποιητή.\par

Σε κάθε διάταξη, η μέθοδος αυτή παρουσιάζει χειρότερα αποτελέσματα. Συνεπώς, απορρίπτονται οι υποθέσεις σχετικά με τη συνεισφορά των βαρών δρομολόγησης. Τέλος, το πόρισμα αυτό είναι ήδη αναμφίβολο (και λόγω παλαιότερων πειραμάτων σε 100 εποχές) και άρα, δεν έχει νόημα η δοκιμή του αλγορίθμου αυτού σε όλα τα σύνολα δεδομένων.\par

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο τελευταίος αλγόριθμος που εξετάζουμε σε αυτή τη μέθοδο είναι ο \en{Max Routing}. Υπενθυμίζεται ότι ο αλγόριθμος αυτός απορρίπτει τελείως τον δυναμικό αλγόριθμο δρομολόγησης και μαζί του, την υπόθεση των νευρωνικών δικτύων από κάψουλες περί φιλτραρίσματος με πολυδιάστατη σύμπτωση. Υποστηρίζει ότι ο στον δυναμικό αλγόριθμο δρομολόγησης, η ψήφος (για κάθε \en{DigitCap}) με το μεγαλύτερο μέτρο διαδραματίζει τον μεγαλύτερο ρόλο στη διαμόρφωση του μέσου διανύσματος $s_j$ και με αυτόν τον τρόπο, απλά προσελκύει όσες ψήφους τυχαίνει να συμφωνούν μαζί της. Συνεπώς, υποστηρίζουμε με αυτή τη μέθοδο ότι δεν πρόκειται για ένα φιλτράρισμα συμφωνίας αλλά για μια αδικαιολόγητα περίπλοκη επιλογή μεγίστου διανύσματος.\par

Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_mnist_alg4} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, η δοκιμασία για τον αριθμό των επαναλήψεων δεν έχει νόημα καθώς ο αλγόριθμος δεν είναι επαναληπτικός.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c}
            \toprule
            Experiment & Batch Size & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Batch Size} & 32 & 0.001 & no & 0.92 \\
            & 64 & 0.001 & no & 0.81 \\
            & 32 & 0.001 & yes & \textbf{0.72} \\
            & 64 & 0.001 & yes & 0.89 \\
            \midrule
            \multirow{4}{*}{Learning Rate} & 64 & 0.0005 & yes & 1.00 \\
            & 64 & 0.001 & yes & 0.89 \\
            & 64 & 0.002 & yes & 0.90 \\
            & 64 & 0.01 & yes & 0.95 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_mnist_alg4}Πειράματα στο \en{MNIST} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Max Routing} (αλγόριθμος \ref{alg:dynam_max_routing}) για 30 εποχές.}
\end{table}

Από τα πειράματα του πίνακα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{enumerate}
    \item Από τα αποτελέσματα, δεν μπορούμε να κρίνουμε ποια είναι η καλύτερη επιλογή μεγέθους δέσμης. Εκτιμούμε ότι σε αυτό τον αριθμό εποχών για τον συγκεκριμένο αλγόριθμο, μικρότερος αριθμος δέσμης είναι προτιμιταίος.
    \item Σε αυτόν τον αλγόριθμο η τιμή $0.001$ για τον ρυθμό μάθησης φαίνεται να είναι ιδανική.
\end{enumerate}

Για άλλη μια φορά, στην εικόνα ... παρατίθεται το σφάλμα εκπαίδευσης (\en{training loss}) και επαλήθευσης (\en{validation loss}) για το μοντέλο με την καλύτερη επίδοση στον ανωτέρω πίνακα ($Bs = 32, lr = 0.001, Reconstruction = yes$). Παρατηρούμε ότι ακόμα και στη 30κοστή εποχή, το σφάλμα επαλήθευσης μειώνεται ακόμα.\par

Σε κάθε διάταξη, η μέθοδος αυτή παρουσιάζει χειρότερα αποτελέσματα σε σχέση με τις πρώτες δύο. Συνεπώς, αφενός δε θα γίνει εκτενή μελέτη αυτού σε όλα τα σύνολα δεδομένων και αφετέρου φαίνεται ότι ο αλγόριθμος δρομολόγησης πραγματικά συμβάλει στην εκπαίδευση. Περισσότερα σχετικά πειράματα βρίσκονται στην ενότητα \ref{sec:method1_special_experiments}.

\subsubsection{Σύνολο Δεδομένων \en{CIFAR10}}
Αντίστοιχα με τα πειράματα για το σύνολο \en{MNIST}, παρατίθενται παρακάτω τα πειράματα για το πιο σύνθετο σύνολο δεδομένων \en{CIFAR10}. Οι λίγες εποχές που χρησιμοποιούνται για το συγκεκριμένο σύνολο σε συνδειασμό με την τάση των νευρωνικών δικτύων από κάψουλες να εξηγούν το οτιδήποτε βρίσκεται στην εικόνα (ακόμα και το παρασκήνιο) οδηγούν σε χαμηλές επιδόσεις. Σημειώνουμε ότι κατά αντιστοιχία με το έργο \cite{sabour2017dynamic}, χρησιμοποιήθηκε μια επιπλέον κλάση \en{none-of-the-above} για να δρομολογούν εκεί την ψήφο τους κάψουλες που αναπαριστούν αντικείμενα που δεν εμφανίζονται στην εικόνα. Επίσης, ο αριθμός των τύπων από κάψουλες αυξήθηκε από 32 σε 64 (όπως και στο έργο \cite{sabour2017dynamic}).\par

Ο αριθμός των πειραμάτων είναι μικρότερος από αυτόν για το σύνολο \en{MNIST}. Αυτό διότι, αν και το σύνολο \en{CIFAR10} έχει μεγάλη ετερογένεια με το προηγούμενο, ορισμένα πειράματα ποτέ δεν οδηγούν σε καλύτερα αποτελέσματα (ανεξαρτήτως συνόλου δεδομένων). Τα πειράματα αυτά έχουν αφαιρεθεί (όπως αυτό της δοκιμής του συνόλου δέσμης).

Τα πρώτα πειράματα για το σύνολο δεδομένων \en{CIFAR10} αφορούν τον κλασσικό δυναμικό αλγόριθμο δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}). Τα αποτελέσματα των πειραμάτων παρατίθενται στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg1}. Είναι λιγότερα από πρίν
\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{2}{*}{Batch Size} & 32 & 3 & 0.0005 & yes & \textbf{20.60} \\
            & 64 & 3 & 0.0005 & yes & 23.96 \\
            \midrule
            \multirow{7}{*}{Routing Iterations} & 64 & 1 & 0.001 & no & 24.64 \\
            & 64 & 1 & 0.001 & yes & 23.19 \\
            & 64 & 2 & 0.001 & no & 29.25* \\
            & 64 & 2 & 0.001 & yes & 26.23* \\
            & 64 & 3 & 0.001 & no & 31.24* \\
            & 64 & 3 & 0.001 & yes & 29.15* \\
            \midrule
            \multirow{2}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 23.96 \\
            & 64 & 3 & 0.001 & yes & 29.15* \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg1}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο δυναμικής δρομολόγησης με συμφωνία (αλγόριθμος \ref{alg:dynam_routing}) για 30 εποχές. Οι αριθμοί με αστερίσκο αναφέρονται σε περιπτώσεις αστάθειας του αλγορίθμου εκπαίδευσης.}
\end{table}

Από τα ανωτέρω πειράματα μπορούμε να εξάγουμε τα εξής συμπεράσματα:
\begin{itemize}
    \item Αναφορικά με το μέγεθος της δέσμης, παρατηρήθηκε ότι χαμηλότερο μέγεθος (32) εξαλείφει την όποια αστάθεια στον αλγόριθμο εκπαίδευσης. Για αυτό και η καλύτερη επίδοση επιτυγχάνεται με αυτό.
    \item Αναφορικά με τον αριθμό των επαναλήψεων, παρατηρούμε ότι η αύξησή τους επιδεινώνει την αστάθεια. Το γεγονός ότι οι καλύτερες παραμετροποιήσεις είναι για 3 επαναλήψεις σε συνδειασμό με τα πειράματα στο προηγούμενο σύνολο δεδομένων υποδεικνύουν ότι αυτή είναι η βέλτιστη υπερπαράμετρος.
    \item Για άλλη μια φορά, χαμηλότερη τιμή ρυθμού μάθησης βελτιώνει την επίδοση και μειώνει την πιθανότητα αστάθειας.
\end{itemize}


% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος αλγόριθμος είναι ο \en{Argmax Scaled Routing}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg2} παρατίθενται τα αποτελέσματα των πειραμάτων. Να επαναλάβουμε ότι και για αυτό το σύνολο δεδομένων η δοκιμασία για 1 επανάληψη αλγορίθμου δρομολόγησης δεν υπάρχει καθώς σε αυτή την περίπτωση, όλα τα βάρη δρομολόγησης είναι ίσα (λόγω αρχικοποίησης).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 28.10 \\
            & 64 & 2 & 0.001 & yes & \textbf{27.46} \\
            & 64 & 3 & 0.001 & no & 29.65 \\
            & 64 & 3 & 0.001 & yes & 31.02 \\
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 29.06 \\
            & 64 & 3 & 0.001 & yes & 31.02 \\
            & 64 & 3 & 0.002 & yes & 31.60 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg2}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Scaled Routing} (αλγόριθμος \ref{alg:dynam_argmax_scaled_routing}) για 30 εποχές.}
\end{table}

Τα ανωτέρω πειράματα, επιβεβαιώνουν τα συμπεράσματα των πειραμάτων στο σύνολο δεδομένων \en{MNIST}. Βλέπουμε ότι οι 2 επαναλήψεις είναι η βέλτιστη ρύθμιση για τον συγκεκριμένο αλγόριθμο ενώ επίσης, συνήθως βοηθάει η ελάττωση του ρυθμού μάθησης στην τιμή $0.0005$. Σημειώνουμε ότι ο αλγόριθμος αυτός (όπως και όλοι οι αλγόριθμοι εκτός του προηγούμενου), ουδέποτε εμφάνισε την οποιαδήποτε αστάθεια κατά την εκπαίδευση.


% Πες για τα πειράματα του επόμενου αλγορίθμου.
Ο επόμενος σε σειρά αλγόριθμος είναι όπως και πριν ο \en{Argmax Routing}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg3} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων (με τη δοκιμή για 1 επανάληψη να μην έχει ουσία).

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c}
            \toprule
            Experiment & Batch Size & Routing Iterations & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{4}{*}{Routing Iterations} & 64 & 2 & 0.001 & no & 38.96 \\
            & 64 & 2 & 0.001 & yes & 37.96 \\
            & 64 & 3 & 0.001 & no & 38.06 \\
            & 64 & 3 & 0.001 & yes & 38.49 \\
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 3 & 0.0005 & yes & 38.97 \\
            & 64 & 3 & 0.001 & yes & 38.49 \\
            & 64 & 3 & 0.002 & yes & \textbf{37.83} \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg3}Πίνακας που περιέχει τα πειράματα που έγιναν στο σύνολο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Argmax Routing} (αλγόριθμος \ref{alg:dynam_argmax_routing}) για 30 εποχές.}
\end{table}

Για άλλη μια φορά επιβεβαιόνωνται οι παρατηρήσεις που έγιναν στο σύνολο δεδομένων \en{MNIST} για τον αντίστοιχο αλγόριθμο. Φαίνεται ότι οι 3 επαναλήψεις οδηγούν σε καλύτερη επίδοση αφού σε αυτή τη ρύθμιση επιτυγχάνεται η καλύτερη επίδοση.

% Πες για τα πειράματα του επόμενου αλγορίθμου.
Κλείνοντας τα πειράματα για την εύρεση των βέλτιστων υπερπαραμέτρων, αναφέρουμε τα πειράματα που έγιναν στον αλγόριθμο \en{Max Routing} με το σύνολο δεδομένων \en{CIFAR10}. Στον πίνακα \ref{tab:method_1_hyperparameter_tuning_cifar10_alg4} παρατίθενται τα αποτελέσματα των σχετικών πειραμάτων. Προφανώς, όπως και στον προηγούμενο αλγόριθμο, η δοκιμασία για τον αριθμό των επαναλήψεων δεν έχει νόημα καθώς ο αλγόριθμος δεν είναι επαναληπτικός.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c}
            \toprule
            Experiment & Batch Size & Learning Rate & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{3}{*}{Learning Rate} & 64 & 0.0005 & yes & \textbf{37.19} \\
            & 64 & 0.002 & yes & 37.60 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_hyperparameter_tuning_cifar10_alg4}Πειράματα στο \en{CIFAR10} για την αναζήτηση υπερπαραμέτρων στον αλγόριθμο \en{Max Routing} (αλγόριθμος \ref{alg:dynam_max_routing}) για 30 εποχές.}
\end{table}

Από τα δύο περιάματα αποδεικνύεται ότι ο μικρότερος ρυθμός μάθησης βοηθάει γενικότερα στην εκπαίδευση των αλγορίθμων μας.


\subsection{Επιλεκτική Εμβάθυνση Πειραμάτων και Σύγκριση}
Για τους δύο αλγορίθμους της μηθόδου 1 με την καλύτερη επίδοση (αλγόριθμοι \ref{alg:dynam_routing} και \ref{alg:dynam_argmax_scaled_routing}) πραγματοποιήθηκαν εκτενέστερα πειράματα με περισσότερες εποχές, για όλα τα σύνολα δεδομένων και σε μεγαλύτερο μέγεθος δέσμης (εφόσον ήταν εφικτό). Τα νεα πειράματα αυτά, μαζί με τα παλαιότερα πειράμτα από τις άλλες δύο μεθόδους (για 100 εποχές), τα παρουσιάζουμε συγκεντρωτικά στον πίνακα \ref{tab:method_1_all_datasets}.

\begin{table}[h]
    \begin{center}
        \en{
        \begin{tabular}{c c c c c c c}
            \toprule
            Dataset & Algorithm & Bs & r & Lr & Reconstruction & Test Error (\%) \\ 
            \midrule
            \multirow{8}{*}{MNIST} &Classic& 64 & 3 & 0.0005 & yes & 0.41 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 0.43 \\
            &Classic& 128 & 3 & 0.0005 & yes & 0.35 \\
            &Argmax Scaled& 128 & 2 & 0.001 & yes & 0.37 \\
            &Classic& 32 & 3 & 0.0005 & yes & 0.35 \\
            &Argmax Scaled& 32 & 2 & 0.001 & yes & 0.42 \\
            &Argmax& 64 & 3 & 0.001 & yes & 0.60 \\
            &Max& 32 & 0 & 0.001 & yes & 0.75 \\
            \midrule
            \multirow{2}{*}{FashionMNIST} &Classic& 64 & 3 & 0.0005 & yes & 6.95 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 7.00 \\
            \midrule
            \multirow{2}{*}{CIFAR10} &Classic& 64 & 3 & 0.0005 & yes & 21.64 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 22.16 \\
            \midrule
            \multirow{2}{*}{SmallNORB} &Classic& 64 & 3 & 0.0005 & yes & 17.01 \\
            &Argmax Scaled& 64 & 2 & 0.0005 & yes & 16.49 \\
            \bottomrule
            
        \end{tabular}
        }
    \end{center}
    \caption[]{\label{tab:method_1_all_datasets}Πίνακας που περιέχει τα πειράματα που έγιναν σε όλα τα σύνολα δεδομένων στους δυο αλγορίθμους με την καλύτερη επίδοση. Επίσης, περιέχονται για λόγυς σύγκρισης ορισμένα πειράματα των δύο αλγορίθμων με τη λιγότερο καλή επίδοση (όπως μετρήθηκε στην προηγούμενη ενότητα). Σημειώνουμε ότι με τον όρο \en{Classic} αναφερόμαστε στον αλόριθμο δυναμικής δρομολόγησης με συμφωνία.}
\end{table}
Παρακάτω, παρουσιάζουμε τις γραφικές που προκύπτουν από την εκπαίδευση των καλύτερων σε επίδοση μοντέλων για το κάθε σύνολο δεδομένων. Παρατηρούμε ότι γενικά, έχουμε \en{overfitting} αφού υπάρχει σημαντική διαφορά μεταξύ της καμπύλης εκπαίδευσης και της καμπύλης επικύρωσης (\en{validation curve}). Το φαινόμενο αυτό είναι πιο έντονο σε σύνθετα σύνολα δεδομένων (\en{CIFAR10}). ...

\subsubsection{Παρατηρήσεις}
Το πιο βέβαιο συμπέρασμα που μπορούμε να εξάγουμε από τα ανωτέρω αποτελέσματα είναι ότι ο αλγόριθμός μας \en{Argmax Scaled Routing} δεν παρουσιάζει ένονες διαφορές στην επίδοση σε σχέση με τον κλασσικό αλγόριθμο δρομολόγησης με συμφωνία. Αν εξαιρέσουμε τα σύνολα δεδομένων \en{CIFAR10} και \en{SmallNORB} όπου ο αριθμός των εποχών δεν επαρκεί για την σύγκλισή τους, στα υπόλοιπα σύνολα δεδομένων οι διαφορές στις επιδόσεις είναι σχεδόν μηδαμινές.\par

Η παρόμοια ακρίβεια μεταξύ των δύο αλγορίθμων σημαίνει ότι η εξαγωγή ενός διανύσματος για κάθε κάψουλα \en{DigitCap} ως το σταθμισμένο άθροισμα των ψήφων δεν οφελεί πρακτικά την επίδοση του δικτύου. Συνεπώς, τον πρωτεύοντα ρόλο τον έχουν τα βάρη δρομολόγησης που κλιμακώνουν τις ψήφους. Αυτά είναι ένα επαρκές κριτήριο για την επιλογή της ψήφου που θα δρομολογηθεί, κλιμακωμένη στο επόμενο επίπεδο. Το γεγονός αυτό μπορούμε να το εκμεταλευτούμε μειώνωντας τις επαναλήψεις και γλιτώνωντας υπολογιστικό κόστος. Αντί για την μια περιττή επανάληψη θα μπορούσαμε να εισάγουμε ένα παραπάνω συνελικτικό επίπεδο που, όπως έχει αποδηχθεί στο κεφάλαιο \ref{chap:related_work}, βελτιώνει την ακρίβεια.\par 

Το ερώτημα που προκύπτει φυσικά είναι αν το δίκτυο του αλγορίθμου \en{Argmax Scaled Routing} μαθαίνει τα βάρη του ώστε να ανταποκρίνεται με βέλτιστο τρόπο στον νέο μας αλγόριθμο ή αν εξ'αρχής ο κλασσικός αλγόριθμος με συμφωνία έχει την ιδιότητα που περιγράφουμε (Δηλαή ότι το μέγιστο βάρος δρομολόγησς και η αντίστοιχη ψήφος - ψήφος \textquote{εκπρόσωπος} - αρκούν για την ρομολόγηση). Το ερώτημα αυτό το απαντάμε στο \ref{sec:method1_special_experiments} ύστερα από την ιενέργεια κατάλληλων ειδεικών πειραμάτων.\par

Σε σύγκριση με τον απόλυτο αλγόριθμο \en{Max Rooting} της πρώτης μεθόδου, φαίνεται πως το φιλτράρισμα με πολυδιάστατη συμφωνία πραγματικά βοηθάει τις επιδόσεις του μοντέλου χωρίς να εισάγει επιπλέον παραμέτρους. Προφανώς λοιπόν, η υπόθεση ότι ο κλασσικός αλγόριθμος δρομολόγησης μπορεί να μην προσφαίρει κάτι παραπάνω από την απλή δρομολόγηση της μέγιστης σε μήκος ψήφου (για κάθε κάψουλα γονέα) δεν είναι ορθή. Τέλος, σε σύγκριση με τον αλγόριθμο \en{Argmax Routing}, παρατηρούμε ότι υπάρχουν οφέλη από την χρήση των βαρών δρομολόγησης όχι μόνο ως κριτίριο για την επιλογή των ψήφων εκπροσώπων αλλά ως μέγεθος για την επιλογή της κλάσης εξόδου (αφού διαμορφώνει άμεσα το μήκος των \en{DigitCaps}).

\subsection{Ειδικά Πειράματα}
\label{sec:method1_special_experiments}
\subsubsection{Τι Μαθαίνει να Αναπαριστά η Κάθε Διάσταση του Διανύσματος \en{DigitCap}}
% Πειράματα ανακατασκευής
\subsubsection{Συμφωνία Ψήφων}
% Πειράματα με confusion matrix
\subsubsection{Κατανομή των Ψήφων}
% Πειράματα με τα γραφήματα με τις μπάρες.
\subsubsection{Κριτήριο Επιλογής Ψήφων Αλγορίθμου Δυναμικής Δρομολόγησης με Συμφωνία}
% Εκπαίδευση στον κλασικό αλγόριθμο, δοκιμή στον αλγόριθμο max, argmax, argmax scaled.
% Το ίκτυο προσαρμόζεται στον νέο αλγόριθμο ή μαθαίνει έτσι και στην κλασική περίπτωση?
\section{Πειραματική Μελέτη Μεθόδου 2}
\section{Πειραματική Μελέτη Μεθόδου 3}
\section{Πειραματική Μελέτη Μεθόδου 4}
\section{Σύγκριση Πειραματικών Αποτελεσμάτων Μεθόδων}